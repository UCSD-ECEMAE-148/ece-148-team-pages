{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ECE/MAE 148: Introduction to Autonomous Vehicles","text":"<p>Documentation is under development.</p>"},{"location":"win23team2/","title":"Team 2","text":""},{"location":"win23team2/#team-2-ecemae-148-final-report","title":"Team 2 ECE/MAE 148 Final Report","text":""},{"location":"win23team2/#wave-the-team-2-fast-2-furious","title":":wave: The Team: 2 Fast 2 Furious","text":"<p>(Left to Right) - Elias Fang (CSE) - Ainesh Arumugam (ECE) - Matthew Merioles (ECE) - Junhao \"Michael\" Chen (MAE)</p>"},{"location":"win23team2/#project-overview","title":"\ud83d\udcdd Project Overview","text":"<p>Mario Kart in real-life? That's basically what we did. We designed a boost system similar to those in Mario Kart, where detecting colored pieces of paper on the track can change throttle for a short period of time. Depending on the color of the \"pad\" the car drives over, it will either speed up, slow down, or stop for a few seconds, just like in the game!</p> <p></p>"},{"location":"win23team2/#our-robot","title":"\ud83c\udfce Our Robot","text":""},{"location":"win23team2/#birds-eye","title":"Bird's Eye","text":""},{"location":"win23team2/#front","title":"Front","text":""},{"location":"win23team2/#left","title":"Left","text":""},{"location":"win23team2/#right","title":"Right","text":""},{"location":"win23team2/#back","title":"Back","text":""},{"location":"win23team2/#schematic","title":"Schematic","text":""},{"location":"win23team2/#final-project","title":"\ud83c\udf44 Final Project","text":""},{"location":"win23team2/#what-we-promised","title":"What We Promised","text":""},{"location":"win23team2/#must-haves","title":"Must haves","text":"<p>[X] Distinguishing different colors through the camera</p> <p>[X] Adjust the throttle based on the color</p>"},{"location":"win23team2/#nice-to-haves","title":"Nice to haves","text":"<p>[X] Have the car detect a flat piece of paper on the track (like a booster pad)</p> <p>[ ] Combine with lane-following algorithm</p>"},{"location":"win23team2/#gantt-chart","title":"Gantt Chart","text":"<p> https://sharing.clickup.com/9010060626/g/h/8cgn7aj-87/769d44f22562beb</p>"},{"location":"win23team2/#what-we-accomplished","title":"What We Accomplished","text":""},{"location":"win23team2/#color-detection","title":"Color Detection","text":"<ul> <li>Used OpenCV for color detection and edge tracing</li> <li>Used color mask algorithm to detect proportion of frame that color takes up</li> <li>Detected multiple colors at the same time</li> <li>Determined HSVs for orange, pink, and blue</li> </ul> <p>Demo</p>"},{"location":"win23team2/#pyvesc","title":"PyVESC","text":"<ul> <li>Connection through external webcam</li> <li>Different RPM values are sent through PyVesc to achieve different speed for different colors marked by different states:</li> <li>Blue (Boost) = speed up for 3 sec</li> <li>Pink (Slow)= slow down for 3 sec</li> <li>Orange (Stop) = stop for 3 sec</li> <li>Neutral (Normal) = constant rpm</li> </ul> <p>Blue Demo</p> <p>Pink Demo</p> <p>Orange Demo</p>"},{"location":"win23team2/#presentation","title":"Presentation","text":"<p>https://docs.google.com/presentation/d/1oJPRLYIKvHUXEIK9hoYpPFoFAyHuG6sE7ZrU9NQPG8g/edit?usp=sharing</p>"},{"location":"win23team2/#code","title":"Code","text":"<p>https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart.py</p>"},{"location":"win23team2/#possible-future-work","title":"Possible Future Work","text":"<ul> <li>Change the colored paper into Mario Kart items (mushroom, bananas, etc.) for the car to identify</li> <li>Allow the car to run autonomously on a track and still apply speed changes</li> <li>Race with other teams \ud83d\ude09</li> </ul>"},{"location":"win23team2/#autonomous-laps","title":"\ud83c\udfc1 Autonomous Laps","text":"<p>DonkeyCar</p> <p>ROS2 Line Following</p> <p>ROS2 Lanes</p> <p>GNSS</p>"},{"location":"win23team2/#additional-work-done","title":"Additional Work Done","text":"<ul> <li>Our team was tasked with implementing a depth feature with our contour function, using depthAI to integrate with the OAK-D Camera!</li> <li>Our updated code is now able to measure how far the contoured object is, measured in cm!</li> </ul> <p>[Depth Demo] https://www.youtube.com/watch?v=NYIz7--TpgY [Updated Code] https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart_depth.py</p>"},{"location":"win23team2/#acknowledgements","title":"Acknowledgements","text":"<p>Thanks for Professor Jack Silberman, TA Kishore Nukala, and TA Moises Lopez!</p>"},{"location":"win23team3/","title":"Final Report for Team 3 (ECE/MAE 148)","text":""},{"location":"win23team3/#team-members","title":"Team Members","text":"<ul> <li>Nathaniel Barnaby - ECE</li> <li>Yang-Jie Qin - ECE</li> <li>Cheuk Hin Bryan Cheng - MAE</li> <li>Patrick Nguyen - MAE</li> </ul>"},{"location":"win23team3/#project-overview","title":"Project Overview","text":"<ul> <li>Our initial final project was a combination of the usage of an IMU and GNSS to implement position tracking. An IMU is an inertial measurement unit, composing of gyros, accelerometers, and more sensors to track the movement of something. With these sensors, the car's location can be estimated off of a general initial GPS location with the addition to its movement measured by its speed, acceleration, turning, etc. This ended up being too complex for our team which resulted in little progress. We were then assigned new mini tasks which consists of using 2 of the sensors provided in our kits. The assignment was to use the OAK-D camera and the lidar separately to measure depth of both a small (0.15m) object and a larger (0.5m) object at different distances. We ended up comparing results of both objects at distances of 0.5, 1, 2, 3, and 4 meters. We would then compare the outputed values from the sensors to what the actual correspond measurment. A comparison between the accuracy of depth finding between the Oak-D camera and lidar would also be necesasry. A second task was assigned which was to output a face recognition system out of the OAK-D camera. </li> </ul>"},{"location":"win23team3/#results","title":"Results","text":"<ul> <li>For the distance measurement assignment, both the camera and lidar were able to successfully measure distance for the small the large object at the different ranges. </li> <li>For the camera, it was accurate at determining smaller distances, but at larger distances (3+ meters) error seemed to begin growing exponentionally. The difference between small and large objects was negligible as long as the area in which the distances were averaged fit within the object, which at further distances can start causing fluctuations with smaller objects. </li> <li>For the lidar, it was accurate at determining all distances with a linear or almost static amount of error. At smaller distances this was larger than the error of the camera, but at larger distances it vastly outperformed the camera due to the nature of the camera's exponential error. Additionally, the lidar had to be hand-calibrated, so with more time the error could have been lowered due to this effect. Also, since we recorded distance measurements within a range of 0.4 degrees, the measurement would be inaccurate with a smaller object at longer distances. This could be overcame by decreasing the range and waiting longer.</li> <li> <p>For most scenarios the lidar seems to be the winning choice for distance measurement. While at lower distances the camera seemed to outperform the lidar, the lidar seems to be more consistent with its measurements than the camera. Additionally, the lidar offers 360 degrees of distance measurements while the camera only works in one direction.</p> </li> <li> <p>DepthAI Distance Measurement with ~0.5 m^2 Object Video Link</p> </li> <li> <p>DepthAI Distance Measurement with 0.15 m^2 Box Video Link</p> </li> <li> <p>LiDAR Distance Measurement with ~0.5 m^2 Object Video Link</p> </li> <li> <p>LiDAR Distance Measurement with 0.15 m^2 Box Video Link</p> </li> <li> <p>As for the camera face recognition, we were succesfully able to output video display which recognizes faces. This is a relatively fast responding system. It outputs the number of faces it recognizes which we tested from 0-3 faces real time. There is some error within the system as it can be innacurate thinking other shiny objects and or parts of a face are another face. It is also not limited to stationary faces as it recognizes people moving too. </p> </li> <li>Face Recognition Video Link</li> </ul>"},{"location":"win23team3/#gantt-chart","title":"Gantt Chart","text":""},{"location":"win23team3/#hardware-mechanical-design","title":"Hardware: Mechanical Design","text":"<p>\\ Camera/flashlight Mount</p> <p>\\ Electronics Tray</p> <p>\\ Front/rear Electronics Plate Offset</p> <p>\\ GPS Mount</p> <p>\\ IMU Mount</p> <p>\\ Jetson Case Key Mount</p> <p>\\ Jetson Nano Main Case</p> <p>\\ Lidar Tower</p> <p>\\ Servo Voltage Converter</p> <p>\\ Vesc Power Distributor</p>"},{"location":"win23team3/#previous-designs","title":"Previous Designs","text":""},{"location":"win23team3/#electronic-components","title":"Electronic Components","text":"<p>\\ Jetson Nano</p> <p>\\ OAK-D Camera</p> <p>\\ Lidar LD06</p>"},{"location":"win23team3/#electronic-wiring-schematic","title":"Electronic Wiring Schematic","text":""},{"location":"win23team3/#final-set-up","title":"Final Set Up","text":"<p> Bird's Eye View</p> <p> Left View</p> <p> Right View</p>"},{"location":"win23team3/#packages-and-drivers","title":"Packages and Drivers","text":"<ul> <li>cv2 (OpenCV)</li> <li>depthai (DepthAI)</li> <li>numpy</li> <li>math</li> <li>binascii</li> </ul>"},{"location":"win23team3/#milestones","title":"Milestones","text":"<ul> <li>Face Recognition using DepthAI - Detects faces through a webcam and displays a count in the terminal</li> <li>Distance Measurement using DepthAI - Using the disparity between the left and right cameras of the OAKD, distance can be calculated. This was averaged over an area to give an estimated distance of an object.</li> <li>Distance Measurement using LiDAR - Using a LiDAR is is relatively simple to detect distances in a 360 degree range. By averaging distances over a very small range (0.4 degrees) we determined the distance of an object.</li> </ul>"},{"location":"win23team3/#potential-future-workunaccomplished-goals","title":"Potential Future Work/Unaccomplished Goals","text":"<ul> <li>Recognizing and labeling specific faces</li> <li>Running code off of the OAK-D Camera instead of needing an external computer to run the code.</li> </ul>"},{"location":"win23team3/#presentations","title":"Presentations","text":"<p>-Final Project Proposal -Final Presentation</p>"},{"location":"win23team3/#acknowledgments","title":"Acknowledgments","text":"<p>Professor Jack Silberman, TA Kishore Nukala, Moises Lopez-Mendoza, Design and Innovation Building, all of our wonderful classmates</p>"},{"location":"win23team4/","title":"Team 4 Final Project Report","text":""},{"location":"win23team4/#members-vasanth-senthil-ece-eddy-rodas-limamae-lingpeng-mengece","title":"Members: Vasanth Senthil (ECE), Eddy Rodas Lima(MAE), Lingpeng Meng(ECE)","text":""},{"location":"win23team4/#physical-setup","title":"Physical Setup","text":""},{"location":"win23team4/#initial-goals","title":"Initial Goals","text":""},{"location":"win23team4/#objective","title":"Objective","text":"<ul> <li>Make our RoboCar follow sound using sound localization from multiple microphones.</li> </ul>"},{"location":"win23team4/#must-haves","title":"Must Haves","text":"<ul> <li>Car is able to determine approximate direction of an audio source</li> <li>Car moves in towards audio source once direction of audio is determined</li> </ul>"},{"location":"win23team4/#nice-to-haves","title":"Nice to Haves","text":"<ul> <li>Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume</li> <li>Accurate movement towards source</li> </ul>"},{"location":"win23team4/#accomplishments","title":"Accomplishments","text":"<ul> <li>Researched and experimented with different methods of sound localization, such as using time of arrival delay between microphones</li> <li>Used the pyaudio library to collect streams of microphone data from each of the three microphone, and processed this data in order to tell direction real-time based on sound level</li> <li>Used the pyvesc library to create a custom movement file that allows us to easily move the RoboCar based on provided angles and throttle until specified conditions are met</li> <li>The microphone processing file called upon the movement functions after determining current direction.</li> <li>We also spread the left and right microphones to minimize the noise created by the car\u2019s movement, which would influence the accuracy of direction calculations.</li> </ul>"},{"location":"win23team4/#demo-videos","title":"Demo Videos","text":"<ul> <li>Static Source Left</li> <li>Static Source Right</li> <li>Moving Source Front</li> <li>Moving Source Back</li> <li>Moving Source Further Away</li> </ul>"},{"location":"win23team4/#issues-faced","title":"Issues Faced","text":"Issue Solution Original idea of using time delay between microphones was not possible due to jetson limitations Decided to use volume of sound signals instead of microphone timing in order to determine direction of audio. Sound from car\u2019s movement, such as servo motors and wheels, were causing our robot to turn randomly. Created wings out of cardboard which lifted and spread out the left/right microphones away from the wheels. Random/sudden loud noises from the environment made the car turn in wrong directions, even when originally was moving correctly towards audio source. Averaged the audio signals of each microphone over the last few ticks using a buffer in order to reduce impact of outlier noises. Required tuning, as a small buffer would not remove outliers as effectively while a large buffer reduced reaction time of robot\u2019s movement when direction of audio actually changed"},{"location":"win23team4/#what-did-not-work","title":"What did not work","text":"<ul> <li>Original plan was to use the time delay between when microphones head a noise over a sound threshold in order to calculate angle of sound. Problem with that is the Jetson Nano is running Linux, and due to background processes and overhead, it is not precise enough to identify the timing needed for this method to work when the microphones are this close together. This may have been possible with a real-time system such as an Arduino</li> <li>Another method was using real-time localized sound by performing beamforming using microphones in array, but we were not certain if we had enough time to implement and debug this method in time (received the microphones on Saturday) as it was significantly more complex than the other options. </li> </ul>"},{"location":"win23team4/#next-steps-if-we-had-more-time","title":"Next Steps (If we had more time)","text":"<ul> <li>Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume, would help filter interference with background noises.</li> <li>Function based on sound difference between microphones to get precise steering angles. Current only steers forward, full left, full right.</li> <li>More accurate movement with our backwards direction</li> <li>Minimize unwanted noise coming from either surroundings or the vehicle.</li> </ul>"},{"location":"win23team5/","title":"Final Project Repository for Team 5 of the 2023 Winter Class MAE ECE 148 at UCSD","text":"<p>Our Final Project has one main objective, which is inspired by a pet dog that plays fetch. Our goal is to design a robot that can identify a green ball, like a tennis ball, locate it, move towards it, pick it up, and return back to its initial location. We achieved this using an OpenCV-based vision system to recognize the ball and Pyvesc to control the car's movements. We also designed a claw mechanism to pick up the ball when it's within range and a servo to move the ball into the claw.</p> <p>In summary, our project demonstrates the capabilities of an autonomous robot that can navigate an environment, recognize objects, and perform tasks like fetching. With further improvements, this type of robot could have many potential applications in various industries.</p> <p> </p>"},{"location":"win23team6/","title":"MAE/ECE 148 Winter 2023 at UCSD","text":""},{"location":"win23team6/#team-6","title":"TEAM 6","text":"<p>Our project uses the OAK-D camera, a roboflow YOLO model, PyVESC module, and an Arduino-powered camera mount to get our car to scan its surroundings until it finds a basketball and drive until it is within about 0.5 m of the ball.</p>"},{"location":"win23team6/#car-assembly","title":"Car Assembly","text":""},{"location":"win23team6/#vehicle-body","title":"Vehicle Body","text":""},{"location":"win23team6/#camera-mount","title":"Camera Mount","text":""},{"location":"win23team6/#tech-stack","title":"Tech Stack","text":""},{"location":"win23team6/#roboflowoak-module","title":"RoboflowOak Module","text":"<p>We used roboflow to train a ball detection model and host the model. We, then, made API calls to hosted model to retrieve predictions on frame captures from the OAK-D camera. Once a ball is found, we wrote a script to calculate the angle between the center of the bounding box drawn around the detected ball and the centerline of the camera (which by default is the center of the frame).</p>"},{"location":"win23team6/#pyvesc-module","title":"PyVESC Module","text":"<p>We used the pyvesc module to set our servo's angle once a detection has been made. The steering angle is proportional to the calculated angle. Once the steering is set, we increase throttle for about half a second and then stop the motor to make another detection. We then loop over these steps until the ball is within 0.5 m of the frame. Our stopping condition was for the width of the bounding box of the ball to be a certain ratio of the total frame width. The ratio is hardcoded based on fine-tuning to get the car to stop at about 0.5 m.</p>"},{"location":"win23team6/#arduino-board","title":"Arduino Board","text":"<p>We used an arduino nano to control the camera mount. The camera mount can rotate horizontally (yaw-equivalent) within a range of 180 degrees. And it can move up and down (pitch-equivalent) within a range of 90 degrees. This is used to move the camera around so it can scan the surroundings for a ball (in case the ball is not in frame). </p>"},{"location":"win23team6/#motorized-camera-mount","title":"Motorized Camera Mount","text":"<p>We designed a camera mount that is actuated by 2 servos, one controls the camera's pitch angle and the other controls the camera's yaw angle. The mount elevates the camera 5 inches amove the vehicle's mounting plate. It allows the camera to turn and scan for the target ball. </p> <p>https://user-images.githubusercontent.com/58583277/227646168-1071f237-de95-4f92-ad65-a90ee5fe2b01.mp4</p>"},{"location":"win23team6/#how-to-run-the-code","title":"How To Run The Code","text":"<p>One can run python run.py from inside the Jetson Nano mounted on their car. This will load the model and also detect the motor and begin the purported task of finding a basketball. If no basketball is found, it will remain stationary. The 148remote/ folder contains a cool arduino program for proof of concept. It moves the camera mount through its full range of motion in a rhythmic fashion. </p>"},{"location":"win23team6/#vehicle-in-action","title":"Vehicle In Action","text":"<p>Click Here For Video</p>"},{"location":"win23team6/#future-improvements","title":"Future Improvements","text":"<p>We can use the Jetson to control the servo motors through the Arduino. We can account for the yaw angle of the camera mount and add that to the steering, so that the car can steer toward targets that is not in the field of view of the camera when the camera is pointing straight forward. We can also improve the precision of the ball-recognition model by using more pictures of the ball to train the model. </p>"},{"location":"win23team6/#team-6-wall-e-used-to-be-cyclops","title":"Team 6: Wall-E (used to be Cyclops)","text":"<ul> <li>Saathvik Dirisala (Data Science)</li> <li>Victor Chen (Computer Engineering)</li> <li>Yang Song (Mechanical Engineering) </li> </ul>"},{"location":"win23team7/","title":"Team 7","text":""},{"location":"win23team7/#ucsd-ecemae-148-2023-winter-team-7","title":"UCSD ECE/MAE-148 2023 Winter Team 7","text":""},{"location":"win23team7/#team-members","title":"Team Members","text":"<p>Francisco Downey (BENG), Jonathan Xiong (ECE), Nicholas Preston (MAE), Karthik Srinivasan (MAE)</p>"},{"location":"win23team7/#final-project-overview","title":"Final Project Overview","text":"<p>For our final project, we made our car drive from point A to B, given starting and ending GPS points. </p>"},{"location":"win23team7/#assembled-car-design","title":"Assembled Car Design","text":"<p> LIDAR was set in the front of the car. This was an easy decision for the team since we cared for varying obstacles crossing the points that comprise the path from Point A to Point B. Essentially, it was only important to care for obstacles that come into the front of the moving car. GNSS was secure near the center of the car with the antenna placed high and toward the rear.</p>"},{"location":"win23team7/#donkeycar-3-autonomous-laps","title":"DonkeyCar - 3 Autonomous Laps","text":"<p>https://user-images.githubusercontent.com/103704890/227807530-35ed2ea5-2bc0-4b8b-983d-ca6bed659390.mp4</p>"},{"location":"win23team7/#ros2-line-following-3-autonomous-laps","title":"ROS2 Line Following - 3 Autonomous Laps","text":"<p>https://user-images.githubusercontent.com/103704890/227801950-dfd60ddd-300e-4af8-9878-c62338184269.mp4</p>"},{"location":"win23team7/#ros2-left-lane-3-autonomous-laps","title":"ROS2 Left Lane - 3 Autonomous Laps","text":"<p>https://user-images.githubusercontent.com/103704890/227804039-c1aafb7a-8bf6-4f86-89cf-1fa21c7ea5d2.mp4</p>"},{"location":"win23team7/#gps-3-autonomous-laps","title":"GPS - 3 Autonomous Laps","text":"<p>https://user-images.githubusercontent.com/103704890/227804055-d9c3ccce-ef8d-427d-a6b6-e1db12eba440.mp4 Important to note for the final project, many of the configurations that were found to work in the GPS 3 autonomous laps were used for the final project. The GPS points making up the path were 0.55 meters apart. The nature of the csv points used in the final project matched the one used for this assignment. This allowed the project to move faster as the configurations did work out. See section Algorithmic Design of Directing Car from GPS point A to B.</p>"},{"location":"win23team7/#final-project","title":"Final Project","text":"<p>Plan: Go from point A to B with object detection. </p> <p>Overview Originally, we were going to have LIDAR detect objects in front of the car so the car can see what it needs to go around. However, we did not have enough time to see how to use LIDAR, since there was no assignment with it and not enough time at end of quarter given rain. We, however, got quite experienced with the GPS functionality of the car. Using the GPS modules, we were able to direct the car using a path of car-readble GPS coordinates.</p> <p>Algorithmic Design of Directing Car from GPS point A to B By the end of the quarter, we wrote a program, AtoB.py, which requires two sets of GPS coordinates as inputs. This program generates a .csv file containing a path of GPS coordinates about .55 meters apart from point A to B. These generated points latitude longitude formatted and relative to the base station antenna. The inputted absolute (planet's) GPS coordinates, thus, needed to be translated to these relative coordinates and converted to rectangular from polar positions. It took some testing to increase the precision of the translation as we did not know the exact conversion constants.</p> <p>Demonstration</p> <p>https://user-images.githubusercontent.com/103704890/227807009-ec59e649-c068-4543-9dba-9ddb16ea8ee5.mp4</p>"},{"location":"win23team8/","title":"Team 8 Final Project Proposal","text":""},{"location":"win23team8/#team-members","title":"Team Members","text":"<p> - Youssef Georgy | Electrical &amp; Computer Engineering - Rizzi Galibut | Mechanical &amp; Aerospace Engineering - Shuhang Xu | Computer Science - Kavin Raj | Cognitive Science w/ Emphasis in Machine Learning</p> <p>Such a lovely team!!</p>"},{"location":"win23team8/#project-overall","title":"Project Overall","text":"<p>A waiter-bot that takes visual input from the camera, navigates autonomously to different specified locations (i.e., tables) and then back to the starting point. Use image-detection via camera to give robocar a location or GPS coordinates to navigate  to, then use GPS data to plot path there and avoid any obstacles in the way. The robocar will be able to take any location given (provided it\u2019s in range of the network connection) and determine how to get there</p>"},{"location":"win23team8/#physical-setup","title":"Physical Setup","text":""},{"location":"win23team8/#gantt-chart","title":"Gantt Chart","text":""},{"location":"win23team8/#demonstration","title":"Demonstration","text":""},{"location":"win23team8/#using-depthai-for-text-recognition","title":"Using DepthAI for text recognition","text":"<p>Accomplished: - Used DepthAI to enable the camera to detect numbers which are associated with different tables (e.g., 001, 002, etc.) and different CSV files - Originally started by looking at OpenCV and Tesseract for OCR - These are not SpatialAI platforms so accomplishing what we were trying to do would be much harder - Default code provided in DepthAI library launched windows that displayed video stream and words detected, which worked when directly connected to camera through host computer but running code through Jetson would require a container to launch these windows - Needed to find a way to disable them and have camera run in the background Default code also rewrote the variable that contained the decoded text each time it detected something - Had to rewrite and remove sections of the code to stop detection once certain prompts are given - Light conditions really mattered when showing prompt to camera - Worked better during the day and when shown prompt on a backlit-screen (with white background)</p> <p>What did't work as expected: - Camera could read \u201c001\u201d but not \u201c1\u201d, etc., so we decided to use a set of numbers instead - Number recognition is much more accurate than word recognition so we decided to use numbers as the prompt</p>"},{"location":"win23team8/#using-gps-to-record-different-paths","title":"Using GPS to record different paths","text":"<ul> <li>GPS navigation code is based on the USCD donkeycar GPS library</li> <li>We are trying to make it stay in auto-pilot mode by default and reset the origin in the beginning</li> <li>Depend on what text is detected with DepthAI, make it move following the pre-recorded path</li> <li>When it moves back to the origin, stop and start detecting next text</li> </ul>"},{"location":"win23team8/#desired-but-not-accomplished","title":"Desired but not accomplished","text":"<ul> <li>Include the ability to restart the process once the waiter-bot returned to the starting point without having to manually run the script again</li> <li>Have waiter-bot stay at the table for a certain amount of time before returning to the starting point</li> <li>Incorporate the LiDAR so the waiter-bot can avoid obstacles (e.g., students walking past) while traveling to tables</li> <li>Could also be used to detect when the waiter-bot reaches the table and ensure it doesn\u2019t crash into it</li> <li>Have controller inputs programmed in so the waiter-bot can be truly autonomous.  Currently still requires human input</li> </ul>"},{"location":"win23team9/","title":"Team 9","text":"ECE/MAE 148 Winter 2023 Team 9 <p>     JetBuddy     Indoor Delivery Bot based on DepthAI, OpenCV and LiDAR </p> Table of Contents <li>Team Members</li> <li> Hardware and Schematics <ul> <li>Parts</li> <li> Schematics</li> </ul> </li> <li>Final Project</li> <ul> <li>Abstract</li> <li>Part 1: Human Detection and Following with Depthai and PyVesc</li> <li> Part 2: Stopping Mechanism with Lidar</li> <li> Part 3: Facial Recognition</li> <li> Part 4: Spatial Detection with DepthAI</li> </ul> <li>Reflection</li> <ul> <li>Challenges</li> <li> Potential Improvements</li> </ul> <li>Presentation Files</li> <li>Reference</li>"},{"location":"win23team9/#team-members","title":"Team members","text":"<ul> <li>Ben Zhang (ECE)</li> <li>Joseph Katona (ECE) </li> <li>Yichen Yang (ECE)</li> <li>Zijian Wang (MAE)</li> </ul>"},{"location":"win23team9/#hardware","title":"Hardware","text":""},{"location":"win23team9/#parts","title":"Parts","text":""},{"location":"win23team9/#full-assembly","title":"Full Assembly","text":""},{"location":"win23team9/#mounting-plate","title":"Mounting Plate","text":""},{"location":"win23team9/#jetson-case","title":"Jetson Case","text":""},{"location":"win23team9/#camera-lidar-mount","title":"Camera LiDAR Mount","text":""},{"location":"win23team9/#final-project-delivery-box","title":"Final Project Delivery Box","text":""},{"location":"win23team9/#schematics","title":"Schematics","text":""},{"location":"win23team9/#wire-diagram","title":"Wire Diagram","text":""},{"location":"win23team9/#final-project","title":"Final Project","text":""},{"location":"win23team9/#abstract","title":"Abstract","text":"<p>This project aims to develop a delivery system for our robocar that can detect and follow humans while also incorporating a stopping mechanism to prevent collisions. Additionally, the robot will utilize facial recognition to identify individuals and personalize interactions.</p>"},{"location":"win23team9/#part-1-human-detection-and-following-with-depthai-and-pyvesc","title":"Part 1: Human Detection and Following with Depthai and PyVesc","text":"<p>The OAKD camera will be used to detect and track humans in the robot's vicinity. The PyVesc motor controllers will then be used to move the robot in the direction of the detected human.</p>"},{"location":"win23team9/#required-components","title":"Required Components","text":"<ul> <li>Tiny-Yolov3 model integrated in DepthAi for object detection</li> <li>PyVesc Python package for robocar control</li> </ul>"},{"location":"win23team9/#algorithm-workflow","title":"Algorithm Workflow","text":"<ul> <li>Use Tiny-Yolov3 to detect the bounding box of the person in the OAKD camera's field of view.</li> <li>Determine the position of the person by finding the central line of the bounding box, and denote the x-axis value as x0.</li> <li>Calculate the error between the central line of the frame (416x416 pixels), e = x - x0.</li> <li>Calculate the steering value using the formula: v = (Kp * e) / 416 + 0.5, where Kp = 1.</li> <li>Use PyVesc to control steering by calling vesc.set_servo(v).</li> </ul>"},{"location":"win23team9/#additional-settings","title":"Additional Settings","text":"<ul> <li>Use vesc.set_rpm() to run the car once it detects people.</li> <li>The steering value is sampled at a rate of 5Hz to prevent frequent drifting.</li> </ul>"},{"location":"win23team9/#part-2-stopping-mechanism-with-lidar","title":"Part 2: Stopping Mechanism with Lidar","text":"<p>The Lidar sensor will be used to detect obstacles in the robot's path. If an obstacle is detected, the robot will stop moving and wait for the obstacle to clear before continuing on its path.</p>"},{"location":"win23team9/#the-lidar-on-this-robot-aim-to","title":"The LiDAR on this robot aim to","text":"<ul> <li>Detect anything that is in a close range</li> <li>If the position is too clase, the robot will stop to avoid collision</li> <li>The robot will back up after it stop for a while and still detect obstacle is close</li> <li>Transform raw binary data from LiDAR to numerical data through BinASCII library</li> </ul>"},{"location":"win23team9/#how-to-read-lidar","title":"How to read LiDAR?","text":"<ul> <li>Each measurement data point of LiDAR is consists of a distance value of 2 bytes and a confidence of 1 byte</li> <li>We transform this data through chopping it to bytes and translate it.</li> <li> <p>We get the angle by getting the start angle and end angle.</p> </li> <li> <p>Putting all the distance into a list and it will stop the car if there\u2019s an object within certain distance that LiDAR detected.</p> </li> </ul> <p></p>"},{"location":"win23team9/#part-3-facial-recognition","title":"Part 3: Facial Recognition","text":"<p>The robot will be equipped with a facial recognition system, using a webcam, that will allow it to identify individuals and personalize interactions. Once it recognizes the right person, the delivery box will open. The facial recognition software uses a simple python import of facial_recognition. In the facial_recognition library all we do is use openCV to capture images for the frames and use facial_recognitions \"matching\" function to to add a box around the persons face. In our case when this value is detected over an interval then a true value is then sent to the box to open.</p> <p></p>"},{"location":"win23team9/#part-4-spatial-detection-with-depthai","title":"Part 4: Spatial Detection with DepthAI","text":"<p>Utilizing Depthai's pipeline system we take their spatial location pipeline to simply calculate the distance of individual from the camera. The Object detection pipeline detects a person and creates a bounded box, then with the x and y coordinates from the bounded box we can pinpoint where we want the camera to point. After these coordinates are gathered the z location is stored in a circular list. This is because the bounded box and tracker of object distance aren't always in sync so some erroneous values are given. Once we have around 50 samples then we take the average to get a good idea of what the distance of the person from the car is. Finally we utilize pyvescs set_rpm() features to give out a more smooth acceleration system. So, basically if you're far away the robot will speed up and slow down as it moves closer to you.  Get more info on Spatial Depth here </p>"},{"location":"win23team9/#gantt-chart","title":"Gantt Chart","text":""},{"location":"win23team9/#demonstrations","title":"Demonstrations","text":"<p>The Video might not show up, please go to img folder for full demo.</p> <p> </p>    Your browser does not support HTML video."},{"location":"win23team9/#reflection","title":"Reflection","text":""},{"location":"win23team9/#challenges","title":"Challenges","text":"<ul> <li>Getting everything to work together<ul> <li>Different libraries working together and all send signals to PyVESC</li> <li>Everything worked fine on a local machine but when running on the Jetson, crashes would occur</li> </ul> </li> <li>Scope of the original idea<ul> <li>Mapping the path for future references using SLAM</li> </ul> </li> <li>Depth ai pipeline caused crashes<ul> <li>X-Link Problem(Serial bus issues)</li> </ul> </li> <li>Translate raw LiDAR output to data we need</li> <li>Making the car look smooth</li> <li>Better algorithm to adjust speed(rpm)</li> </ul>"},{"location":"win23team9/#potential-improvements","title":"Potential Improvements","text":"<ul> <li>Implement all the features together flawlessly<ul> <li>Currently cannot run together good due to delay from different components</li> </ul> </li> <li>Get the locking mechanism working<ul> <li>Locking mechanism to make sure the right receiver get the package</li> </ul> </li> <li>LiDar also scans the path for future path planning<ul> <li>Trying to find a person if it cannot detect anything</li> </ul> </li> </ul> <p>Maybe try different frameworks since we can use different libraries without limitation in ROS or donkeycar</p>"},{"location":"win23team9/#presentations","title":"Presentations","text":"<li>Project Proposal &amp; Progress Report</li> <li>Final Presntation</li>"},{"location":"win23team9/#reference","title":"Reference","text":"<p>We would like to give special thanks to:</p> <ul> <li>Professor Jack Silberman</li> <li>TA Moises Lopez</li> <li>TA Kishore Nukala</li> <li>All The teams that helped us on the way</li> </ul>"},{"location":"win23team12/","title":"ECE 148 Winter 2023 Team 12 Final Project","text":""},{"location":"win23team12/#team-members","title":"Team members","text":"<ul> <li>Jake Kindley (ECE)</li> <li>Yiteng Zhao (ECE)</li> <li>Noah Jones (MAE)</li> </ul>"},{"location":"win23team12/#overview","title":"Overview","text":"<p>We want to create a sorter bot similar to warehouse bots that picks up a package, follow the path with certain color and delivers it to designated dropoff location based on the labels on that package. We plan to put AR tags as labels on the package, and each AR tag is mapped to an id associated with either the color of the lane our bot should follow or stop action when it reaches the designated dropoff zone. At dropoff zone, the bot will perform a series of maneuver and return to the starting point.</p>"},{"location":"win23team12/#robot-design-implementation","title":"Robot Design &amp; Implementation","text":""},{"location":"win23team12/#software","title":"Software","text":"<p>Our code is running on ROS2 and is modified based on the provided lane following code from class. The following flowchart shows the original relationship between each component in the provided lane following code from class: </p> <p>We added a node responsible for detecting AR tag, read states from AR tag, and changing lane detector's behavior. </p> <p>In our new node, we have defined two types of AR tags:</p> <ul> <li>Type 1: Declares the lane color our bot should follow</li> <li>Type 2: Stop signal that indicates the bot to drop the package</li> </ul> <p>The default behavior when no AR tag is being detected is following blue lane until a type 1 tag is detected.</p> <p></p>"},{"location":"win23team12/#hardware","title":"Hardware","text":"<p>We designed a bracket as package holder that snaps on the front bumper of the car and holds one package:</p> <p></p> <p></p>"},{"location":"win23team12/#showcase","title":"Showcase","text":""},{"location":"win23team12/#assembled-robots","title":"Assembled Robots","text":""},{"location":"win23team12/#final-project-demo","title":"Final Project Demo","text":""},{"location":"win23team12/#remarks","title":"Remarks","text":"<p>Our video demonstrated our bot's capability of detecting AR tag, selecting the corresponding lane color, following path to dropoff zone, dropping off package in front of stop sign, and return to the starting point. Potential improvements for this project includes adding obstacle avoidance, redesigning the package holder to hold the package above ground, adjusting camera position for better view angle of the lane, and adding capability to navigate through multiple junctions.</p>"},{"location":"win23team13/","title":"Team 13","text":"Final Project: Robot Mapping Team 13: Girish, Muhammad, Andy, and Van ECE MAE 148, Winter 2023  Welcome to the project report for Team 13! This page contains a report of all the progress we made throughout this busy and fun quarter, including our final project.  |![](/images/car.jpg)| ![](/images/car2.jpg)| |---|---|  *Team 13's assembled RC Car with the lidar placed at the front.*  ## Table of Contents  - [Table of Contents](#table-of-contents) - [The Team](#the-team) - [Final Project Abstract](#final-project-abstract) - [Hardware Setup](#hardware-setup)     - [Base Plate](#base-plate)     - [Camera Mount](#camera-mount)     - [Jetson Nano Case](#jetson-nano-case)     - [Electronics Circuit Diagram](#electronics-circuit-diagram) - [Software Documentation](#software-documentation) - [Autonomous Laps](#autonomous-laps) - [Acknowledgements](#acknowledgements) - [Credit and References](#credit-and-references)  ## The Team   |![](/images/girish.jpeg)|![](/images/muhammad.png)|![](images/andy.png)|![](/images/van.jpeg)| |---|---|---|---| |**Girish Krishnan** [[LinkedIn](https://linkedin.com/in/girk)]|**Muhammad Bintang Gemilang** |**Andy Zhang** |**Zhengyu (Van) Huang** | |Electrical Engineering|Mechanical Engineering|Electrical Engineering|Computer Engineering|  ## Final Project Abstract   Our final project was themed around **mapping an unknown environment**. Our project involved the following tasks.  __What we promised__  * [\u2714] To implement SLAM (Simultaneous Localization and Mapping) using a lidar. This effectively creates a map of the environment around the robot, showing the locations of all objects present. * [\u2714] To display the map generated from SLAM in real-time using a web application.  The __challenges faced__ during the project were:  * Integrating the web application for live previewing (HTML/CSS/JS) with the Python code needed to run SLAM. * Avoiding delays in the updating map.  The __accomplishments__ of the project were:  * We were able to achieve a decent visualization that updates over time as the robot is driven around * The visualization/map can be saved easily for tasks such as path planning.  __Final Presentation__  * *[Link to Final Presentation](https://docs.google.com/presentation/d/1ybNZCItvh3Inb4xyIm9jMdvE7QkdUHSAgcw-_27GDpM/edit?usp=sharing)*  * *[Link to video showing real-time mapping](https://youtu.be/89NYezgTyDc)*  __Weekly Update Presentations__  * [Project Proposal](https://docs.google.com/presentation/d/1BA-ZTRFRMCwjRi_ehfDn4Fll8nvdWeSgA-vcFacIhXk/edit?usp=sharing) * [Week 8](https://docs.google.com/presentation/d/1aqbrDsI9-qD3Cdtn5Y08WQpj--RP-w43sHdjfP7Nmr0/edit?usp=sharing) * [Week 9](https://docs.google.com/presentation/d/1mEIa4phqNgUHyFd5yaW6FJG1RRwc04al4UiSqWt_TiU/edit?usp=sharing) * [Week 10](https://docs.google.com/presentation/d/1PwtRmnkmlwk-wUWYZGFs0CjX7lSMBmuzCTyH2MfmV7g/edit?usp=sharing)  __Gantt Chart__  ![](/images/gantt.png)  ## Hardware Setup   * __3D Printing:__ Camera Mount, Jetson Nano Case, GPS (GNSS) Case. * __Laser Cutting:__ Base plate to mount electronics and other components.  __Parts List__  * Traxxas Chassis with steering servo and sensored brushless DC motor * Jetson Nano * WiFi adapter * 64 GB Micro SD Card * Adapter/reader for Micro SD Card * Logitech F710 controller * OAK-D Lite Camera * LD06 Lidar * VESC * Anti-spark switch with power switch * DC-DC Converter * 4-cell LiPo battery * Battery voltage checker/alarm * DC Barrel Connector * XT60, XT30, MR60 connectors  *Additional Parts used for testing/debugging*  * Car stand * USB-C to USB-A cable * Micro USB to USB cable * 5V, 4A power supply for Jetson Nano  #### Base Plate   ![](/images/base_plate.png)  *All measurements shown above are in millimeters (mm)*  Our base plate was laser cut on a thick acrylic sheet. The circular hole at the end of the base plate is meant to hold the power on/off button. The long holes in the side of the plate are meant for wires to easily pass to and from the bottom of the plate.  #### Camera Mount   |![](/images/camera_mount.png)|![](/images/camera_mount_base.png)| |---|---| |Camera Holder|Base for attachment to base plate|  The two parts of the camera mount shown above were screwed together. The angle of the camera was carefully chosen (facing downward approximately 10 degrees from the vertical) so that the road ahead is clearly visible. This is essential for accurate results in OpenCV/ROS2 autonomous laps.  One of our older camera mount designs is shown below.  |![](/images/camera_mount_rotate.png)| |---|  This camera mount consists of three parts: one base for attachment to the base plate, one middle piece to connect the base and the camera, and the camera holder. This camera mount design allows you to rotate the camera up and down. However, it is important that the rotating hinge is screwed securely so that the hinge doesn't wobble out while the robot does autonomous laps!  #### Jetson Nano Case   *Credit to flyattack from Thingiverse, see: https://www.thingiverse.com/thing:3532828*   |![](/images/jetson_case.jpeg)| |---|  This case is excellent because it is robust and doesn't break easily, unlike most common Jetson Nano cases.  #### Electronics Circuit Diagram  ![](/images/schematic.png) ![](/images/circuit.png)  *Note: some of these components and connections will vary depending on the exact components you have - check the component specifications carefully.*  ## Software Documentation   To install all the necessary Python modules needed, run the following on the Jetson Nano.   <pre><code>pip install -r requirements.txt\n</code></pre>   For our final project, we implemented a real-time visualization system for the Hector SLAM algorithm implemented using the lidar sensor. The base code for the SLAM algorithm is accessible in the Docker container provided to us in class, and the code for the real-time implementation is present in the **slam_gui** folder of this repository.  The SLAM real-time visualization GUI that we built has the following features:  * A web application whose routes are made using FastAPI in Python. Uvicorn is used to run the web server. * HTML and JS to update the map in real-time * The HTML and JS is interfaced with Python, ROS1, ROS2 and ROSBridge, so that the data collected is displayed on the web app. * The interfacing process is difficult to implement directly in Python, so we use *subprocessing* to call relevant bash scripts that handle the processes in ROS1 and ROS2. These subprocesses are made to run in parallel using *threading* in Python.  To run the visualizer, first open up a docker container containing the ucsd_robocar ROS packages.  Run the following:   <pre><code>cd slam_gui\npython slam_map.py\n</code></pre>   This sets up the web app running on the Jetson Nano (although the app could potentially be run on any device, provided it can communicate with the Jetson Nano using the relevant ROS topics).  Opening up the web app on **http://localhost:8000** reveals the GUI showing the results of SLAM in real-time. The code in *slam_map.py* can be adjusted to fine-tune the time-delay that occurs as the map updates.  ![](/images/project_prev.png)  __Additional Scope for the Final Project__  Although SLAM is useful for mapping an unknown environment, it can be useful to integrate GPS data with SLAM to provide better location accuracy. To implement this in Python, we created the folder **gps_slam** that contains starter code with lidar, PyVESC, and GPS implementation and a basic SLAM algorithm with the Kalman filter (implemented using the filterpy library in Python). This additional, nice-to-have part of the project hasn't been tested out yet, but we plan to get it working soon.  ## Autonomous Laps   As part of the class deliverables and as preparation for the final project, here are our autonomous laps videos:  __Donkey Sim__  * Local Computer: https://youtu.be/lXEStSEVikQ * GPU training: https://youtu.be/4_BzKP9-XAQ * External Server: https://youtu.be/Yvo1yqRJhX4  __Physical Robot__  * DonkeyCar: https://youtu.be/bPUSS2g0Ves * Lane detection using OpenCV + ROS2: https://youtu.be/omcDCBSrl2I * Inner lane: https://youtu.be/9hN8HUlGcas * Outer lane: https://youtu.be/nXZNPscVlX0 * GPS: https://youtu.be/Y3I9AWW1R6o  ## Acknowledgements   Thanks Prof. Jack Silberman and TAs Moises Lopez and Kishore Nukala for an awesome quarter! See you in DSC 178 next quarter, professor Jack ;)  ## Credit and References  * Jetson Nano Case Design: https://www.thingiverse.com/thing:3532828  * Lidar (LD06) Python Tutorial: https://github.com/henjin0/LIDAR_LD06_python_loder * PyVESC: https://github.com/LiamBindle/PyVESC * SLAM tutorial, Dominic Nightingale. https://gitlab.com/ucsd_robocar/ucsd_robocar_nav1_pkg/-/tree/master/"},{"location":"win23team14/","title":"Team 14","text":"<p>ECE - MAE 148 Team 14 Winter 2023 Repository</p> <p>Project Goal: </p> <p>We used the OAK-D camera to run object detection (within the camera) and track different traffic details (common signs, speed limits, etc.)</p> <p>Software and Hardware Description:</p> <p>The OAK-D and depthAI are AI-enabled stereo cameras that allow for depth perception and 3D mapping. They are powerful tools for computer vision applications, such as object detection and tracking. The depthAI is a board that enables faster processing of images by offloading the computational workload from the main processor to dedicated hardware.  YOLO (You Only Look Once) is a real-time object detection system that is capable of detecting objects in an image or video feed. It is a popular deep learning model that is used in many computer vision applications, including self-driving cars and robotics. PyVesc is a Python library for controlling VESC-based motor controllers. The VESC is an open-source ESC (Electronic Speed Controller) that is widely used in DIY robotics projects. PyVesc allows for easy communication with the VESC and provides an interface for setting motor parameters and reading sensor data.</p> <p>Project Overview:</p> <p>We first used the OAK-D and depthAI to detect stop signs in the robot's field of view. Then, we executed the deep learning model YOLO to process the camera feed and identify the stop sign(text detection can be another method to achieve the same function). Once the stop sign is detected, we implemented PyVesc to send a command to the motor controller to stop the robot and started to set up the OAK-D and depthAI cameras by installing the necessary software libraries. YOLO is capable of detecting multiple objects simultaneously, so we needed to filter out the stop sign from other detected objects. However, we needed a blob converter to take different data types and convert them into a Binary Large Object (BLOB) that could fit in our code. Finally, once the stop sign is detected, we accessed PyVesc to send a command to the motor controller to stop the robot. In summary, the integration of OAK-D, depthAI, YOLO, and PyVesc allows for efficient and accurate stop sign detection and safe stopping of the robot. This implementation can be further customized and optimized for specific robotic platforms and use cases.</p> <p>Final Projet Presentation: </p> <p>https://docs.google.com/presentation/d/1BTMwfktHvDzfzYd6oSnHeaQTEBbtYg8wEMnqoWkamHE/edit?usp=sharing</p> <p>Final Project Video:</p> <p>https://drive.google.com/file/d/1OnO5qWczQbH_aVrgKAtejwLMw6-fFSRW/view?usp=share_link</p> <p>Team Members: Anish Kulkarni, Manuel Abitia, Zizhe Zhang.</p> <p>Special Thanks to: Professor Silberman, Kishore, Moises, and Freddy C. the Robot.</p>"},{"location":"win23team15/","title":"Final Project Repository for Team 15 of the 2023 Winter Class MAE ECE 148 at UCSD","text":"<p>Our Final Project uses the AI controlled autonomous vehicle developed in early course sections to implement a driving protocol based on hand signals. To do this, we utilize the GPS-tracking and following code developed/given in class, and combined it with the DepthAI gesture recognition software pack. Our idea developed from our interest in the OAK-D Lite camera\u2019s stereo vision system and ability to run DepthAI within the camera for processing. To use both of these functions, we chose to run a hand detection program that uses Google Mediapipe to combine these features in a unique project that no one on our team had tried before. Using this bleeding edge hardware was really interesting and showed the potential in the given components and also in general the viability of gesture-based control even in relatively low-cost projects. </p> <p>Below are examples of how our hand detection tracker will recognize our gestures </p> <p> </p>"},{"location":"win23team15/#car-physical-setup","title":"Car Physical Setup","text":"<p>Our car setup used a large piece of acrylic to connect across the RC car strut towers to support all of our electronics. The acrylic had two grooves along the entire length that were 3 inches apart, allowing us to reconfigure our electronics layout as the class progressed. We knew we would be given different devices through the length of the class, so this modular setup gave us the ability to adapt to them quickly rather than redesigning everytime. We also made heave use of 3D printing, which further cemented the viability of our acrylic rail mount system, since we just had to design a mount that could take mounting screws three inches apart. This enabled us to quickly and with low effort include different configurations and components. We then further used 3D-printed components to functionalize parts of our design, such as a height-adjustable camera mount, and a lidar mount employing the same functionality.</p> <p> </p> <p>Our camera setup was originally a rigid mount on the front of the car, but as we tested our initial Donkey Car laps on the outdoor track we recognized the need for adjusting our angle to detect the lines better. This led to our tall hinging mount that gave us better viewing angles that we could set up within seconds for testing.</p> <p> </p>"},{"location":"win23team15/#software","title":"Software","text":"<p>In total, we used three different software packages either from discord, or developed in the course of the class, those beeing the depthai software pack, the d5 GPS control system and the donkeycar Software.</p>"},{"location":"win23team15/#depthai-software-pack","title":"depthai software pack","text":"<p>To detect hand gestures, we use the Luxonis DepthAI Software on the Luxonis OAK-D stereo-vision camera. This software pack brings plenty of functionality, from simple hand tracking, two two-handed interactions, all the way to gesture recognition. We then modified the existing codebase to suit our needs, including specialized code to regognize a set of largely custom, non-included hand gestures:  - Thumbs-up gesture to activate driving - Held up flat hand as a stop signal  - different number of fingers (with the thumb not pointing outwards) changes the speed in a four level control scheme - pointing the index finger with thumb outstreched left or right to change direction of steering </p>"},{"location":"win23team15/#d5-gps-control","title":"d5 GPS-control","text":"<p>In the d5 subdirectory, we used the donkeycar interfaces to program a simple GPS training and following routine to generate a path for the car to follow. This part of the project is still partly in development, as currently only the speed can be varied with hand signals. We largely reused tech generated and tought to us in the course of the class, providing a nice framework on which to test our system.</p>"},{"location":"win23team15/#donkeycar-software","title":"DonkeyCar Software","text":"<p>We use the preconfigured DonkeyCar software package to facilitate control and interfacing of the Jetson nano embedded system with the rest of the RC car. We implemented a special interrupt in the signal chain to be able to pass in different speed values, which are first stored by the modified DepthAI software pack into the file finalproject/comm.txt as a simple string command corresponding to the recognized hand sign. which is then read into the DonkeyCar VESC control subroutine to change the throttle values dynamically. </p>"},{"location":"win23team15/#operation","title":"Operation","text":"<p>One can either run the depthai software on its own with the VESC class implemented right in the file demo_bdf.py by just executing it with all lines commented in, or, if it is desired to just utilize the throttle values, comment out the VESC related code, run the demo_bpf.py and then simultaneaously run the d5 donkeycar code with <code>./python manage.py</code> drive. Then, steering will be controlled by the GPS following algorithm, while the speed is controlled via the detected hand signals. The follwing youtube videos show the basic functionality of our project, and how it interacts with the complex drivetrain of the car.</p> <p>Basic Command Gesture</p> <p>Throttle Changing Values</p>"},{"location":"win23team15/#team-15-boba-team","title":"Team 15: Boba Team","text":"<ul> <li>Reinwei Bai</li> <li>Manu Mittal</li> <li>Reisandy Lamdjani</li> <li>Moritz Wagner</li> </ul>"}]}