{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ECE/MAE 148: Introduction to Autonomous Vehicles Documentation is under development.","title":"Welcome to ECE/MAE 148: Introduction to Autonomous Vehicles"},{"location":"#welcome-to-ecemae-148-introduction-to-autonomous-vehicles","text":"Documentation is under development.","title":"Welcome to ECE/MAE 148: Introduction to Autonomous Vehicles"},{"location":"win23team2/","text":"Team 2 ECE/MAE 148 Final Report :wave: The Team: 2 Fast 2 Furious (Left to Right) - Elias Fang (CSE) - Ainesh Arumugam (ECE) - Matthew Merioles (ECE) - Junhao \"Michael\" Chen (MAE) \ud83d\udcdd Project Overview Mario Kart in real-life? That's basically what we did. We designed a boost system similar to those in Mario Kart, where detecting colored pieces of paper on the track can change throttle for a short period of time. Depending on the color of the \"pad\" the car drives over, it will either speed up, slow down, or stop for a few seconds, just like in the game! \ud83c\udfce Our Robot Bird's Eye Front Left Right Back Schematic \ud83c\udf44 Final Project What We Promised Must haves [X] Distinguishing different colors through the camera [X] Adjust the throttle based on the color Nice to haves [X] Have the car detect a flat piece of paper on the track (like a booster pad) [ ] Combine with lane-following algorithm Gantt Chart https://sharing.clickup.com/9010060626/g/h/8cgn7aj-87/769d44f22562beb What We Accomplished Color Detection Used OpenCV for color detection and edge tracing Used color mask algorithm to detect proportion of frame that color takes up Detected multiple colors at the same time Determined HSVs for orange, pink, and blue Demo PyVESC Connection through external webcam Different RPM values are sent through PyVesc to achieve different speed for different colors marked by different states: Blue (Boost) = speed up for 3 sec Pink (Slow)= slow down for 3 sec Orange (Stop) = stop for 3 sec Neutral (Normal) = constant rpm Blue Demo Pink Demo Orange Demo Presentation https://docs.google.com/presentation/d/1oJPRLYIKvHUXEIK9hoYpPFoFAyHuG6sE7ZrU9NQPG8g/edit?usp=sharing Code https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart.py Possible Future Work Change the colored paper into Mario Kart items (mushroom, bananas, etc.) for the car to identify Allow the car to run autonomously on a track and still apply speed changes Race with other teams \ud83d\ude09 \ud83c\udfc1 Autonomous Laps DonkeyCar ROS2 Line Following ROS2 Lanes GNSS Additional Work Done Our team was tasked with implementing a depth feature with our contour function, using depthAI to integrate with the OAK-D Camera! Our updated code is now able to measure how far the contoured object is, measured in cm! [Depth Demo] https://www.youtube.com/watch?v=NYIz7--TpgY [Updated Code] https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart_depth.py Acknowledgements Thanks for Professor Jack Silberman, TA Kishore Nukala, and TA Moises Lopez!","title":"Team 2"},{"location":"win23team2/#team-2-ecemae-148-final-report","text":"","title":"Team 2 ECE/MAE 148 Final Report"},{"location":"win23team2/#wave-the-team-2-fast-2-furious","text":"(Left to Right) - Elias Fang (CSE) - Ainesh Arumugam (ECE) - Matthew Merioles (ECE) - Junhao \"Michael\" Chen (MAE)","title":":wave: The Team: 2 Fast 2 Furious"},{"location":"win23team2/#project-overview","text":"Mario Kart in real-life? That's basically what we did. We designed a boost system similar to those in Mario Kart, where detecting colored pieces of paper on the track can change throttle for a short period of time. Depending on the color of the \"pad\" the car drives over, it will either speed up, slow down, or stop for a few seconds, just like in the game!","title":"\ud83d\udcdd Project Overview"},{"location":"win23team2/#our-robot","text":"","title":"\ud83c\udfce Our Robot"},{"location":"win23team2/#birds-eye","text":"","title":"Bird's Eye"},{"location":"win23team2/#front","text":"","title":"Front"},{"location":"win23team2/#left","text":"","title":"Left"},{"location":"win23team2/#right","text":"","title":"Right"},{"location":"win23team2/#back","text":"","title":"Back"},{"location":"win23team2/#schematic","text":"","title":"Schematic"},{"location":"win23team2/#final-project","text":"","title":"\ud83c\udf44 Final Project"},{"location":"win23team2/#what-we-promised","text":"","title":"What We Promised"},{"location":"win23team2/#must-haves","text":"[X] Distinguishing different colors through the camera [X] Adjust the throttle based on the color","title":"Must haves"},{"location":"win23team2/#nice-to-haves","text":"[X] Have the car detect a flat piece of paper on the track (like a booster pad) [ ] Combine with lane-following algorithm","title":"Nice to haves"},{"location":"win23team2/#gantt-chart","text":"https://sharing.clickup.com/9010060626/g/h/8cgn7aj-87/769d44f22562beb","title":"Gantt Chart"},{"location":"win23team2/#what-we-accomplished","text":"","title":"What We Accomplished"},{"location":"win23team2/#color-detection","text":"Used OpenCV for color detection and edge tracing Used color mask algorithm to detect proportion of frame that color takes up Detected multiple colors at the same time Determined HSVs for orange, pink, and blue Demo","title":"Color Detection"},{"location":"win23team2/#pyvesc","text":"Connection through external webcam Different RPM values are sent through PyVesc to achieve different speed for different colors marked by different states: Blue (Boost) = speed up for 3 sec Pink (Slow)= slow down for 3 sec Orange (Stop) = stop for 3 sec Neutral (Normal) = constant rpm Blue Demo Pink Demo Orange Demo","title":"PyVESC"},{"location":"win23team2/#presentation","text":"https://docs.google.com/presentation/d/1oJPRLYIKvHUXEIK9hoYpPFoFAyHuG6sE7ZrU9NQPG8g/edit?usp=sharing","title":"Presentation"},{"location":"win23team2/#code","text":"https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart.py","title":"Code"},{"location":"win23team2/#possible-future-work","text":"Change the colored paper into Mario Kart items (mushroom, bananas, etc.) for the car to identify Allow the car to run autonomously on a track and still apply speed changes Race with other teams \ud83d\ude09","title":"Possible Future Work"},{"location":"win23team2/#autonomous-laps","text":"DonkeyCar ROS2 Line Following ROS2 Lanes GNSS","title":"\ud83c\udfc1 Autonomous Laps"},{"location":"win23team2/#additional-work-done","text":"Our team was tasked with implementing a depth feature with our contour function, using depthAI to integrate with the OAK-D Camera! Our updated code is now able to measure how far the contoured object is, measured in cm! [Depth Demo] https://www.youtube.com/watch?v=NYIz7--TpgY [Updated Code] https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart_depth.py","title":"Additional Work Done"},{"location":"win23team2/#acknowledgements","text":"Thanks for Professor Jack Silberman, TA Kishore Nukala, and TA Moises Lopez!","title":"Acknowledgements"},{"location":"win23team3/","text":"Final Report for Team 3 (ECE/MAE 148) Team Members Nathaniel Barnaby - ECE Yang-Jie Qin - ECE Cheuk Hin Bryan Cheng - MAE Patrick Nguyen - MAE Project Overview Our initial final project was a combination of the usage of an IMU and GNSS to implement position tracking. An IMU is an inertial measurement unit, composing of gyros, accelerometers, and more sensors to track the movement of something. With these sensors, the car's location can be estimated off of a general initial GPS location with the addition to its movement measured by its speed, acceleration, turning, etc. This ended up being too complex for our team which resulted in little progress. We were then assigned new mini tasks which consists of using 2 of the sensors provided in our kits. The assignment was to use the OAK-D camera and the lidar separately to measure depth of both a small (0.15m) object and a larger (0.5m) object at different distances. We ended up comparing results of both objects at distances of 0.5, 1, 2, 3, and 4 meters. We would then compare the outputed values from the sensors to what the actual correspond measurment. A comparison between the accuracy of depth finding between the Oak-D camera and lidar would also be necesasry. A second task was assigned which was to output a face recognition system out of the OAK-D camera. Results For the distance measurement assignment, both the camera and lidar were able to successfully measure distance for the small the large object at the different ranges. For the camera, it was accurate at determining smaller distances, but at larger distances (3+ meters) error seemed to begin growing exponentionally. The difference between small and large objects was negligible as long as the area in which the distances were averaged fit within the object, which at further distances can start causing fluctuations with smaller objects. For the lidar, it was accurate at determining all distances with a linear or almost static amount of error. At smaller distances this was larger than the error of the camera, but at larger distances it vastly outperformed the camera due to the nature of the camera's exponential error. Additionally, the lidar had to be hand-calibrated, so with more time the error could have been lowered due to this effect. Also, since we recorded distance measurements within a range of 0.4 degrees, the measurement would be inaccurate with a smaller object at longer distances. This could be overcame by decreasing the range and waiting longer. For most scenarios the lidar seems to be the winning choice for distance measurement. While at lower distances the camera seemed to outperform the lidar, the lidar seems to be more consistent with its measurements than the camera. Additionally, the lidar offers 360 degrees of distance measurements while the camera only works in one direction. DepthAI Distance Measurement with ~0.5 m^2 Object Video Link DepthAI Distance Measurement with 0.15 m^2 Box Video Link LiDAR Distance Measurement with ~0.5 m^2 Object Video Link LiDAR Distance Measurement with 0.15 m^2 Box Video Link As for the camera face recognition, we were succesfully able to output video display which recognizes faces. This is a relatively fast responding system. It outputs the number of faces it recognizes which we tested from 0-3 faces real time. There is some error within the system as it can be innacurate thinking other shiny objects and or parts of a face are another face. It is also not limited to stationary faces as it recognizes people moving too. Face Recognition Video Link Gantt Chart Hardware: Mechanical Design \\ Camera/flashlight Mount \\ Electronics Tray \\ Front/rear Electronics Plate Offset \\ GPS Mount \\ IMU Mount \\ Jetson Case Key Mount \\ Jetson Nano Main Case \\ Lidar Tower \\ Servo Voltage Converter \\ Vesc Power Distributor Previous Designs Electronic Components \\ Jetson Nano \\ OAK-D Camera \\ Lidar LD06 Electronic Wiring Schematic Final Set Up Bird's Eye View Left View Right View Packages and Drivers cv2 (OpenCV) depthai (DepthAI) numpy math binascii Milestones Face Recognition using DepthAI - Detects faces through a webcam and displays a count in the terminal Distance Measurement using DepthAI - Using the disparity between the left and right cameras of the OAKD, distance can be calculated. This was averaged over an area to give an estimated distance of an object. Distance Measurement using LiDAR - Using a LiDAR is is relatively simple to detect distances in a 360 degree range. By averaging distances over a very small range (0.4 degrees) we determined the distance of an object. Potential Future Work/Unaccomplished Goals Recognizing and labeling specific faces Running code off of the OAK-D Camera instead of needing an external computer to run the code. Presentations - Final Project Proposal - Final Presentation Acknowledgments Professor Jack Silberman, TA Kishore Nukala, Moises Lopez-Mendoza, Design and Innovation Building, all of our wonderful classmates","title":"Team 3"},{"location":"win23team3/#final-report-for-team-3-ecemae-148","text":"","title":"Final Report for Team 3 (ECE/MAE 148)"},{"location":"win23team3/#team-members","text":"Nathaniel Barnaby - ECE Yang-Jie Qin - ECE Cheuk Hin Bryan Cheng - MAE Patrick Nguyen - MAE","title":"Team Members"},{"location":"win23team3/#project-overview","text":"Our initial final project was a combination of the usage of an IMU and GNSS to implement position tracking. An IMU is an inertial measurement unit, composing of gyros, accelerometers, and more sensors to track the movement of something. With these sensors, the car's location can be estimated off of a general initial GPS location with the addition to its movement measured by its speed, acceleration, turning, etc. This ended up being too complex for our team which resulted in little progress. We were then assigned new mini tasks which consists of using 2 of the sensors provided in our kits. The assignment was to use the OAK-D camera and the lidar separately to measure depth of both a small (0.15m) object and a larger (0.5m) object at different distances. We ended up comparing results of both objects at distances of 0.5, 1, 2, 3, and 4 meters. We would then compare the outputed values from the sensors to what the actual correspond measurment. A comparison between the accuracy of depth finding between the Oak-D camera and lidar would also be necesasry. A second task was assigned which was to output a face recognition system out of the OAK-D camera.","title":"Project Overview"},{"location":"win23team3/#results","text":"For the distance measurement assignment, both the camera and lidar were able to successfully measure distance for the small the large object at the different ranges. For the camera, it was accurate at determining smaller distances, but at larger distances (3+ meters) error seemed to begin growing exponentionally. The difference between small and large objects was negligible as long as the area in which the distances were averaged fit within the object, which at further distances can start causing fluctuations with smaller objects. For the lidar, it was accurate at determining all distances with a linear or almost static amount of error. At smaller distances this was larger than the error of the camera, but at larger distances it vastly outperformed the camera due to the nature of the camera's exponential error. Additionally, the lidar had to be hand-calibrated, so with more time the error could have been lowered due to this effect. Also, since we recorded distance measurements within a range of 0.4 degrees, the measurement would be inaccurate with a smaller object at longer distances. This could be overcame by decreasing the range and waiting longer. For most scenarios the lidar seems to be the winning choice for distance measurement. While at lower distances the camera seemed to outperform the lidar, the lidar seems to be more consistent with its measurements than the camera. Additionally, the lidar offers 360 degrees of distance measurements while the camera only works in one direction. DepthAI Distance Measurement with ~0.5 m^2 Object Video Link DepthAI Distance Measurement with 0.15 m^2 Box Video Link LiDAR Distance Measurement with ~0.5 m^2 Object Video Link LiDAR Distance Measurement with 0.15 m^2 Box Video Link As for the camera face recognition, we were succesfully able to output video display which recognizes faces. This is a relatively fast responding system. It outputs the number of faces it recognizes which we tested from 0-3 faces real time. There is some error within the system as it can be innacurate thinking other shiny objects and or parts of a face are another face. It is also not limited to stationary faces as it recognizes people moving too. Face Recognition Video Link","title":"Results"},{"location":"win23team3/#gantt-chart","text":"","title":"Gantt Chart"},{"location":"win23team3/#hardware-mechanical-design","text":"\\ Camera/flashlight Mount \\ Electronics Tray \\ Front/rear Electronics Plate Offset \\ GPS Mount \\ IMU Mount \\ Jetson Case Key Mount \\ Jetson Nano Main Case \\ Lidar Tower \\ Servo Voltage Converter \\ Vesc Power Distributor","title":"Hardware: Mechanical Design"},{"location":"win23team3/#previous-designs","text":"","title":"Previous Designs"},{"location":"win23team3/#electronic-components","text":"\\ Jetson Nano \\ OAK-D Camera \\ Lidar LD06","title":"Electronic Components"},{"location":"win23team3/#electronic-wiring-schematic","text":"","title":"Electronic Wiring Schematic"},{"location":"win23team3/#final-set-up","text":"Bird's Eye View Left View Right View","title":"Final Set Up"},{"location":"win23team3/#packages-and-drivers","text":"cv2 (OpenCV) depthai (DepthAI) numpy math binascii","title":"Packages and Drivers"},{"location":"win23team3/#milestones","text":"Face Recognition using DepthAI - Detects faces through a webcam and displays a count in the terminal Distance Measurement using DepthAI - Using the disparity between the left and right cameras of the OAKD, distance can be calculated. This was averaged over an area to give an estimated distance of an object. Distance Measurement using LiDAR - Using a LiDAR is is relatively simple to detect distances in a 360 degree range. By averaging distances over a very small range (0.4 degrees) we determined the distance of an object.","title":"Milestones"},{"location":"win23team3/#potential-future-workunaccomplished-goals","text":"Recognizing and labeling specific faces Running code off of the OAK-D Camera instead of needing an external computer to run the code.","title":"Potential Future Work/Unaccomplished Goals"},{"location":"win23team3/#presentations","text":"- Final Project Proposal - Final Presentation","title":"Presentations"},{"location":"win23team3/#acknowledgments","text":"Professor Jack Silberman, TA Kishore Nukala, Moises Lopez-Mendoza, Design and Innovation Building, all of our wonderful classmates","title":"Acknowledgments"},{"location":"win23team4/","text":"Team 4 Final Project Report Members: Vasanth Senthil (ECE), Eddy Rodas Lima(MAE), Lingpeng Meng(ECE) Physical Setup Initial Goals Objective Make our RoboCar follow sound using sound localization from multiple microphones. Must Haves Car is able to determine approximate direction of an audio source Car moves in towards audio source once direction of audio is determined Nice to Haves Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume Accurate movement towards source Accomplishments Researched and experimented with different methods of sound localization, such as using time of arrival delay between microphones Used the pyaudio library to collect streams of microphone data from each of the three microphone, and processed this data in order to tell direction real-time based on sound level Used the pyvesc library to create a custom movement file that allows us to easily move the RoboCar based on provided angles and throttle until specified conditions are met The microphone processing file called upon the movement functions after determining current direction. We also spread the left and right microphones to minimize the noise created by the car\u2019s movement, which would influence the accuracy of direction calculations. Demo Videos Static Source Left Static Source Right Moving Source Front Moving Source Back Moving Source Further Away Issues Faced Issue Solution Original idea of using time delay between microphones was not possible due to jetson limitations Decided to use volume of sound signals instead of microphone timing in order to determine direction of audio. Sound from car\u2019s movement, such as servo motors and wheels, were causing our robot to turn randomly. Created wings out of cardboard which lifted and spread out the left/right microphones away from the wheels. Random/sudden loud noises from the environment made the car turn in wrong directions, even when originally was moving correctly towards audio source. Averaged the audio signals of each microphone over the last few ticks using a buffer in order to reduce impact of outlier noises. Required tuning, as a small buffer would not remove outliers as effectively while a large buffer reduced reaction time of robot\u2019s movement when direction of audio actually changed What did not work Original plan was to use the time delay between when microphones head a noise over a sound threshold in order to calculate angle of sound. Problem with that is the Jetson Nano is running Linux, and due to background processes and overhead, it is not precise enough to identify the timing needed for this method to work when the microphones are this close together. This may have been possible with a real-time system such as an Arduino Another method was using real-time localized sound by performing beamforming using microphones in array, but we were not certain if we had enough time to implement and debug this method in time (received the microphones on Saturday) as it was significantly more complex than the other options. Next Steps (If we had more time) Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume, would help filter interference with background noises. Function based on sound difference between microphones to get precise steering angles. Current only steers forward, full left, full right. More accurate movement with our backwards direction Minimize unwanted noise coming from either surroundings or the vehicle.","title":"Team 4"},{"location":"win23team4/#team-4-final-project-report","text":"","title":"Team 4 Final Project Report"},{"location":"win23team4/#members-vasanth-senthil-ece-eddy-rodas-limamae-lingpeng-mengece","text":"","title":"Members: Vasanth Senthil (ECE), Eddy Rodas Lima(MAE), Lingpeng Meng(ECE)"},{"location":"win23team4/#physical-setup","text":"","title":"Physical Setup"},{"location":"win23team4/#initial-goals","text":"","title":"Initial Goals"},{"location":"win23team4/#objective","text":"Make our RoboCar follow sound using sound localization from multiple microphones.","title":"Objective"},{"location":"win23team4/#must-haves","text":"Car is able to determine approximate direction of an audio source Car moves in towards audio source once direction of audio is determined","title":"Must Haves"},{"location":"win23team4/#nice-to-haves","text":"Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume Accurate movement towards source","title":"Nice to Haves"},{"location":"win23team4/#accomplishments","text":"Researched and experimented with different methods of sound localization, such as using time of arrival delay between microphones Used the pyaudio library to collect streams of microphone data from each of the three microphone, and processed this data in order to tell direction real-time based on sound level Used the pyvesc library to create a custom movement file that allows us to easily move the RoboCar based on provided angles and throttle until specified conditions are met The microphone processing file called upon the movement functions after determining current direction. We also spread the left and right microphones to minimize the noise created by the car\u2019s movement, which would influence the accuracy of direction calculations.","title":"Accomplishments"},{"location":"win23team4/#demo-videos","text":"Static Source Left Static Source Right Moving Source Front Moving Source Back Moving Source Further Away","title":"Demo Videos"},{"location":"win23team4/#issues-faced","text":"Issue Solution Original idea of using time delay between microphones was not possible due to jetson limitations Decided to use volume of sound signals instead of microphone timing in order to determine direction of audio. Sound from car\u2019s movement, such as servo motors and wheels, were causing our robot to turn randomly. Created wings out of cardboard which lifted and spread out the left/right microphones away from the wheels. Random/sudden loud noises from the environment made the car turn in wrong directions, even when originally was moving correctly towards audio source. Averaged the audio signals of each microphone over the last few ticks using a buffer in order to reduce impact of outlier noises. Required tuning, as a small buffer would not remove outliers as effectively while a large buffer reduced reaction time of robot\u2019s movement when direction of audio actually changed","title":"Issues Faced"},{"location":"win23team4/#what-did-not-work","text":"Original plan was to use the time delay between when microphones head a noise over a sound threshold in order to calculate angle of sound. Problem with that is the Jetson Nano is running Linux, and due to background processes and overhead, it is not precise enough to identify the timing needed for this method to work when the microphones are this close together. This may have been possible with a real-time system such as an Arduino Another method was using real-time localized sound by performing beamforming using microphones in array, but we were not certain if we had enough time to implement and debug this method in time (received the microphones on Saturday) as it was significantly more complex than the other options.","title":"What did not work"},{"location":"win23team4/#next-steps-if-we-had-more-time","text":"Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume, would help filter interference with background noises. Function based on sound difference between microphones to get precise steering angles. Current only steers forward, full left, full right. More accurate movement with our backwards direction Minimize unwanted noise coming from either surroundings or the vehicle.","title":"Next Steps (If we had more time)"},{"location":"win23team5/","text":"Final Project Repository for Team 5 of the 2023 Winter Class MAE ECE 148 at UCSD Our Final Project has one main objective, which is inspired by a pet dog that plays fetch. Our goal is to design a robot that can identify a green ball, like a tennis ball, locate it, move towards it, pick it up, and return back to its initial location. We achieved this using an OpenCV-based vision system to recognize the ball and Pyvesc to control the car's movements. We also designed a claw mechanism to pick up the ball when it's within range and a servo to move the ball into the claw. In summary, our project demonstrates the capabilities of an autonomous robot that can navigate an environment, recognize objects, and perform tasks like fetching. With further improvements, this type of robot could have many potential applications in various industries.","title":"Team 5"},{"location":"win23team5/#final-project-repository-for-team-5-of-the-2023-winter-class-mae-ece-148-at-ucsd","text":"Our Final Project has one main objective, which is inspired by a pet dog that plays fetch. Our goal is to design a robot that can identify a green ball, like a tennis ball, locate it, move towards it, pick it up, and return back to its initial location. We achieved this using an OpenCV-based vision system to recognize the ball and Pyvesc to control the car's movements. We also designed a claw mechanism to pick up the ball when it's within range and a servo to move the ball into the claw. In summary, our project demonstrates the capabilities of an autonomous robot that can navigate an environment, recognize objects, and perform tasks like fetching. With further improvements, this type of robot could have many potential applications in various industries.","title":"Final Project Repository for Team 5 of the 2023 Winter Class MAE ECE 148 at UCSD"},{"location":"win23team6/","text":"MAE/ECE 148 Winter 2023 at UCSD TEAM 6 Our project uses the OAK-D camera, a roboflow YOLO model, PyVESC module, and an Arduino-powered camera mount to get our car to scan its surroundings until it finds a basketball and drive until it is within about 0.5 m of the ball. Car Assembly Vehicle Body Camera Mount Tech Stack RoboflowOak Module We used roboflow to train a ball detection model and host the model. We, then, made API calls to hosted model to retrieve predictions on frame captures from the OAK-D camera. Once a ball is found, we wrote a script to calculate the angle between the center of the bounding box drawn around the detected ball and the centerline of the camera (which by default is the center of the frame). PyVESC Module We used the pyvesc module to set our servo's angle once a detection has been made. The steering angle is proportional to the calculated angle. Once the steering is set, we increase throttle for about half a second and then stop the motor to make another detection. We then loop over these steps until the ball is within 0.5 m of the frame. Our stopping condition was for the width of the bounding box of the ball to be a certain ratio of the total frame width. The ratio is hardcoded based on fine-tuning to get the car to stop at about 0.5 m. Arduino Board We used an arduino nano to control the camera mount. The camera mount can rotate horizontally (yaw-equivalent) within a range of 180 degrees. And it can move up and down (pitch-equivalent) within a range of 90 degrees. This is used to move the camera around so it can scan the surroundings for a ball (in case the ball is not in frame). Motorized Camera Mount We designed a camera mount that is actuated by 2 servos, one controls the camera's pitch angle and the other controls the camera's yaw angle. The mount elevates the camera 5 inches amove the vehicle's mounting plate. It allows the camera to turn and scan for the target ball. https://user-images.githubusercontent.com/58583277/227646168-1071f237-de95-4f92-ad65-a90ee5fe2b01.mp4 How To Run The Code One can run python run.py from inside the Jetson Nano mounted on their car. This will load the model and also detect the motor and begin the purported task of finding a basketball. If no basketball is found, it will remain stationary. The 148remote/ folder contains a cool arduino program for proof of concept. It moves the camera mount through its full range of motion in a rhythmic fashion. Vehicle In Action Click Here For Video Future Improvements We can use the Jetson to control the servo motors through the Arduino. We can account for the yaw angle of the camera mount and add that to the steering, so that the car can steer toward targets that is not in the field of view of the camera when the camera is pointing straight forward. We can also improve the precision of the ball-recognition model by using more pictures of the ball to train the model. Team 6: Wall-E (used to be Cyclops) Saathvik Dirisala (Data Science) Victor Chen (Computer Engineering) Yang Song (Mechanical Engineering)","title":"Team 6"},{"location":"win23team6/#maeece-148-winter-2023-at-ucsd","text":"","title":"MAE/ECE 148 Winter 2023 at UCSD"},{"location":"win23team6/#team-6","text":"Our project uses the OAK-D camera, a roboflow YOLO model, PyVESC module, and an Arduino-powered camera mount to get our car to scan its surroundings until it finds a basketball and drive until it is within about 0.5 m of the ball.","title":"TEAM 6"},{"location":"win23team6/#car-assembly","text":"","title":"Car Assembly"},{"location":"win23team6/#vehicle-body","text":"","title":"Vehicle Body"},{"location":"win23team6/#camera-mount","text":"","title":"Camera Mount"},{"location":"win23team6/#tech-stack","text":"","title":"Tech Stack"},{"location":"win23team6/#roboflowoak-module","text":"We used roboflow to train a ball detection model and host the model. We, then, made API calls to hosted model to retrieve predictions on frame captures from the OAK-D camera. Once a ball is found, we wrote a script to calculate the angle between the center of the bounding box drawn around the detected ball and the centerline of the camera (which by default is the center of the frame).","title":"RoboflowOak Module"},{"location":"win23team6/#pyvesc-module","text":"We used the pyvesc module to set our servo's angle once a detection has been made. The steering angle is proportional to the calculated angle. Once the steering is set, we increase throttle for about half a second and then stop the motor to make another detection. We then loop over these steps until the ball is within 0.5 m of the frame. Our stopping condition was for the width of the bounding box of the ball to be a certain ratio of the total frame width. The ratio is hardcoded based on fine-tuning to get the car to stop at about 0.5 m.","title":"PyVESC Module"},{"location":"win23team6/#arduino-board","text":"We used an arduino nano to control the camera mount. The camera mount can rotate horizontally (yaw-equivalent) within a range of 180 degrees. And it can move up and down (pitch-equivalent) within a range of 90 degrees. This is used to move the camera around so it can scan the surroundings for a ball (in case the ball is not in frame).","title":"Arduino Board"},{"location":"win23team6/#motorized-camera-mount","text":"We designed a camera mount that is actuated by 2 servos, one controls the camera's pitch angle and the other controls the camera's yaw angle. The mount elevates the camera 5 inches amove the vehicle's mounting plate. It allows the camera to turn and scan for the target ball. https://user-images.githubusercontent.com/58583277/227646168-1071f237-de95-4f92-ad65-a90ee5fe2b01.mp4","title":"Motorized Camera Mount"},{"location":"win23team6/#how-to-run-the-code","text":"One can run python run.py from inside the Jetson Nano mounted on their car. This will load the model and also detect the motor and begin the purported task of finding a basketball. If no basketball is found, it will remain stationary. The 148remote/ folder contains a cool arduino program for proof of concept. It moves the camera mount through its full range of motion in a rhythmic fashion.","title":"How To Run The Code"},{"location":"win23team6/#vehicle-in-action","text":"Click Here For Video","title":"Vehicle In Action"},{"location":"win23team6/#future-improvements","text":"We can use the Jetson to control the servo motors through the Arduino. We can account for the yaw angle of the camera mount and add that to the steering, so that the car can steer toward targets that is not in the field of view of the camera when the camera is pointing straight forward. We can also improve the precision of the ball-recognition model by using more pictures of the ball to train the model.","title":"Future Improvements"},{"location":"win23team6/#team-6-wall-e-used-to-be-cyclops","text":"Saathvik Dirisala (Data Science) Victor Chen (Computer Engineering) Yang Song (Mechanical Engineering)","title":"Team 6: Wall-E (used to be Cyclops)"},{"location":"win23team7/","text":"UCSD ECE/MAE-148 2023 Winter Team 7 Team Members Francisco Downey (BENG), Jonathan Xiong (ECE), Nicholas Preston (MAE), Karthik Srinivasan (MAE) Final Project Overview For our final project, we made our car drive from point A to B, given starting and ending GPS points. Assembled Car Design LIDAR was set in the front of the car. This was an easy decision for the team since we cared for varying obstacles crossing the points that comprise the path from Point A to Point B. Essentially, it was only important to care for obstacles that come into the front of the moving car. GNSS was secure near the center of the car with the antenna placed high and toward the rear. DonkeyCar - 3 Autonomous Laps https://user-images.githubusercontent.com/103704890/227807530-35ed2ea5-2bc0-4b8b-983d-ca6bed659390.mp4 ROS2 Line Following - 3 Autonomous Laps https://user-images.githubusercontent.com/103704890/227801950-dfd60ddd-300e-4af8-9878-c62338184269.mp4 ROS2 Left Lane - 3 Autonomous Laps https://user-images.githubusercontent.com/103704890/227804039-c1aafb7a-8bf6-4f86-89cf-1fa21c7ea5d2.mp4 GPS - 3 Autonomous Laps https://user-images.githubusercontent.com/103704890/227804055-d9c3ccce-ef8d-427d-a6b6-e1db12eba440.mp4 Important to note for the final project, many of the configurations that were found to work in the GPS 3 autonomous laps were used for the final project. The GPS points making up the path were 0.55 meters apart. The nature of the csv points used in the final project matched the one used for this assignment. This allowed the project to move faster as the configurations did work out. See section Algorithmic Design of Directing Car from GPS point A to B. Final Project Plan: Go from point A to B with object detection. Overview Originally, we were going to have LIDAR detect objects in front of the car so the car can see what it needs to go around. However, we did not have enough time to see how to use LIDAR, since there was no assignment with it and not enough time at end of quarter given rain. We, however, got quite experienced with the GPS functionality of the car. Using the GPS modules, we were able to direct the car using a path of car-readble GPS coordinates. Algorithmic Design of Directing Car from GPS point A to B By the end of the quarter, we wrote a program, AtoB.py, which requires two sets of GPS coordinates as inputs. This program generates a .csv file containing a path of GPS coordinates about .55 meters apart from point A to B. These generated points latitude longitude formatted and relative to the base station antenna. The inputted absolute (planet's) GPS coordinates, thus, needed to be translated to these relative coordinates and converted to rectangular from polar positions. It took some testing to increase the precision of the translation as we did not know the exact conversion constants. Demonstration https://user-images.githubusercontent.com/103704890/227807009-ec59e649-c068-4543-9dba-9ddb16ea8ee5.mp4","title":"Team 7"},{"location":"win23team7/#ucsd-ecemae-148-2023-winter-team-7","text":"","title":"UCSD ECE/MAE-148 2023 Winter Team 7"},{"location":"win23team7/#team-members","text":"Francisco Downey (BENG), Jonathan Xiong (ECE), Nicholas Preston (MAE), Karthik Srinivasan (MAE)","title":"Team Members"},{"location":"win23team7/#final-project-overview","text":"For our final project, we made our car drive from point A to B, given starting and ending GPS points.","title":"Final Project Overview"},{"location":"win23team7/#assembled-car-design","text":"LIDAR was set in the front of the car. This was an easy decision for the team since we cared for varying obstacles crossing the points that comprise the path from Point A to Point B. Essentially, it was only important to care for obstacles that come into the front of the moving car. GNSS was secure near the center of the car with the antenna placed high and toward the rear.","title":"Assembled Car Design"},{"location":"win23team7/#donkeycar-3-autonomous-laps","text":"https://user-images.githubusercontent.com/103704890/227807530-35ed2ea5-2bc0-4b8b-983d-ca6bed659390.mp4","title":"DonkeyCar - 3 Autonomous Laps"},{"location":"win23team7/#ros2-line-following-3-autonomous-laps","text":"https://user-images.githubusercontent.com/103704890/227801950-dfd60ddd-300e-4af8-9878-c62338184269.mp4","title":"ROS2 Line Following - 3 Autonomous Laps"},{"location":"win23team7/#ros2-left-lane-3-autonomous-laps","text":"https://user-images.githubusercontent.com/103704890/227804039-c1aafb7a-8bf6-4f86-89cf-1fa21c7ea5d2.mp4","title":"ROS2 Left Lane - 3 Autonomous Laps"},{"location":"win23team7/#gps-3-autonomous-laps","text":"https://user-images.githubusercontent.com/103704890/227804055-d9c3ccce-ef8d-427d-a6b6-e1db12eba440.mp4 Important to note for the final project, many of the configurations that were found to work in the GPS 3 autonomous laps were used for the final project. The GPS points making up the path were 0.55 meters apart. The nature of the csv points used in the final project matched the one used for this assignment. This allowed the project to move faster as the configurations did work out. See section Algorithmic Design of Directing Car from GPS point A to B.","title":"GPS - 3 Autonomous Laps"},{"location":"win23team7/#final-project","text":"Plan: Go from point A to B with object detection. Overview Originally, we were going to have LIDAR detect objects in front of the car so the car can see what it needs to go around. However, we did not have enough time to see how to use LIDAR, since there was no assignment with it and not enough time at end of quarter given rain. We, however, got quite experienced with the GPS functionality of the car. Using the GPS modules, we were able to direct the car using a path of car-readble GPS coordinates. Algorithmic Design of Directing Car from GPS point A to B By the end of the quarter, we wrote a program, AtoB.py, which requires two sets of GPS coordinates as inputs. This program generates a .csv file containing a path of GPS coordinates about .55 meters apart from point A to B. These generated points latitude longitude formatted and relative to the base station antenna. The inputted absolute (planet's) GPS coordinates, thus, needed to be translated to these relative coordinates and converted to rectangular from polar positions. It took some testing to increase the precision of the translation as we did not know the exact conversion constants. Demonstration https://user-images.githubusercontent.com/103704890/227807009-ec59e649-c068-4543-9dba-9ddb16ea8ee5.mp4","title":"Final Project"},{"location":"win23team8/","text":"Team 8 Final Project Proposal Team Members - Youssef Georgy | Electrical & Computer Engineering - Rizzi Galibut | Mechanical & Aerospace Engineering - Shuhang Xu | Computer Science - Kavin Raj | Cognitive Science w/ Emphasis in Machine Learning Such a lovely team!! Project Overall A waiter-bot that takes visual input from the camera, navigates autonomously to different specified locations (i.e., tables) and then back to the starting point. Use image-detection via camera to give robocar a location or GPS coordinates to navigate to, then use GPS data to plot path there and avoid any obstacles in the way. The robocar will be able to take any location given (provided it\u2019s in range of the network connection) and determine how to get there Physical Setup Gantt Chart Demonstration Using DepthAI for text recognition Accomplished: - Used DepthAI to enable the camera to detect numbers which are associated with different tables (e.g., 001, 002, etc.) and different CSV files - Originally started by looking at OpenCV and Tesseract for OCR - These are not SpatialAI platforms so accomplishing what we were trying to do would be much harder - Default code provided in DepthAI library launched windows that displayed video stream and words detected, which worked when directly connected to camera through host computer but running code through Jetson would require a container to launch these windows - Needed to find a way to disable them and have camera run in the background Default code also rewrote the variable that contained the decoded text each time it detected something - Had to rewrite and remove sections of the code to stop detection once certain prompts are given - Light conditions really mattered when showing prompt to camera - Worked better during the day and when shown prompt on a backlit-screen (with white background) What did't work as expected: - Camera could read \u201c001\u201d but not \u201c1\u201d, etc., so we decided to use a set of numbers instead - Number recognition is much more accurate than word recognition so we decided to use numbers as the prompt Using GPS to record different paths GPS navigation code is based on the USCD donkeycar GPS library We are trying to make it stay in auto-pilot mode by default and reset the origin in the beginning Depend on what text is detected with DepthAI, make it move following the pre-recorded path When it moves back to the origin, stop and start detecting next text Desired but not accomplished Include the ability to restart the process once the waiter-bot returned to the starting point without having to manually run the script again Have waiter-bot stay at the table for a certain amount of time before returning to the starting point Incorporate the LiDAR so the waiter-bot can avoid obstacles (e.g., students walking past) while traveling to tables Could also be used to detect when the waiter-bot reaches the table and ensure it doesn\u2019t crash into it Have controller inputs programmed in so the waiter-bot can be truly autonomous. Currently still requires human input","title":"Team 8"},{"location":"win23team8/#team-8-final-project-proposal","text":"","title":"Team 8 Final Project Proposal"},{"location":"win23team8/#team-members","text":"- Youssef Georgy | Electrical & Computer Engineering - Rizzi Galibut | Mechanical & Aerospace Engineering - Shuhang Xu | Computer Science - Kavin Raj | Cognitive Science w/ Emphasis in Machine Learning Such a lovely team!!","title":"Team Members"},{"location":"win23team8/#project-overall","text":"A waiter-bot that takes visual input from the camera, navigates autonomously to different specified locations (i.e., tables) and then back to the starting point. Use image-detection via camera to give robocar a location or GPS coordinates to navigate to, then use GPS data to plot path there and avoid any obstacles in the way. The robocar will be able to take any location given (provided it\u2019s in range of the network connection) and determine how to get there","title":"Project Overall"},{"location":"win23team8/#physical-setup","text":"","title":"Physical Setup"},{"location":"win23team8/#gantt-chart","text":"","title":"Gantt Chart"},{"location":"win23team8/#demonstration","text":"","title":"Demonstration"},{"location":"win23team8/#using-depthai-for-text-recognition","text":"Accomplished: - Used DepthAI to enable the camera to detect numbers which are associated with different tables (e.g., 001, 002, etc.) and different CSV files - Originally started by looking at OpenCV and Tesseract for OCR - These are not SpatialAI platforms so accomplishing what we were trying to do would be much harder - Default code provided in DepthAI library launched windows that displayed video stream and words detected, which worked when directly connected to camera through host computer but running code through Jetson would require a container to launch these windows - Needed to find a way to disable them and have camera run in the background Default code also rewrote the variable that contained the decoded text each time it detected something - Had to rewrite and remove sections of the code to stop detection once certain prompts are given - Light conditions really mattered when showing prompt to camera - Worked better during the day and when shown prompt on a backlit-screen (with white background) What did't work as expected: - Camera could read \u201c001\u201d but not \u201c1\u201d, etc., so we decided to use a set of numbers instead - Number recognition is much more accurate than word recognition so we decided to use numbers as the prompt","title":"Using DepthAI for text recognition"},{"location":"win23team8/#using-gps-to-record-different-paths","text":"GPS navigation code is based on the USCD donkeycar GPS library We are trying to make it stay in auto-pilot mode by default and reset the origin in the beginning Depend on what text is detected with DepthAI, make it move following the pre-recorded path When it moves back to the origin, stop and start detecting next text","title":"Using GPS to record different paths"},{"location":"win23team8/#desired-but-not-accomplished","text":"Include the ability to restart the process once the waiter-bot returned to the starting point without having to manually run the script again Have waiter-bot stay at the table for a certain amount of time before returning to the starting point Incorporate the LiDAR so the waiter-bot can avoid obstacles (e.g., students walking past) while traveling to tables Could also be used to detect when the waiter-bot reaches the table and ensure it doesn\u2019t crash into it Have controller inputs programmed in so the waiter-bot can be truly autonomous. Currently still requires human input","title":"Desired but not accomplished"},{"location":"win23team9/","text":"ECE/MAE 148 Winter 2023 Team 9 JetBuddy Indoor Delivery Bot based on DepthAI, OpenCV and LiDAR Table of Contents Team Members Hardware and Schematics Parts Schematics Final Project Abstract Part 1: Human Detection and Following with Depthai and PyVesc Part 2: Stopping Mechanism with Lidar Part 3: Facial Recognition Part 4: Spatial Detection with DepthAI Reflection Challenges Potential Improvements Presentation Files Reference Team members Ben Zhang (ECE) Joseph Katona (ECE) Yichen Yang (ECE) Zijian Wang (MAE) Hardware Parts Full Assembly Mounting Plate Jetson Case Camera LiDAR Mount Final Project Delivery Box Schematics Wire Diagram Final Project Abstract This project aims to develop a delivery system for our robocar that can detect and follow humans while also incorporating a stopping mechanism to prevent collisions. Additionally, the robot will utilize facial recognition to identify individuals and personalize interactions. Part 1: Human Detection and Following with Depthai and PyVesc The OAKD camera will be used to detect and track humans in the robot's vicinity. The PyVesc motor controllers will then be used to move the robot in the direction of the detected human. Required Components Tiny-Yolov3 model integrated in DepthAi for object detection PyVesc Python package for robocar control Algorithm Workflow Use Tiny-Yolov3 to detect the bounding box of the person in the OAKD camera's field of view. Determine the position of the person by finding the central line of the bounding box, and denote the x-axis value as x0. Calculate the error between the central line of the frame (416x416 pixels), e = x - x0. Calculate the steering value using the formula: v = (Kp * e) / 416 + 0.5, where Kp = 1. Use PyVesc to control steering by calling vesc.set_servo(v). Additional Settings Use vesc.set_rpm() to run the car once it detects people. The steering value is sampled at a rate of 5Hz to prevent frequent drifting. Part 2: Stopping Mechanism with Lidar The Lidar sensor will be used to detect obstacles in the robot's path. If an obstacle is detected, the robot will stop moving and wait for the obstacle to clear before continuing on its path. The LiDAR on this robot aim to Detect anything that is in a close range If the position is too clase, the robot will stop to avoid collision The robot will back up after it stop for a while and still detect obstacle is close Transform raw binary data from LiDAR to numerical data through BinASCII library How to read LiDAR? Each measurement data point of LiDAR is consists of a distance value of 2 bytes and a confidence of 1 byte We transform this data through chopping it to bytes and translate it. We get the angle by getting the start angle and end angle. Putting all the distance into a list and it will stop the car if there\u2019s an object within certain distance that LiDAR detected. Part 3: Facial Recognition The robot will be equipped with a facial recognition system, using a webcam, that will allow it to identify individuals and personalize interactions. Once it recognizes the right person, the delivery box will open. The facial recognition software uses a simple python import of facial_recognition. In the facial_recognition library all we do is use openCV to capture images for the frames and use facial_recognitions \"matching\" function to to add a box around the persons face. In our case when this value is detected over an interval then a true value is then sent to the box to open. Part 4: Spatial Detection with DepthAI Utilizing Depthai's pipeline system we take their spatial location pipeline to simply calculate the distance of individual from the camera. The Object detection pipeline detects a person and creates a bounded box, then with the x and y coordinates from the bounded box we can pinpoint where we want the camera to point. After these coordinates are gathered the z location is stored in a circular list. This is because the bounded box and tracker of object distance aren't always in sync so some erroneous values are given. Once we have around 50 samples then we take the average to get a good idea of what the distance of the person from the car is. Finally we utilize pyvescs set_rpm() features to give out a more smooth acceleration system. So, basically if you're far away the robot will speed up and slow down as it moves closer to you. Get more info on Spatial Depth here Gantt Chart Demonstrations The Video might not show up, please go to img folder for full demo. Your browser does not support HTML video. Reflection Challenges Getting everything to work together Different libraries working together and all send signals to PyVESC Everything worked fine on a local machine but when running on the Jetson, crashes would occur Scope of the original idea Mapping the path for future references using SLAM Depth ai pipeline caused crashes X-Link Problem(Serial bus issues) Translate raw LiDAR output to data we need Making the car look smooth Better algorithm to adjust speed(rpm) Potential Improvements Implement all the features together flawlessly Currently cannot run together good due to delay from different components Get the locking mechanism working Locking mechanism to make sure the right receiver get the package LiDar also scans the path for future path planning Trying to find a person if it cannot detect anything Maybe try different frameworks since we can use different libraries without limitation in ROS or donkeycar Presentations Project Proposal & Progress Report Final Presntation Reference We would like to give special thanks to: Professor Jack Silberman TA Moises Lopez TA Kishore Nukala All The teams that helped us on the way","title":"Team 9"},{"location":"win23team9/#team-members","text":"Ben Zhang (ECE) Joseph Katona (ECE) Yichen Yang (ECE) Zijian Wang (MAE)","title":"Team members"},{"location":"win23team9/#hardware","text":"","title":"Hardware"},{"location":"win23team9/#parts","text":"","title":"Parts"},{"location":"win23team9/#full-assembly","text":"","title":"Full Assembly"},{"location":"win23team9/#mounting-plate","text":"","title":"Mounting Plate"},{"location":"win23team9/#jetson-case","text":"","title":"Jetson Case"},{"location":"win23team9/#camera-lidar-mount","text":"","title":"Camera LiDAR Mount"},{"location":"win23team9/#final-project-delivery-box","text":"","title":"Final Project Delivery Box"},{"location":"win23team9/#schematics","text":"","title":"Schematics"},{"location":"win23team9/#wire-diagram","text":"","title":"Wire Diagram"},{"location":"win23team9/#final-project","text":"","title":"Final Project"},{"location":"win23team9/#abstract","text":"This project aims to develop a delivery system for our robocar that can detect and follow humans while also incorporating a stopping mechanism to prevent collisions. Additionally, the robot will utilize facial recognition to identify individuals and personalize interactions.","title":"Abstract"},{"location":"win23team9/#part-1-human-detection-and-following-with-depthai-and-pyvesc","text":"The OAKD camera will be used to detect and track humans in the robot's vicinity. The PyVesc motor controllers will then be used to move the robot in the direction of the detected human.","title":"Part 1: Human Detection and Following with Depthai and PyVesc"},{"location":"win23team9/#required-components","text":"Tiny-Yolov3 model integrated in DepthAi for object detection PyVesc Python package for robocar control","title":"Required Components"},{"location":"win23team9/#algorithm-workflow","text":"Use Tiny-Yolov3 to detect the bounding box of the person in the OAKD camera's field of view. Determine the position of the person by finding the central line of the bounding box, and denote the x-axis value as x0. Calculate the error between the central line of the frame (416x416 pixels), e = x - x0. Calculate the steering value using the formula: v = (Kp * e) / 416 + 0.5, where Kp = 1. Use PyVesc to control steering by calling vesc.set_servo(v).","title":"Algorithm Workflow"},{"location":"win23team9/#additional-settings","text":"Use vesc.set_rpm() to run the car once it detects people. The steering value is sampled at a rate of 5Hz to prevent frequent drifting.","title":"Additional Settings"},{"location":"win23team9/#part-2-stopping-mechanism-with-lidar","text":"The Lidar sensor will be used to detect obstacles in the robot's path. If an obstacle is detected, the robot will stop moving and wait for the obstacle to clear before continuing on its path.","title":"Part 2: Stopping Mechanism with Lidar"},{"location":"win23team9/#the-lidar-on-this-robot-aim-to","text":"Detect anything that is in a close range If the position is too clase, the robot will stop to avoid collision The robot will back up after it stop for a while and still detect obstacle is close Transform raw binary data from LiDAR to numerical data through BinASCII library","title":"The LiDAR on this robot aim to"},{"location":"win23team9/#how-to-read-lidar","text":"Each measurement data point of LiDAR is consists of a distance value of 2 bytes and a confidence of 1 byte We transform this data through chopping it to bytes and translate it. We get the angle by getting the start angle and end angle. Putting all the distance into a list and it will stop the car if there\u2019s an object within certain distance that LiDAR detected.","title":"How to read LiDAR?"},{"location":"win23team9/#part-3-facial-recognition","text":"The robot will be equipped with a facial recognition system, using a webcam, that will allow it to identify individuals and personalize interactions. Once it recognizes the right person, the delivery box will open. The facial recognition software uses a simple python import of facial_recognition. In the facial_recognition library all we do is use openCV to capture images for the frames and use facial_recognitions \"matching\" function to to add a box around the persons face. In our case when this value is detected over an interval then a true value is then sent to the box to open.","title":"Part 3: Facial Recognition"},{"location":"win23team9/#part-4-spatial-detection-with-depthai","text":"Utilizing Depthai's pipeline system we take their spatial location pipeline to simply calculate the distance of individual from the camera. The Object detection pipeline detects a person and creates a bounded box, then with the x and y coordinates from the bounded box we can pinpoint where we want the camera to point. After these coordinates are gathered the z location is stored in a circular list. This is because the bounded box and tracker of object distance aren't always in sync so some erroneous values are given. Once we have around 50 samples then we take the average to get a good idea of what the distance of the person from the car is. Finally we utilize pyvescs set_rpm() features to give out a more smooth acceleration system. So, basically if you're far away the robot will speed up and slow down as it moves closer to you. Get more info on Spatial Depth here","title":"Part 4: Spatial Detection with DepthAI"},{"location":"win23team9/#gantt-chart","text":"","title":"Gantt Chart"},{"location":"win23team9/#demonstrations","text":"The Video might not show up, please go to img folder for full demo. Your browser does not support HTML video.","title":"Demonstrations"},{"location":"win23team9/#reflection","text":"","title":"Reflection"},{"location":"win23team9/#challenges","text":"Getting everything to work together Different libraries working together and all send signals to PyVESC Everything worked fine on a local machine but when running on the Jetson, crashes would occur Scope of the original idea Mapping the path for future references using SLAM Depth ai pipeline caused crashes X-Link Problem(Serial bus issues) Translate raw LiDAR output to data we need Making the car look smooth Better algorithm to adjust speed(rpm)","title":"Challenges"},{"location":"win23team9/#potential-improvements","text":"Implement all the features together flawlessly Currently cannot run together good due to delay from different components Get the locking mechanism working Locking mechanism to make sure the right receiver get the package LiDar also scans the path for future path planning Trying to find a person if it cannot detect anything Maybe try different frameworks since we can use different libraries without limitation in ROS or donkeycar","title":"Potential Improvements"},{"location":"win23team9/#presentations","text":"Project Proposal & Progress Report Final Presntation","title":"Presentations"},{"location":"win23team9/#reference","text":"We would like to give special thanks to: Professor Jack Silberman TA Moises Lopez TA Kishore Nukala All The teams that helped us on the way","title":"Reference"},{"location":"win23team12/","text":"ECE 148 Winter 2023 Team 12 Final Project Team members Jake Kindley (ECE) Yiteng Zhao (ECE) Noah Jones (MAE) Overview We want to create a sorter bot similar to warehouse bots that picks up a package, follow the path with certain color and delivers it to designated dropoff location based on the labels on that package. We plan to put AR tags as labels on the package, and each AR tag is mapped to an id associated with either the color of the lane our bot should follow or stop action when it reaches the designated dropoff zone. At dropoff zone, the bot will perform a series of maneuver and return to the starting point. Robot Design & Implementation Software Our code is running on ROS2 and is modified based on the provided lane following code from class. The following flowchart shows the original relationship between each component in the provided lane following code from class: We added a node responsible for detecting AR tag, read states from AR tag, and changing lane detector's behavior. In our new node, we have defined two types of AR tags: Type 1: Declares the lane color our bot should follow Type 2: Stop signal that indicates the bot to drop the package The default behavior when no AR tag is being detected is following blue lane until a type 1 tag is detected. Hardware We designed a bracket as package holder that snaps on the front bumper of the car and holds one package: Showcase Assembled Robots Final Project Demo Remarks Our video demonstrated our bot's capability of detecting AR tag, selecting the corresponding lane color, following path to dropoff zone, dropping off package in front of stop sign, and return to the starting point. Potential improvements for this project includes adding obstacle avoidance, redesigning the package holder to hold the package above ground, adjusting camera position for better view angle of the lane, and adding capability to navigate through multiple junctions.","title":"Team 12"},{"location":"win23team12/#ece-148-winter-2023-team-12-final-project","text":"","title":"ECE 148 Winter 2023 Team 12 Final Project"},{"location":"win23team12/#team-members","text":"Jake Kindley (ECE) Yiteng Zhao (ECE) Noah Jones (MAE)","title":"Team members"},{"location":"win23team12/#overview","text":"We want to create a sorter bot similar to warehouse bots that picks up a package, follow the path with certain color and delivers it to designated dropoff location based on the labels on that package. We plan to put AR tags as labels on the package, and each AR tag is mapped to an id associated with either the color of the lane our bot should follow or stop action when it reaches the designated dropoff zone. At dropoff zone, the bot will perform a series of maneuver and return to the starting point.","title":"Overview"},{"location":"win23team12/#robot-design-implementation","text":"","title":"Robot Design &amp; Implementation"},{"location":"win23team12/#software","text":"Our code is running on ROS2 and is modified based on the provided lane following code from class. The following flowchart shows the original relationship between each component in the provided lane following code from class: We added a node responsible for detecting AR tag, read states from AR tag, and changing lane detector's behavior. In our new node, we have defined two types of AR tags: Type 1: Declares the lane color our bot should follow Type 2: Stop signal that indicates the bot to drop the package The default behavior when no AR tag is being detected is following blue lane until a type 1 tag is detected.","title":"Software"},{"location":"win23team12/#hardware","text":"We designed a bracket as package holder that snaps on the front bumper of the car and holds one package:","title":"Hardware"},{"location":"win23team12/#showcase","text":"","title":"Showcase"},{"location":"win23team12/#assembled-robots","text":"","title":"Assembled Robots"},{"location":"win23team12/#final-project-demo","text":"","title":"Final Project Demo"},{"location":"win23team12/#remarks","text":"Our video demonstrated our bot's capability of detecting AR tag, selecting the corresponding lane color, following path to dropoff zone, dropping off package in front of stop sign, and return to the starting point. Potential improvements for this project includes adding obstacle avoidance, redesigning the package holder to hold the package above ground, adjusting camera position for better view angle of the lane, and adding capability to navigate through multiple junctions.","title":"Remarks"},{"location":"win23team13/","text":"Final Project: Robot Mapping Team 13: Girish, Muhammad, Andy, and Van ECE MAE 148, Winter 2023 Welcome to the project report for Team 13! This page contains a report of all the progress we made throughout this busy and fun quarter, including our final project. Team 13's assembled RC Car with the lidar placed at the front. Table of Contents Final Project: Robot Mapping - Team 13: Girish, Muhammad, Andy, and Van - ECE MAE 148, Winter 2023 Table of Contents The Team Final Project Abstract Hardware Setup Base Plate Camera Mount Jetson Nano Case Electronics Circuit Diagram Software Documentation Autonomous Laps Acknowledgements Credit and References The Team Girish Krishnan [ LinkedIn ] Muhammad Bintang Gemilang Andy Zhang Zhengyu (Van) Huang Electrical Engineering Mechanical Engineering Electrical Engineering Computer Engineering Final Project Abstract Our final project was themed around mapping an unknown environment . Our project involved the following tasks. What we promised [\u2714] To implement SLAM (Simultaneous Localization and Mapping) using a lidar. This effectively creates a map of the environment around the robot, showing the locations of all objects present. [\u2714] To display the map generated from SLAM in real-time using a web application. The challenges faced during the project were: Integrating the web application for live previewing (HTML/CSS/JS) with the Python code needed to run SLAM. Avoiding delays in the updating map. The accomplishments of the project were: We were able to achieve a decent visualization that updates over time as the robot is driven around The visualization/map can be saved easily for tasks such as path planning. Final Presentation Link to Final Presentation Link to video showing real-time mapping Weekly Update Presentations Project Proposal Week 8 Week 9 Week 10 Gantt Chart Hardware Setup 3D Printing: Camera Mount, Jetson Nano Case, GPS (GNSS) Case. Laser Cutting: Base plate to mount electronics and other components. Parts List Traxxas Chassis with steering servo and sensored brushless DC motor Jetson Nano WiFi adapter 64 GB Micro SD Card Adapter/reader for Micro SD Card Logitech F710 controller OAK-D Lite Camera LD06 Lidar VESC Anti-spark switch with power switch DC-DC Converter 4-cell LiPo battery Battery voltage checker/alarm DC Barrel Connector XT60, XT30, MR60 connectors Additional Parts used for testing/debugging Car stand USB-C to USB-A cable Micro USB to USB cable 5V, 4A power supply for Jetson Nano Base Plate All measurements shown above are in millimeters (mm) Our base plate was laser cut on a thick acrylic sheet. The circular hole at the end of the base plate is meant to hold the power on/off button. The long holes in the side of the plate are meant for wires to easily pass to and from the bottom of the plate. Camera Mount Camera Holder Base for attachment to base plate The two parts of the camera mount shown above were screwed together. The angle of the camera was carefully chosen (facing downward approximately 10 degrees from the vertical) so that the road ahead is clearly visible. This is essential for accurate results in OpenCV/ROS2 autonomous laps. One of our older camera mount designs is shown below. This camera mount consists of three parts: one base for attachment to the base plate, one middle piece to connect the base and the camera, and the camera holder. This camera mount design allows you to rotate the camera up and down. However, it is important that the rotating hinge is screwed securely so that the hinge doesn't wobble out while the robot does autonomous laps! Jetson Nano Case Credit to flyattack from Thingiverse, see: https://www.thingiverse.com/thing:3532828 This case is excellent because it is robust and doesn't break easily, unlike most common Jetson Nano cases. Electronics Circuit Diagram Note: some of these components and connections will vary depending on the exact components you have - check the component specifications carefully. Software Documentation To install all the necessary Python modules needed, run the following on the Jetson Nano. pip install -r requirements.txt For our final project, we implemented a real-time visualization system for the Hector SLAM algorithm implemented using the lidar sensor. The base code for the SLAM algorithm is accessible in the Docker container provided to us in class, and the code for the real-time implementation is present in the slam_gui folder of this repository. The SLAM real-time visualization GUI that we built has the following features: A web application whose routes are made using FastAPI in Python. Uvicorn is used to run the web server. HTML and JS to update the map in real-time The HTML and JS is interfaced with Python, ROS1, ROS2 and ROSBridge, so that the data collected is displayed on the web app. The interfacing process is difficult to implement directly in Python, so we use subprocessing to call relevant bash scripts that handle the processes in ROS1 and ROS2. These subprocesses are made to run in parallel using threading in Python. To run the visualizer, first open up a docker container containing the ucsd_robocar ROS packages. Run the following: cd slam_gui python slam_map.py This sets up the web app running on the Jetson Nano (although the app could potentially be run on any device, provided it can communicate with the Jetson Nano using the relevant ROS topics). Opening up the web app on http://localhost:8000 reveals the GUI showing the results of SLAM in real-time. The code in slam_map.py can be adjusted to fine-tune the time-delay that occurs as the map updates. Additional Scope for the Final Project Although SLAM is useful for mapping an unknown environment, it can be useful to integrate GPS data with SLAM to provide better location accuracy. To implement this in Python, we created the folder gps_slam that contains starter code with lidar, PyVESC, and GPS implementation and a basic SLAM algorithm with the Kalman filter (implemented using the filterpy library in Python). This additional, nice-to-have part of the project hasn't been tested out yet, but we plan to get it working soon. Autonomous Laps As part of the class deliverables and as preparation for the final project, here are our autonomous laps videos: Donkey Sim Local Computer: https://youtu.be/lXEStSEVikQ GPU training: https://youtu.be/4_BzKP9-XAQ External Server: https://youtu.be/Yvo1yqRJhX4 Physical Robot DonkeyCar: https://youtu.be/bPUSS2g0Ves Lane detection using OpenCV + ROS2: https://youtu.be/omcDCBSrl2I Inner lane: https://youtu.be/9hN8HUlGcas Outer lane: https://youtu.be/nXZNPscVlX0 GPS: https://youtu.be/Y3I9AWW1R6o Acknowledgements Thanks Prof. Jack Silberman and TAs Moises Lopez and Kishore Nukala for an awesome quarter! See you in DSC 178 next quarter, professor Jack ;) Credit and References Jetson Nano Case Design: https://www.thingiverse.com/thing:3532828 Lidar (LD06) Python Tutorial: https://github.com/henjin0/LIDAR_LD06_python_loder PyVESC: https://github.com/LiamBindle/PyVESC SLAM tutorial, Dominic Nightingale. https://gitlab.com/ucsd_robocar/ucsd_robocar_nav1_pkg/-/tree/master/","title":"Team 13"},{"location":"win23team13/#final-project-robot-mapping","text":"","title":"Final Project: Robot Mapping"},{"location":"win23team13/#team-13-girish-muhammad-andy-and-van","text":"","title":"Team 13: Girish, Muhammad, Andy, and Van"},{"location":"win23team13/#ece-mae-148-winter-2023","text":"Welcome to the project report for Team 13! This page contains a report of all the progress we made throughout this busy and fun quarter, including our final project. Team 13's assembled RC Car with the lidar placed at the front.","title":"ECE MAE 148, Winter 2023"},{"location":"win23team13/#table-of-contents","text":"Final Project: Robot Mapping - Team 13: Girish, Muhammad, Andy, and Van - ECE MAE 148, Winter 2023 Table of Contents The Team Final Project Abstract Hardware Setup Base Plate Camera Mount Jetson Nano Case Electronics Circuit Diagram Software Documentation Autonomous Laps Acknowledgements Credit and References","title":"Table of Contents"},{"location":"win23team13/#the-team","text":"Girish Krishnan [ LinkedIn ] Muhammad Bintang Gemilang Andy Zhang Zhengyu (Van) Huang Electrical Engineering Mechanical Engineering Electrical Engineering Computer Engineering","title":"The Team"},{"location":"win23team13/#final-project-abstract","text":"Our final project was themed around mapping an unknown environment . Our project involved the following tasks. What we promised [\u2714] To implement SLAM (Simultaneous Localization and Mapping) using a lidar. This effectively creates a map of the environment around the robot, showing the locations of all objects present. [\u2714] To display the map generated from SLAM in real-time using a web application. The challenges faced during the project were: Integrating the web application for live previewing (HTML/CSS/JS) with the Python code needed to run SLAM. Avoiding delays in the updating map. The accomplishments of the project were: We were able to achieve a decent visualization that updates over time as the robot is driven around The visualization/map can be saved easily for tasks such as path planning. Final Presentation Link to Final Presentation Link to video showing real-time mapping Weekly Update Presentations Project Proposal Week 8 Week 9 Week 10 Gantt Chart","title":"Final Project Abstract"},{"location":"win23team13/#hardware-setup","text":"3D Printing: Camera Mount, Jetson Nano Case, GPS (GNSS) Case. Laser Cutting: Base plate to mount electronics and other components. Parts List Traxxas Chassis with steering servo and sensored brushless DC motor Jetson Nano WiFi adapter 64 GB Micro SD Card Adapter/reader for Micro SD Card Logitech F710 controller OAK-D Lite Camera LD06 Lidar VESC Anti-spark switch with power switch DC-DC Converter 4-cell LiPo battery Battery voltage checker/alarm DC Barrel Connector XT60, XT30, MR60 connectors Additional Parts used for testing/debugging Car stand USB-C to USB-A cable Micro USB to USB cable 5V, 4A power supply for Jetson Nano","title":"Hardware Setup"},{"location":"win23team13/#base-plate","text":"All measurements shown above are in millimeters (mm) Our base plate was laser cut on a thick acrylic sheet. The circular hole at the end of the base plate is meant to hold the power on/off button. The long holes in the side of the plate are meant for wires to easily pass to and from the bottom of the plate.","title":"Base Plate"},{"location":"win23team13/#camera-mount","text":"Camera Holder Base for attachment to base plate The two parts of the camera mount shown above were screwed together. The angle of the camera was carefully chosen (facing downward approximately 10 degrees from the vertical) so that the road ahead is clearly visible. This is essential for accurate results in OpenCV/ROS2 autonomous laps. One of our older camera mount designs is shown below. This camera mount consists of three parts: one base for attachment to the base plate, one middle piece to connect the base and the camera, and the camera holder. This camera mount design allows you to rotate the camera up and down. However, it is important that the rotating hinge is screwed securely so that the hinge doesn't wobble out while the robot does autonomous laps!","title":"Camera Mount"},{"location":"win23team13/#jetson-nano-case","text":"Credit to flyattack from Thingiverse, see: https://www.thingiverse.com/thing:3532828 This case is excellent because it is robust and doesn't break easily, unlike most common Jetson Nano cases.","title":"Jetson Nano Case"},{"location":"win23team13/#electronics-circuit-diagram","text":"Note: some of these components and connections will vary depending on the exact components you have - check the component specifications carefully.","title":"Electronics Circuit Diagram"},{"location":"win23team13/#software-documentation","text":"To install all the necessary Python modules needed, run the following on the Jetson Nano. pip install -r requirements.txt For our final project, we implemented a real-time visualization system for the Hector SLAM algorithm implemented using the lidar sensor. The base code for the SLAM algorithm is accessible in the Docker container provided to us in class, and the code for the real-time implementation is present in the slam_gui folder of this repository. The SLAM real-time visualization GUI that we built has the following features: A web application whose routes are made using FastAPI in Python. Uvicorn is used to run the web server. HTML and JS to update the map in real-time The HTML and JS is interfaced with Python, ROS1, ROS2 and ROSBridge, so that the data collected is displayed on the web app. The interfacing process is difficult to implement directly in Python, so we use subprocessing to call relevant bash scripts that handle the processes in ROS1 and ROS2. These subprocesses are made to run in parallel using threading in Python. To run the visualizer, first open up a docker container containing the ucsd_robocar ROS packages. Run the following: cd slam_gui python slam_map.py This sets up the web app running on the Jetson Nano (although the app could potentially be run on any device, provided it can communicate with the Jetson Nano using the relevant ROS topics). Opening up the web app on http://localhost:8000 reveals the GUI showing the results of SLAM in real-time. The code in slam_map.py can be adjusted to fine-tune the time-delay that occurs as the map updates. Additional Scope for the Final Project Although SLAM is useful for mapping an unknown environment, it can be useful to integrate GPS data with SLAM to provide better location accuracy. To implement this in Python, we created the folder gps_slam that contains starter code with lidar, PyVESC, and GPS implementation and a basic SLAM algorithm with the Kalman filter (implemented using the filterpy library in Python). This additional, nice-to-have part of the project hasn't been tested out yet, but we plan to get it working soon.","title":"Software Documentation"},{"location":"win23team13/#autonomous-laps","text":"As part of the class deliverables and as preparation for the final project, here are our autonomous laps videos: Donkey Sim Local Computer: https://youtu.be/lXEStSEVikQ GPU training: https://youtu.be/4_BzKP9-XAQ External Server: https://youtu.be/Yvo1yqRJhX4 Physical Robot DonkeyCar: https://youtu.be/bPUSS2g0Ves Lane detection using OpenCV + ROS2: https://youtu.be/omcDCBSrl2I Inner lane: https://youtu.be/9hN8HUlGcas Outer lane: https://youtu.be/nXZNPscVlX0 GPS: https://youtu.be/Y3I9AWW1R6o","title":"Autonomous Laps"},{"location":"win23team13/#acknowledgements","text":"Thanks Prof. Jack Silberman and TAs Moises Lopez and Kishore Nukala for an awesome quarter! See you in DSC 178 next quarter, professor Jack ;)","title":"Acknowledgements"},{"location":"win23team13/#credit-and-references","text":"Jetson Nano Case Design: https://www.thingiverse.com/thing:3532828 Lidar (LD06) Python Tutorial: https://github.com/henjin0/LIDAR_LD06_python_loder PyVESC: https://github.com/LiamBindle/PyVESC SLAM tutorial, Dominic Nightingale. https://gitlab.com/ucsd_robocar/ucsd_robocar_nav1_pkg/-/tree/master/","title":"Credit and References"},{"location":"win23team14/","text":"ECE - MAE 148 Team 14 Winter 2023 Repository Project Goal: We used the OAK-D camera to run object detection (within the camera) and track different traffic details (common signs, speed limits, etc.) Software and Hardware Description: The OAK-D and depthAI are AI-enabled stereo cameras that allow for depth perception and 3D mapping. They are powerful tools for computer vision applications, such as object detection and tracking. The depthAI is a board that enables faster processing of images by offloading the computational workload from the main processor to dedicated hardware. YOLO (You Only Look Once) is a real-time object detection system that is capable of detecting objects in an image or video feed. It is a popular deep learning model that is used in many computer vision applications, including self-driving cars and robotics. PyVesc is a Python library for controlling VESC-based motor controllers. The VESC is an open-source ESC (Electronic Speed Controller) that is widely used in DIY robotics projects. PyVesc allows for easy communication with the VESC and provides an interface for setting motor parameters and reading sensor data. Project Overview: We first used the OAK-D and depthAI to detect stop signs in the robot's field of view. Then, we executed the deep learning model YOLO to process the camera feed and identify the stop sign(text detection can be another method to achieve the same function). Once the stop sign is detected, we implemented PyVesc to send a command to the motor controller to stop the robot and started to set up the OAK-D and depthAI cameras by installing the necessary software libraries. YOLO is capable of detecting multiple objects simultaneously, so we needed to filter out the stop sign from other detected objects. However, we needed a blob converter to take different data types and convert them into a Binary Large Object (BLOB) that could fit in our code. Finally, once the stop sign is detected, we accessed PyVesc to send a command to the motor controller to stop the robot. In summary, the integration of OAK-D, depthAI, YOLO, and PyVesc allows for efficient and accurate stop sign detection and safe stopping of the robot. This implementation can be further customized and optimized for specific robotic platforms and use cases. Final Projet Presentation: https://docs.google.com/presentation/d/1BTMwfktHvDzfzYd6oSnHeaQTEBbtYg8wEMnqoWkamHE/edit?usp=sharing Final Project Video: https://drive.google.com/file/d/1OnO5qWczQbH_aVrgKAtejwLMw6-fFSRW/view?usp=share_link Team Members: Anish Kulkarni, Manuel Abitia, Zizhe Zhang. Special Thanks to: Professor Silberman, Kishore, Moises, and Freddy C. the Robot.","title":"Team 14"},{"location":"win23team15/","text":"Final Project Repository for Team 15 of the 2023 Winter Class MAE ECE 148 at UCSD Our Final Project uses the AI controlled autonomous vehicle developed in early course sections to implement a driving protocol based on hand signals. To do this, we utilize the GPS-tracking and following code developed/given in class, and combined it with the DepthAI gesture recognition software pack. Our idea developed from our interest in the OAK-D Lite camera\u2019s stereo vision system and ability to run DepthAI within the camera for processing. To use both of these functions, we chose to run a hand detection program that uses Google Mediapipe to combine these features in a unique project that no one on our team had tried before. Using this bleeding edge hardware was really interesting and showed the potential in the given components and also in general the viability of gesture-based control even in relatively low-cost projects. Below are examples of how our hand detection tracker will recognize our gestures Car Physical Setup Our car setup used a large piece of acrylic to connect across the RC car strut towers to support all of our electronics. The acrylic had two grooves along the entire length that were 3 inches apart, allowing us to reconfigure our electronics layout as the class progressed. We knew we would be given different devices through the length of the class, so this modular setup gave us the ability to adapt to them quickly rather than redesigning everytime. We also made heave use of 3D printing, which further cemented the viability of our acrylic rail mount system, since we just had to design a mount that could take mounting screws three inches apart. This enabled us to quickly and with low effort include different configurations and components. We then further used 3D-printed components to functionalize parts of our design, such as a height-adjustable camera mount, and a lidar mount employing the same functionality. Our camera setup was originally a rigid mount on the front of the car, but as we tested our initial Donkey Car laps on the outdoor track we recognized the need for adjusting our angle to detect the lines better. This led to our tall hinging mount that gave us better viewing angles that we could set up within seconds for testing. Software In total, we used three different software packages either from discord, or developed in the course of the class, those beeing the depthai software pack, the d5 GPS control system and the donkeycar Software. depthai software pack To detect hand gestures, we use the Luxonis DepthAI Software on the Luxonis OAK-D stereo-vision camera. This software pack brings plenty of functionality, from simple hand tracking, two two-handed interactions, all the way to gesture recognition. We then modified the existing codebase to suit our needs, including specialized code to regognize a set of largely custom, non-included hand gestures: - Thumbs-up gesture to activate driving - Held up flat hand as a stop signal - different number of fingers (with the thumb not pointing outwards) changes the speed in a four level control scheme - pointing the index finger with thumb outstreched left or right to change direction of steering d5 GPS-control In the d5 subdirectory, we used the donkeycar interfaces to program a simple GPS training and following routine to generate a path for the car to follow. This part of the project is still partly in development, as currently only the speed can be varied with hand signals. We largely reused tech generated and tought to us in the course of the class, providing a nice framework on which to test our system. DonkeyCar Software We use the preconfigured DonkeyCar software package to facilitate control and interfacing of the Jetson nano embedded system with the rest of the RC car. We implemented a special interrupt in the signal chain to be able to pass in different speed values, which are first stored by the modified DepthAI software pack into the file finalproject/comm.txt as a simple string command corresponding to the recognized hand sign. which is then read into the DonkeyCar VESC control subroutine to change the throttle values dynamically. Operation One can either run the depthai software on its own with the VESC class implemented right in the file demo_bdf.py by just executing it with all lines commented in, or, if it is desired to just utilize the throttle values, comment out the VESC related code, run the demo_bpf.py and then simultaneaously run the d5 donkeycar code with ./python manage.py drive. Then, steering will be controlled by the GPS following algorithm, while the speed is controlled via the detected hand signals. The follwing youtube videos show the basic functionality of our project, and how it interacts with the complex drivetrain of the car. Basic Command Gesture Throttle Changing Values Team 15: Boba Team Reinwei Bai Manu Mittal Reisandy Lamdjani Moritz Wagner","title":"Team 15"},{"location":"win23team15/#final-project-repository-for-team-15-of-the-2023-winter-class-mae-ece-148-at-ucsd","text":"Our Final Project uses the AI controlled autonomous vehicle developed in early course sections to implement a driving protocol based on hand signals. To do this, we utilize the GPS-tracking and following code developed/given in class, and combined it with the DepthAI gesture recognition software pack. Our idea developed from our interest in the OAK-D Lite camera\u2019s stereo vision system and ability to run DepthAI within the camera for processing. To use both of these functions, we chose to run a hand detection program that uses Google Mediapipe to combine these features in a unique project that no one on our team had tried before. Using this bleeding edge hardware was really interesting and showed the potential in the given components and also in general the viability of gesture-based control even in relatively low-cost projects. Below are examples of how our hand detection tracker will recognize our gestures","title":"Final Project Repository for Team 15 of the 2023 Winter Class MAE ECE 148 at UCSD"},{"location":"win23team15/#car-physical-setup","text":"Our car setup used a large piece of acrylic to connect across the RC car strut towers to support all of our electronics. The acrylic had two grooves along the entire length that were 3 inches apart, allowing us to reconfigure our electronics layout as the class progressed. We knew we would be given different devices through the length of the class, so this modular setup gave us the ability to adapt to them quickly rather than redesigning everytime. We also made heave use of 3D printing, which further cemented the viability of our acrylic rail mount system, since we just had to design a mount that could take mounting screws three inches apart. This enabled us to quickly and with low effort include different configurations and components. We then further used 3D-printed components to functionalize parts of our design, such as a height-adjustable camera mount, and a lidar mount employing the same functionality. Our camera setup was originally a rigid mount on the front of the car, but as we tested our initial Donkey Car laps on the outdoor track we recognized the need for adjusting our angle to detect the lines better. This led to our tall hinging mount that gave us better viewing angles that we could set up within seconds for testing.","title":"Car Physical Setup"},{"location":"win23team15/#software","text":"In total, we used three different software packages either from discord, or developed in the course of the class, those beeing the depthai software pack, the d5 GPS control system and the donkeycar Software.","title":"Software"},{"location":"win23team15/#depthai-software-pack","text":"To detect hand gestures, we use the Luxonis DepthAI Software on the Luxonis OAK-D stereo-vision camera. This software pack brings plenty of functionality, from simple hand tracking, two two-handed interactions, all the way to gesture recognition. We then modified the existing codebase to suit our needs, including specialized code to regognize a set of largely custom, non-included hand gestures: - Thumbs-up gesture to activate driving - Held up flat hand as a stop signal - different number of fingers (with the thumb not pointing outwards) changes the speed in a four level control scheme - pointing the index finger with thumb outstreched left or right to change direction of steering","title":"depthai software pack"},{"location":"win23team15/#d5-gps-control","text":"In the d5 subdirectory, we used the donkeycar interfaces to program a simple GPS training and following routine to generate a path for the car to follow. This part of the project is still partly in development, as currently only the speed can be varied with hand signals. We largely reused tech generated and tought to us in the course of the class, providing a nice framework on which to test our system.","title":"d5 GPS-control"},{"location":"win23team15/#donkeycar-software","text":"We use the preconfigured DonkeyCar software package to facilitate control and interfacing of the Jetson nano embedded system with the rest of the RC car. We implemented a special interrupt in the signal chain to be able to pass in different speed values, which are first stored by the modified DepthAI software pack into the file finalproject/comm.txt as a simple string command corresponding to the recognized hand sign. which is then read into the DonkeyCar VESC control subroutine to change the throttle values dynamically.","title":"DonkeyCar Software"},{"location":"win23team15/#operation","text":"One can either run the depthai software on its own with the VESC class implemented right in the file demo_bdf.py by just executing it with all lines commented in, or, if it is desired to just utilize the throttle values, comment out the VESC related code, run the demo_bpf.py and then simultaneaously run the d5 donkeycar code with ./python manage.py drive. Then, steering will be controlled by the GPS following algorithm, while the speed is controlled via the detected hand signals. The follwing youtube videos show the basic functionality of our project, and how it interacts with the complex drivetrain of the car. Basic Command Gesture Throttle Changing Values","title":"Operation"},{"location":"win23team15/#team-15-boba-team","text":"Reinwei Bai Manu Mittal Reisandy Lamdjani Moritz Wagner","title":"Team 15: Boba Team"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/","text":"UCSD Robocar Using RC Controller Arduino Micro Pro - Leonard Compatible 22Mar23 - V2.0 ExpressLRS 2.4GHz (ELRS) Radio Long Range Digital #Here are the radios we have for UCSD evGKart #There radios use the open source ELRS #There are several advantages on ELRS: Long Range Low Latency, several suppliers, software upgradable #There are few options for radios. We got the radios below because they were compacts, seemed rugged, and received good reviews #These radios will allow us to keep it all digital without the need to use PWM/PPM. ex: Radio UART - > Single Board Computer (SBC). We will use a microcontroller (MCU) to translate near real-time the protocol used on the radios into serial communication with a SBC using USB and also use the MCU for the emergency stop (off) [EMO] directly from the radio command vs. using separate radio for the EMO. Since the radio for PPM/PWM was affordable we have one too in case we want to directly control RC Cars and or use a multiplexer as part of our EMO. This would require two PWM like cables for each radio channel used. BetaFPV LiteRadio 3 Pro Radio Transmitter- ELRS (Supports External Nano TX Module) Links Image https://betafpv.com/collections/tx/products/literadio-3-pro-radio-transmitter https://support.betafpv.com/hc/en-us/articles/5987468200601-Manual-for-Lite-Radio3-Pro https://www.getfpv.com/betafpv-literadio-3-pro-radio-transmitter-elrs-supports-external-nano-tx-module.html MATEKSYS ExpressLRS 2.4GHz Receiver - ELRS R24 D Links Image https://www.getfpv.com/mateksys-expresslrs-2-4ghz-receiver-elrs-r24-d.html MATEKSYS ExpressLRS 2.4GHz Receiver - PWM ELRS-R24-P6 Links Image https://www.getfpv.com/mateksys-expresslrs-2-4ghz-receiver-pwm-elrs-r24-p6.html Upgrading the ELRS firmware 24Mar23 #Get back to revise these to show version 3.2.0 again. I had to use 3.2.0 because it is the version that ELRS configurator has for the receiver. It seems I need to keep the ERLSv2.luna vs3 for the radio to read the luna script. I hope they will fix it in the future. _#There is a good software that helps on on the upgrade https://www.expresslrs.org/quick-start/installing-configurator/ #They support Windows, Mac, and Linux #Ideally you upgrade the radios Tx and Rx on the same session to make sure they have the same firmware version #Also if you have multiple of the same Tx and Rx doing them consecutively can save you time and help reduce the risk of having different firmware versions. #For the BETA FPV 3 PRO https://www.expresslrs.org/quick-start/transmitters/betfpvlr3pro/ #Turn on the TX radio #Plum the USB C cable connected to your computer #The radio will ask what connection to use #Select USB Serial (Debug) #You will need to look for the Radio in the connections options #Version 2.5.2 #Build & Flash #The first time it will take a while to download, install tools and build the firmware #Download and save on your computer the the LUA Script file #Unplug the USB cable, turn off the radio, turn on again #To upload the LUA script to the radio, unplug the USB cable if connected, connect it again #Select USB Storage (SD) #Then you can use your computer to upload the LUA script you saved earlier https://www.expresslrs.org/quick-start/transmitters/lua-howto/ Download the ELRSv3 Lua Script (you can simply right-click, save-as) into your radio's SD Card under the Scripts/Tools #elrsV2.lua #Also, let's delete the older .lua script from the root of the DISK_IMG of the radio #Remove the USB cable from the radio #Hold the right top joystick to the left for 1~2 seconds #Tools #EspressLRS #See if the configuration loads, here is where I have problems with V3.x for some problem. V2.5.2 works just fine #The BetaFPV LiteRadio 3 Pro Radio Transmitter uses the two smaller joystick to navigate the configuration menu Moving the right side change the GUI on the display Moving the right side to the left and holding it for few seconds get you into the settings Receiver MATEKYS R24-D 2.4Ghz https://www.expresslrs.org/quick-start/receivers/matek2400/ If this is the first time you're flashing/updating your receiver or you're updating it from a previous 2.x firmware via WiFi, first ensure that it has version 2.5.2. Once it has the 2.5.2 flashed, update to 3.x. #Connect to the Wifi Network the receiver has created. It should be named something like ExpressLRS RX with the same expresslrs password as the TX Module Hotspot. #After the Rx radio boots, wait to see the LED flashing quick. That is an indicadion that its Access Point and web server is running. #Using a web browser http://10.0.0.1/ #Connect to the website of the device and upload new firmware Then if you connect the receiver to your local WiFi you can get to it by usings its IP address or name. You can scan the network and look for a device called elrs_rx ex: http://elrs_rx.local #Because we will be on the field and not necessarily close to WiFi, let\u2019s leave the AP of the radio on so we can configure it. Not very safe since someone can connect to it while the AP is on. We will check how to protect it with a password on it a bit later. Moreover, I did not see how to name the radios, it would be hard to know what Rx radios is what in the network. #Let's get the Rx to be on 115200 baud rate #Just connect to the webinterface of the Rx and set the baudrate #We need to use a USB to TTL cable - example from Amazon #Or you can use the embedded WiFi, how cool is that? Lets try the UART first Red wire of the USB to TTL = + Black wire of the USB to TTL = - Green = White = #Press and hold the boot button while connecting the USB to TTL device to your computer http://www.mateksys.com/?portfolio=elrs-r24#tab-id-3 For ELRS-R24-D, if update from 2.x to 3.x, Pls select target MATEK 2400 RX R24D and click on Force Flash https://github.com/kkbin505/Simple_RX Readme.md CRSF decode library for arduino atmega32u4/328p. Based on arduino SBUS decode, modified to decode crsf protocol from elrs receiver #Let's keep it simple first just by using Arduinos, then we will try a Raspberry PI Pico, and then UCSD DRTC if we want to make everything CAN https://www.amazon.com/gp/product/B09C5H78BP/ref=ppx_yo_dt_b_asin_title_o01_s02?ie=UTF8&psc=1 #I got this coming too so we use a better USB connector, USB C vs. uUSB. #I just did not see there was a version with USB C earlier. https://www.amazon.com/gp/product/B0BCW67NJP/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1 https://github.com/kkbin505/Simple_RX Some Background and References ELRS Radio (Long Range Digital) TCIII \u2014 03/17/2023 1:11 PM Hi Jack. I spent half of today learning about how to update (flash) the BETAFPV gamepad/plug-in transmitter/receiver software. Both the gamepad or the plug-in transmitter software version has to be the same as the receiver and vice versa. The gamepad configuration and update flashing can be done with the BETAFPV Configurator while even BETAFPV uses the ELRS Configurator to update the plug-in transmitter and receiver. Nothing like consistency. \ud83d\ude04 The BETAFPV LiteRadio 3 gamepad RC transmitter can output 25, 50, or 100 mw while the plug-in RC transmitter can output 100, 250, or 500 mw and comes with ELRS version 2.01. The Matek ELRS six channel PWM receiver comes with ELRS version 3.0 while the BETAFPV Nano serial output receiver comes with ELRS version 1.0.0-RC5 so I am going to have to reprogram both receivers with ELRS version 2.01. These LoRa gamepads and receivers are definitely not for beginners. \ud83d\ude32 TCIII \u2014 Yesterday at 7:06 AM Hi Jack. I think that it would be best to converse about the Expresslrs LoRa gamepad project on DM until we get the circuitry and software validated, otherwise we might wind up with DC Users trying to implement a LoRa gamepad system that has not been full vetted for functionality and have substantial issues. TCIII \u2014 Yesterday at 7:25 AM Hi Jack. I bought a couple of Arduino Pro Micros so that I could use the crsf_decode_hid.ino sketch from https://github.com/kkbin505/Simple_RX The sketch compiled just fine after I installed the additional required libraries and I will connect the serial output of the BetaFPV serial output Nano receiver to the Pro Micro and see what kind of output I get using the serial monitor. The Arduino Joystick Library (https://github.com/MHeironimus/ArduinoJoystic+kLibrary) is used by the crsf_decode_hid.ino sketch, but the included sketches in the library are designed to take individual joystick and button inputs connected to the Pro Micro and and produce a HID joystick that can be used by PCs for games. Could you please take a look at the crsf_decode_hid.ino sketch as I understand the debug portion, but I don't see where he is outputting the decoded crsf gamepad joystick/button HID values on the USB port. If the crsf_decode_hid.ino sketch is really making the Nano Receiver crsf output look like a HID gamepad then we have it made, though it does require the Pro Micro to interface the Receiver to the SBC as an HID device. \ud83d\ude42 I am using a BetaFPV LiteRadio 3 RC gamepad to do the testing compared to your BetFPV LiteRadio 3 Pro. All of the BetaFPV products are still using Expresslrs version 2.x when Expresslrs version 3.x is now available. The BetaFPV LiteRadio 3 can only be programmed with the BetaFPV Configurator so that gamepad is limited to Expresslrs version 2.x where as your BetFPV LiteRadio 3 Pro can be programmed with the Expresslrs Configurator and can be programmed with Expresslrs 3.x. Remember, both the RC gamepad and the receiver must have the same software version of Expresslrs and pass phrase to bind correctly. GitHub GitHub - kkbin505/Simple_RX: crsf protocol decode for avr crsf protocol decode for avr. Contribute to kkbin505/Simple_RX development by creating an account on GitHub. GitHub - kkbin505/Simple_RX: crsf protocol decode for avr GitHub GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... An Arduino library that adds one or more joysticks to the list of HID devices an Arduino Leonardo or Arduino Micro can support. - GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... This afternoon I will try to determine the voltage of the BetaFPV Nano serial channel receiver output. It will either be 5 vdc or 3.3 vdc. The Nano receiver is powered by 5 vdc, but there is a 3.3 vdc on the Nano receiver pwb because the receiver/processor and WIFI ICs work on 3.3 vdc. So it would seem to me that the serial channel is a 3.3 vdc signal output which will be compatible with either the Rpi or Nano GPIO bus for onboard crsf decoding. Though I like the fact that the Pro Micro, when programmed with the crsf_decode_hid.ino sketch looks just like a HID compliant game controller which makes it portable between RC cars. Additionally the Joystick Descriptor in the crsf_decode_hid.ino sketch allows the customizing of the crsf data stream as to the type of LoRa game controller in use. TCIII \u2014 Yesterday at 9:53 AM Observations concerning BetaFPV Expresslrs products: 1. Only the BetaFPV LiteRadio 3 Pro can be programmed with the Expresslrs Configurator, unlike the LiteRadio 3 and the LiteRadio 2 SE which require the BetaFPV Configurator, which allows access to the latest Expresslrs version 3.x. 2. The LiteRadio 3 and the LiteRadio 2 SE must programmed with the BetaFPV Configurator because they use a custom version of Expresslrs which is stuck at version 2.x.\ud83d\ude41 3. All of the BetaFPV receivers can be programmed with the Expresslrs Configurator, but keep in mind that both the BetaFPV RC gamepads and receivers must have the same version of the software and pass phrase to bind. 4. Therefore if you are using a LiteRadio 3 or a LiteRadio 2 SE, you cannot upgrade the software of any of the BetaFPV receivers to Expresslrs 3.x or you will lose binding capability. 5. The default baud rate of the BetaFPV Nano serial channel receiver UART is 420000 baud so I had to use the Expresslrs Configurator to change the Nano receiver's UART baud rate to 115200 baud to work with the Pro Micro. 6. Unfortunately the Expresslrs Configurator will only show Expresslrs version 2.5 and up for flashing and the Nano receiver for my purposes needed software version 2.x to update the UART baud rate. 7. Fortunately the Expresslrs Configurator allows access to the Expresslrs software version archives so I could use Expresslrs version 2.0 to update the UART baud rate. \ud83d\ude42 Since I bought the BetaFPV LiteRadio 3, which has an external transmitter port, I also purchased the BetaFPV Nano RF TX Module that can be programmed with the Expresslrs Configurator fortunately. The stock BetaFPV LiteRadio 3 has an output of 100 mw while the BetaFPV Nano RF TX Module can be programmed for outputs of 100, 250, and 500 mw. The higher transmitter output power will obviously result in shortened battery run times and are probably not necessary as 100 mw can be good for over 10 km LOS. \ud83d\ude04 JackSilb \u2014 Yesterday at 10:02 AM You need a USB to TTL to connect it to a Jetson Nano or RPI USB no? Unless you want go direct into the I/Os. TCIII \u2014 Yesterday at 10:06 AM Using the Pro Micro programmed with the crsf_decode_hid.ino sketch allows the BetaFPV Nano serial channel receiver appear to be a HID compliant game controller, like a BT gamepad, to either the Rpi or the Nano without any additional hardware. JackSilb \u2014 Yesterday at 10:06 AM I have the Lite Radio 3 Pro with the cute small display. That will be useful for displaying some info. JackSilb \u2014 Yesterday at 10:07 AM We had something similar for the Teensy too. Stopped using it once we got the VESCs. TCIII \u2014 Yesterday at 10:07 AM That is excellent as you can program any BetaFPV receiver with the latest Expresslrs software. JackSilb \u2014 Yesterday at 10:08 AM Here is the design challenge. If you are using the ELRS radio UART direct to the JTN (Jetson Nano) or RPI and from there USB or Can to the VESC, how can we use the Multiplexer for the EMO. That works only for PWM. On the VESC you can configure a pin for a EMO, we would need a I/O from the Rx radio to bring it 1/0 for EMO direct into the VESC vs. using the JTN or RPI. Ideally, I will find a Rx radio with UART and some I/O capability that I can use a channel, lets say Channel 4 as the EMO switch. JackSilb \u2014 Yesterday at 10:21 AM Maybe this is I can pass the UART info along. Use the PWM for the EMO on/off (1/0) at the VESC TCIII \u2014 Yesterday at 10:31 AM The UART data is in crsf format and will need to be decoded using a Python Program: https://pypi.org/project/crsf-parser/ There are multichannel BetaFPV Expresslrs PWM receivers and I have one: https://www.amazon.com/BETAFPV-ExpressLRS-Compatible-Multirotors-Helicopters/dp/B09WHLJ2GN/ref=sr_1_2?crid=17HV1H415IRJR&keywords=BETAFPV+ExpressLRS+Micro+Receiver&qid=1679420003&sprefix=betafpv+expresslrs+micro+receiver%2Caps%2C107&sr=8-2 PyPI crsf-parser A package to parse and create CRSF (Crossfire) frames, developed primarily to interoperate with ExpressLRS Image BETAFPV ExpressLRS Micro Receiver Support 5 CH PWM Outputs Failsafe... Binding Procedure The Micro receiver comes with officially major release V2.0.0 protocol and has not been set for a Binding Phrase. So please make sure the RF TX module works on officially major release V2.0.0 protocol and no Binding Phrase has been set beforehand. Enter binding mode by plugging ... JackSilb \u2014 Yesterday at 10:42 AM I guess we can do this with ELRS UART -> Arduino or Teensy USB HID -> JTN or RPI and Arduino or Teensy I/O to the disable pin of the VESC. As long as the Arduino / Teensy code is solid and we have a watchdog we should be good. Adding the MCU in the mix get us going with lots of future opportunities... I need this for our evGoKart ASAP. I need to get some time to play with me. Lots of work(x2) on on the way Do you want me to share with you our work using the Teensy 4.0 including the PCB? I can send you a PCB too. TCIII \u2014 Yesterday at 10:45 AM I will have more time on Wednesday to test out the Pro Micro setup with the BetaFPV LoRa gamepad and receiver and get back to you with the results. TCIII \u2014 Yesterday at 10:46 AM Address: 11859 SE 91st Circle, Summerfield, FL 34491. JackSilb \u2014 Yesterday at 10:47 AM Can we do the Arduino code to be compatible with the Raspberry Pico? https://www.youtube.com/watch?v=Q97bFwjQ_vQ YouTube Jan Lunge Pi Pico + KMK = the perfect combo for Custom Keyboards Image https://www.youtube.com/watch?v=__QZQEOG6tA YouTube Print 'N Play Use A Raspberry Pi Pico as a HID [Gamepad, Keyboard, Mouse, and Mul... Image Probably a better HW than Arduino Pro Micro TCIII \u2014 Yesterday at 10:56 AM Probably as the Arduino sketch code is C++. JackSilb \u2014 Yesterday at 11:04 AM I purchased few of the Arduinos and Raspberry PI to experiment. Image Charging the controller already. We go from there. JackSilb \u2014 Yesterday at 11:12 AM Yeah, I have a similar from MATEKSYS too. At UCSD, I went away from the PWMs. We go from the JTN to the VESC using USB. The VESC can use the PPM input as output to drive a servo. My current need is for the autonomous evGoKart. Talking you made me find a solution for long distance EMO cheap vs. using the long range telemetry 3DR like only for the EMO. We were using it for the JoStick too with a ROS application on an RPI. Too complex. I will give a try using ELRS UART -> (Arduino Pro Micro or Teensy or Raspberry PICO) USB HID -> USB (Jetson or RPI) and (Arduino Pro Micro or Teensy or Raspberry PICO) I/O to the disable pin of the VESCs. We have one VESC for the throttle and one for the steering. TCIII \u2014 Yesterday at 11:34 AM I have a whole bunch of 3DR Telemetry Transceivers on 915 Mhz. \ud83d\ude42 They were only good for LOS even at 100 mw and highly directional too. \ud83d\ude41 TCIII \u2014 Yesterday at 11:38 AM I bought three of the Hiltego Arduino Pro Micros to work with the BetaFPV Nano receiver and all three programmed just fine with a Sparkfun version of Blinky as a test of their functionality. \ud83d\ude42 I have one Rpi PICO that Ed convinced me to buy as it it very fast compare to the Arduino Minis. REMEMBER that the BetaFPV LiteRadio 3 and the Pro must have the left joystick in the down position when you turn them on or they will buzz and flash the power button RED. As a result of this characteristic, don't energize your car ESC until you have the left joystick at the neutral position (assuming Mode 2) which is basically straight up or your car will go into reverse at full speed.\ud83d\ude32 TCIII \u2014 Today at 6:16 AM Hi Jack. I measured the Nano receiver serial output signal voltage with my digital o'scope this morning and it appears to be 3.43 v which should be safe as an input to either the Nano or the Rpi GPIO bus for direct decoding of the crsf signal. TCIII \u2014 Today at 7:19 AM Hi Jack. I connected the BetaFPV Nano serial output receiver to the Rx input of the Pico Micro running the crsf_decode_hid.ino sketch and viewed the Pico Micro USB HID output in the PC Game Controller Panel \"game controller settings\". I have the RC gamepad frame rate set at the default 150 frames/sec and the joystick display responses are virtually instantaneous and smooth unlike some gamepads. \ud83d\ude42 The \"+\" cursor in the X Axis / Y Axis box is presently controlled by the right joystick horizontal and vertical axes though the \"+\" moves in the opposite direction from the joystick direction of the vertical axis on the gamepad while the left joystick vertical axis (throttle) controls the Z axis red bar and the horizontal axis controls the X axis rotation. The gamepad left push on push off SA switch controls the Y rotation red bar while the left three way switch SB controls the Z rotation red bar. Observations concerning the Joystick axes and switches definitions: The Joystick HID descriptor report, shown below, controls the definition of the joystick axes and switches: Joystick_ Joystick(JOYSTICK_DEFAULT_REPORT_ID,JOYSTICK_TYPE_GAMEPAD, 0, 0, // Button Count, Hat Switch Count true, true, true, // X, Y, Z true, true, true, // Rx, Ry, Rz false, false, // Rudder, Throttle false, false, false); // Accelerator, Brake, Steering Based on a search of the IoT for information concerning the HID descriptor report, the HID descriptor shown above does not match any of the HID descriptor report documentation I could find. I suspect that the HID descriptor report shown above is unique to the Arduino Joystick Library requirements and we need to understand how to modify the HID descriptor report to meet the DC BT gamepad joystick/button requirements. TCIII \u2014 Today at 8:05 AM So we now have a choice on how we connect the LoRa RC gamepad receivers to our SBCs: Connect the Nano receiver UART serial output to a serial Rx pin on the SBC GPIO bus and create a part (https://pypi.org/project/crsf-parser/) that mimics a joystick gamepad or Connect the Nano receiver UART serial output to a Pico Micro running the crsf_decode_hid.ino sketch which makes the crsf output of the Nano receiver look like a HID gamepad (game controller) though it might require some modification of the Joystick HID descriptor report in the sketch to get the RC gamepad to work with DC as a BT gamepad. Comments? PyPI crsf-parser A package to parse and create CRSF (Crossfire) frames, developed primarily to interoperate with ExpressLRS Image TCIII \u2014 Today at 10:23 AM Hi Jack. Do you think that any of your graduate students would be interested in writing a DC part to decode the crsf data stream frames and provide functional gamepad output? \ud83d\ude42 If so, I will be glad to work with them. TCIII \u2014 Today at 11:30 AM I don't think that the Micro HID output looks completely like a PS4/XBox gamepad so the HID joystick descriptor report will probably have to be adjusted to look like a standard gamepad. Since there is no CONTROLLER_TYPE = defined for this HID RC gamepad, Users will have to use the Joystick Wizard to create a custom joystick? One of the issue I see is that LoRa RC gamepads are Mode 2 gamepads where the throttle is on the left joystick and the steering is on the right joystick. \ud83d\ude41 A standard BT gamepad is a Mode 1 gamepad where the throttle is on the right joystick and the steering is on the left joystick. TCIII \u2014 Today at 12:22 PM I suspect that the only way that this LoRa RC gamepad is really going to be really \"user transparent\" is to create a part that decodes and process the crsf serial data stream to create a standard gamepad? TCIII \u2014 Today at 12:42 PM Hi Jack. Here is a tutorial on how to use an RC Radio transmitter as a gamepad controller: https://www.plop.at/en/rc.html It might give us some insight on modifying, if necessary, the crsf_decode_hid.ino sketch to simulate a generic gamepad? I have contacted the author of the crsf_decode_hid.ino sketch about the joystick HID descriptor report that he used and how he created it. TCIII \u2014 Today at 1:39 PM Hi Jack. Attached is a screen shot of what the output of the RC Gamepad/Pro Micro USB HID Game Controller Panel looks like. The \"+\" cursor in the X Axis / Y Axis box is presently controlled by the right joystick horizontal (Steering) and vertical axes though the \"+\" moves in the opposite direction from the joystick direction of the vertical axis on the gamepad while the left joystick vertical axis (Throttle) controls the Z axis red bar and the horizontal axis controls the X axis rotation. The gamepad left push on push off SA switch controls the Y rotation red bar while the left three way switch SB controls the Z rotation red bar. Image Some background and References PWM PPM Hi guys, I have a question about modifying the controller for the donkey car base on this link https://docs.google.com/document/d/1yx1mF0dgEHLx5S3Vqss8YFFkW4thCrl9fpgVliHcuhw/edit we are able to get the signals from controller to the Arduino after modifying the controller, but now we need to know how to connect the Arduino Leonardo to PWM. Ehsan, the Arduino connects to the JTN or RPI. The PWM board does not change until we have the Teensy in the loop. Therefore, the PWM board is the same. It uses I2C to communicate with the JTN or RPI. That is why it is not in the instructions. Basically the Arduino is simulating a JS0 (Joystick) in a Linux machine. It converts PWM signals from the RC controllers as it was a Game/Pad Joystick. The PWM board is still the same, nothing changes. You need to get the latest Donkey, 3.1.1 if I am not mistaken, and change the Joystick Type on myconfig.py along with your calibration values, Webcam, and use channel 1 for steering, channel 2 for Throttle. The only difference on the myconfig.py for using the Arudino (Leonardo) simulating the Joystick is the setting on the Joystick/game controller type. Everything else is as you were using a regular Donkey setup. https://github.com/n6wxd/wireless-rc-adapter/blob/master/wireless-rc-adapter-2.1/src/PinChangeInterrupt/README.md Arduino Uno/Nano/Mini: All pins are usable Arduino Mega: 10, 11, 12, 13, 50, 51, 52, 53, A8 (62), A9 (63), A10 (64), A11 (65), A12 (66), A13 (67), A14 (68), A15 (69) Arduino Leonardo/Micro: 8, 9, 10, 11, 14 (MISO), 15 (SCK), 16 (MOSI) HoodLoader2: All (broken out 1-7) pins are usable Attiny 24/44/84: All pins are usable Attiny 25/45/85: All pins are usable Attiny 13: All pins are usable Attiny 441/841: All pins are usable ATmega644P/ATmega1284P: All pins are usable Arduino Leonardo/Micro allows interrupts on : 8, 9, 10, 11, 14 (MISO), 15 (SCK), 16 (MOSI) So, let\u2019s use 8, 9, 10 https://github.com/n6wxd/wireless-rc-adapter/blob/master/wireless-rc-adapter-2.1/pwm.ino elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO) const uint8_t RC_PINS[6] = {11, 10, 9, 8, PB2, PB1}; https://github.com/n6wxd/wireless-rc-adapter https://github.com/wireless-rc-adapter/wireless-rc-adapter/wiki We just need 3 channels from the RC Rx The Arduino board we got out of Amazon.com (Pro Micro) it is compatible with Arduino Leonardo firmware not Arduino Pro Micro. Also, it does not have Pin 11 available for a connection. We need to change the pins used at the board and Arduino IDE to be 10,9,8 At pwm.ino From elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO) Ch1 -> Pin 8 Ch2 -> Pin 9 Ch3 -> Pin 10 Also, Ch3 \u201c+\u201d and \u201c-\u201d are used to power the RC Rx Radio Ch3+ (middle pin) -> Pin VCC (5V) Ch3- -> Pin GND (Ground) Note: The power to the Arduino comes from the USB cable that connects to the JTN or RPI. The power to the RC radio comes from the Arduino. https://inventr.io/blogs/arduino/arduino-pro-micro-review-scroller The GPIO for Jetson Nano and RPI are the same. Connect the PWM board using I2C. See instructions for ECE MAE 148. Nothing changes here. The steering servo and the ESC connect to the PWM board, not Arudino. How to Make it Work Install the latest version of the Arduino IDE from here Download the Arduino files from here to a local directory https://github.com/n6wxd/wireless-rc-adapter Plug the Arduino into one of your computer USB port Configure the Arduino GUI to use Arduino Leonardo Configure the USB Port used for the Arduino wireless-rc-adapter-2.1.ino Open wireless-rc-adapter-2.1.ino in the Arduino GUI pwm.ino The Arduino IDE should show a tab for pwm.ino Click over the tab pwm.ino Change the pins as below. Our radio has 3 channels, we just use the first 3 pins numbered 10,9,8 elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO) const uint8_t RC_PINS[6] = {8, 9, 10, PB2, PB1,11}; Save the file led.ino Change back what Steve B. modified for the LEDs From elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO) #define RXLED 13 // RXLED is on pin 17<br> #define TXLED 30 // TXLED is on pin 30<br><br> To elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO) #define RXLED 17 // RXLED is on pin 17<br> #define TXLED 30 // TXLED is on pin 30<br> Save the file Upload the wireless-rc-adapter-2.1.ino into the Aruduino, the LEDs should blink when uploading the software then blink in different patterns depending on software stage See here https://github.com/wireless-rc-adapter/wireless-rc-adapter/wiki wireless-rc-adapter-2.1.ino First make sure you pair your RC Tx and Rx ... To verify that the software is working you can enable the serial debugging. Edit wireless-rc-adapter-2.1.ino Remove the comments in the front of //#define SERIAL_DEBUG and //#define SERIAL_SPD 115200 // >>> Serial-Debug options for troubleshooting <<< define SERIAL_DEBUG // Enable Serial Debug by uncommenting this line define SERIAL_SPD 115200 // Set debug bitrate between 9600-115200 bps (default: 9600) Save the file Upload / run Then using the terminal from the Arduino IDE you can follow the boot process and calibrations. You can also use that to verify the calibration and values the Arduino is receiving from the RC receiver. After you verify that the 3 channels are working, comment the serial DEBUG lines back, save, upload/run // >>> Serial-Debug options for troubleshooting <<< define SERIAL_DEBUG // Enable Serial Debug by uncommenting this line define SERIAL_SPD 115200 // Set debug bitrate between 9600-115200 bps (default: 9600) You can test the JS0 operation in a Linux machine, including the Jetson Nano (JTN) and Raspberry PI (RPI) with #Open a terminal #ls /dev/input/ #this command should show a js0 device listed e.g, #ls /dev/input _#by-id by-path event0 event1 js0 mice #Then lets try reading joystick values #sudo apt-get install joystick #sudo jstest /dev/input/js0 If you don\u2019t see the joystick working on the JTN or RPI, then don\u2019t try to use it on Donkey. Calibration On every startup it tries to load calibration data from the long-term memory and decides if those values are still correct or out of sync. The algorithm triggers a calibration automatically if necessary. Calibration can be triggered manually with powering the adapter while the CAL_CHANNEL is on full duty cycle. In other words the throttle must be up before start and it is automatically starts calibration on boot. When the device is in calibration mode, the leds are flashing steady on the board, and all the channel minimum and maximum values are getting recorded. During this time move all the configured sticks and pots/switches on the transmitter remote to its extents. After there is no more new min's or max's found the algorithm finishing the calibration within CAL_TIMEOUT by checking and saving the values in the long-term memory (eeprom). Leds are flashing twice and turns off, the adapter is now available as joystick on the device it is plugged in.","title":"Controller"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#ucsd-robocar-using-rc-controller","text":"","title":"UCSD Robocar Using RC Controller"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#arduino-micro-pro-leonard-compatible-22mar23-v20","text":"","title":"Arduino Micro Pro - Leonard Compatible 22Mar23 - V2.0 "},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#expresslrs-24ghz-elrs-radio-long-range-digital","text":"#Here are the radios we have for UCSD evGKart #There radios use the open source ELRS #There are several advantages on ELRS: Long Range Low Latency, several suppliers, software upgradable #There are few options for radios. We got the radios below because they were compacts, seemed rugged, and received good reviews #These radios will allow us to keep it all digital without the need to use PWM/PPM. ex: Radio UART - > Single Board Computer (SBC). We will use a microcontroller (MCU) to translate near real-time the protocol used on the radios into serial communication with a SBC using USB and also use the MCU for the emergency stop (off) [EMO] directly from the radio command vs. using separate radio for the EMO. Since the radio for PPM/PWM was affordable we have one too in case we want to directly control RC Cars and or use a multiplexer as part of our EMO. This would require two PWM like cables for each radio channel used.","title":"ExpressLRS 2.4GHz (ELRS) Radio Long Range Digital"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#betafpv-literadio-3-pro-radio-transmitter-elrs-supports-external-nano-tx-module","text":"Links Image https://betafpv.com/collections/tx/products/literadio-3-pro-radio-transmitter https://support.betafpv.com/hc/en-us/articles/5987468200601-Manual-for-Lite-Radio3-Pro https://www.getfpv.com/betafpv-literadio-3-pro-radio-transmitter-elrs-supports-external-nano-tx-module.html","title":"BetaFPV LiteRadio 3 Pro Radio Transmitter- ELRS (Supports External Nano TX Module)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#mateksys-expresslrs-24ghz-receiver-elrs-r24-d","text":"Links Image https://www.getfpv.com/mateksys-expresslrs-2-4ghz-receiver-elrs-r24-d.html","title":"MATEKSYS ExpressLRS 2.4GHz Receiver - ELRS R24 D"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#mateksys-expresslrs-24ghz-receiver-pwm-elrs-r24-p6","text":"Links Image https://www.getfpv.com/mateksys-expresslrs-2-4ghz-receiver-pwm-elrs-r24-p6.html","title":"MATEKSYS ExpressLRS 2.4GHz Receiver - PWM ELRS-R24-P6"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#upgrading-the-elrs-firmware","text":"24Mar23 #Get back to revise these to show version 3.2.0 again. I had to use 3.2.0 because it is the version that ELRS configurator has for the receiver. It seems I need to keep the ERLSv2.luna vs3 for the radio to read the luna script. I hope they will fix it in the future. _#There is a good software that helps on on the upgrade https://www.expresslrs.org/quick-start/installing-configurator/ #They support Windows, Mac, and Linux #Ideally you upgrade the radios Tx and Rx on the same session to make sure they have the same firmware version #Also if you have multiple of the same Tx and Rx doing them consecutively can save you time and help reduce the risk of having different firmware versions. #For the BETA FPV 3 PRO https://www.expresslrs.org/quick-start/transmitters/betfpvlr3pro/ #Turn on the TX radio #Plum the USB C cable connected to your computer #The radio will ask what connection to use #Select USB Serial (Debug) #You will need to look for the Radio in the connections options #Version 2.5.2 #Build & Flash #The first time it will take a while to download, install tools and build the firmware #Download and save on your computer the the LUA Script file #Unplug the USB cable, turn off the radio, turn on again #To upload the LUA script to the radio, unplug the USB cable if connected, connect it again #Select USB Storage (SD) #Then you can use your computer to upload the LUA script you saved earlier https://www.expresslrs.org/quick-start/transmitters/lua-howto/ Download the ELRSv3 Lua Script (you can simply right-click, save-as) into your radio's SD Card under the Scripts/Tools #elrsV2.lua #Also, let's delete the older .lua script from the root of the DISK_IMG of the radio #Remove the USB cable from the radio #Hold the right top joystick to the left for 1~2 seconds #Tools #EspressLRS #See if the configuration loads, here is where I have problems with V3.x for some problem. V2.5.2 works just fine #The BetaFPV LiteRadio 3 Pro Radio Transmitter uses the two smaller joystick to navigate the configuration menu Moving the right side change the GUI on the display Moving the right side to the left and holding it for few seconds get you into the settings","title":"Upgrading the ELRS firmware"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#receiver-matekys-r24-d-24ghz","text":"https://www.expresslrs.org/quick-start/receivers/matek2400/ If this is the first time you're flashing/updating your receiver or you're updating it from a previous 2.x firmware via WiFi, first ensure that it has version 2.5.2. Once it has the 2.5.2 flashed, update to 3.x. #Connect to the Wifi Network the receiver has created. It should be named something like ExpressLRS RX with the same expresslrs password as the TX Module Hotspot. #After the Rx radio boots, wait to see the LED flashing quick. That is an indicadion that its Access Point and web server is running. #Using a web browser http://10.0.0.1/ #Connect to the website of the device and upload new firmware Then if you connect the receiver to your local WiFi you can get to it by usings its IP address or name. You can scan the network and look for a device called elrs_rx ex: http://elrs_rx.local #Because we will be on the field and not necessarily close to WiFi, let\u2019s leave the AP of the radio on so we can configure it. Not very safe since someone can connect to it while the AP is on. We will check how to protect it with a password on it a bit later. Moreover, I did not see how to name the radios, it would be hard to know what Rx radios is what in the network. #Let's get the Rx to be on 115200 baud rate #Just connect to the webinterface of the Rx and set the baudrate #We need to use a USB to TTL cable - example from Amazon #Or you can use the embedded WiFi, how cool is that? Lets try the UART first Red wire of the USB to TTL = + Black wire of the USB to TTL = - Green = White = #Press and hold the boot button while connecting the USB to TTL device to your computer http://www.mateksys.com/?portfolio=elrs-r24#tab-id-3 For ELRS-R24-D, if update from 2.x to 3.x, Pls select target MATEK 2400 RX R24D and click on Force Flash https://github.com/kkbin505/Simple_RX Readme.md CRSF decode library for arduino atmega32u4/328p. Based on arduino SBUS decode, modified to decode crsf protocol from elrs receiver #Let's keep it simple first just by using Arduinos, then we will try a Raspberry PI Pico, and then UCSD DRTC if we want to make everything CAN https://www.amazon.com/gp/product/B09C5H78BP/ref=ppx_yo_dt_b_asin_title_o01_s02?ie=UTF8&psc=1 #I got this coming too so we use a better USB connector, USB C vs. uUSB. #I just did not see there was a version with USB C earlier. https://www.amazon.com/gp/product/B0BCW67NJP/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1 https://github.com/kkbin505/Simple_RX","title":"Receiver MATEKYS R24-D 2.4Ghz"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#some-background-and-references-elrs-radio-long-range-digital","text":"TCIII \u2014 03/17/2023 1:11 PM Hi Jack. I spent half of today learning about how to update (flash) the BETAFPV gamepad/plug-in transmitter/receiver software. Both the gamepad or the plug-in transmitter software version has to be the same as the receiver and vice versa. The gamepad configuration and update flashing can be done with the BETAFPV Configurator while even BETAFPV uses the ELRS Configurator to update the plug-in transmitter and receiver. Nothing like consistency. \ud83d\ude04 The BETAFPV LiteRadio 3 gamepad RC transmitter can output 25, 50, or 100 mw while the plug-in RC transmitter can output 100, 250, or 500 mw and comes with ELRS version 2.01. The Matek ELRS six channel PWM receiver comes with ELRS version 3.0 while the BETAFPV Nano serial output receiver comes with ELRS version 1.0.0-RC5 so I am going to have to reprogram both receivers with ELRS version 2.01. These LoRa gamepads and receivers are definitely not for beginners. \ud83d\ude32 TCIII \u2014 Yesterday at 7:06 AM Hi Jack. I think that it would be best to converse about the Expresslrs LoRa gamepad project on DM until we get the circuitry and software validated, otherwise we might wind up with DC Users trying to implement a LoRa gamepad system that has not been full vetted for functionality and have substantial issues. TCIII \u2014 Yesterday at 7:25 AM Hi Jack. I bought a couple of Arduino Pro Micros so that I could use the crsf_decode_hid.ino sketch from https://github.com/kkbin505/Simple_RX The sketch compiled just fine after I installed the additional required libraries and I will connect the serial output of the BetaFPV serial output Nano receiver to the Pro Micro and see what kind of output I get using the serial monitor. The Arduino Joystick Library (https://github.com/MHeironimus/ArduinoJoystic+kLibrary) is used by the crsf_decode_hid.ino sketch, but the included sketches in the library are designed to take individual joystick and button inputs connected to the Pro Micro and and produce a HID joystick that can be used by PCs for games. Could you please take a look at the crsf_decode_hid.ino sketch as I understand the debug portion, but I don't see where he is outputting the decoded crsf gamepad joystick/button HID values on the USB port. If the crsf_decode_hid.ino sketch is really making the Nano Receiver crsf output look like a HID gamepad then we have it made, though it does require the Pro Micro to interface the Receiver to the SBC as an HID device. \ud83d\ude42 I am using a BetaFPV LiteRadio 3 RC gamepad to do the testing compared to your BetFPV LiteRadio 3 Pro. All of the BetaFPV products are still using Expresslrs version 2.x when Expresslrs version 3.x is now available. The BetaFPV LiteRadio 3 can only be programmed with the BetaFPV Configurator so that gamepad is limited to Expresslrs version 2.x where as your BetFPV LiteRadio 3 Pro can be programmed with the Expresslrs Configurator and can be programmed with Expresslrs 3.x. Remember, both the RC gamepad and the receiver must have the same software version of Expresslrs and pass phrase to bind correctly. GitHub GitHub - kkbin505/Simple_RX: crsf protocol decode for avr crsf protocol decode for avr. Contribute to kkbin505/Simple_RX development by creating an account on GitHub. GitHub - kkbin505/Simple_RX: crsf protocol decode for avr GitHub GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... An Arduino library that adds one or more joysticks to the list of HID devices an Arduino Leonardo or Arduino Micro can support. - GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... This afternoon I will try to determine the voltage of the BetaFPV Nano serial channel receiver output. It will either be 5 vdc or 3.3 vdc. The Nano receiver is powered by 5 vdc, but there is a 3.3 vdc on the Nano receiver pwb because the receiver/processor and WIFI ICs work on 3.3 vdc. So it would seem to me that the serial channel is a 3.3 vdc signal output which will be compatible with either the Rpi or Nano GPIO bus for onboard crsf decoding. Though I like the fact that the Pro Micro, when programmed with the crsf_decode_hid.ino sketch looks just like a HID compliant game controller which makes it portable between RC cars. Additionally the Joystick Descriptor in the crsf_decode_hid.ino sketch allows the customizing of the crsf data stream as to the type of LoRa game controller in use. TCIII \u2014 Yesterday at 9:53 AM Observations concerning BetaFPV Expresslrs products: 1. Only the BetaFPV LiteRadio 3 Pro can be programmed with the Expresslrs Configurator, unlike the LiteRadio 3 and the LiteRadio 2 SE which require the BetaFPV Configurator, which allows access to the latest Expresslrs version 3.x. 2. The LiteRadio 3 and the LiteRadio 2 SE must programmed with the BetaFPV Configurator because they use a custom version of Expresslrs which is stuck at version 2.x.\ud83d\ude41 3. All of the BetaFPV receivers can be programmed with the Expresslrs Configurator, but keep in mind that both the BetaFPV RC gamepads and receivers must have the same version of the software and pass phrase to bind. 4. Therefore if you are using a LiteRadio 3 or a LiteRadio 2 SE, you cannot upgrade the software of any of the BetaFPV receivers to Expresslrs 3.x or you will lose binding capability. 5. The default baud rate of the BetaFPV Nano serial channel receiver UART is 420000 baud so I had to use the Expresslrs Configurator to change the Nano receiver's UART baud rate to 115200 baud to work with the Pro Micro. 6. Unfortunately the Expresslrs Configurator will only show Expresslrs version 2.5 and up for flashing and the Nano receiver for my purposes needed software version 2.x to update the UART baud rate. 7. Fortunately the Expresslrs Configurator allows access to the Expresslrs software version archives so I could use Expresslrs version 2.0 to update the UART baud rate. \ud83d\ude42 Since I bought the BetaFPV LiteRadio 3, which has an external transmitter port, I also purchased the BetaFPV Nano RF TX Module that can be programmed with the Expresslrs Configurator fortunately. The stock BetaFPV LiteRadio 3 has an output of 100 mw while the BetaFPV Nano RF TX Module can be programmed for outputs of 100, 250, and 500 mw. The higher transmitter output power will obviously result in shortened battery run times and are probably not necessary as 100 mw can be good for over 10 km LOS. \ud83d\ude04 JackSilb \u2014 Yesterday at 10:02 AM You need a USB to TTL to connect it to a Jetson Nano or RPI USB no? Unless you want go direct into the I/Os. TCIII \u2014 Yesterday at 10:06 AM Using the Pro Micro programmed with the crsf_decode_hid.ino sketch allows the BetaFPV Nano serial channel receiver appear to be a HID compliant game controller, like a BT gamepad, to either the Rpi or the Nano without any additional hardware. JackSilb \u2014 Yesterday at 10:06 AM I have the Lite Radio 3 Pro with the cute small display. That will be useful for displaying some info. JackSilb \u2014 Yesterday at 10:07 AM We had something similar for the Teensy too. Stopped using it once we got the VESCs. TCIII \u2014 Yesterday at 10:07 AM That is excellent as you can program any BetaFPV receiver with the latest Expresslrs software. JackSilb \u2014 Yesterday at 10:08 AM Here is the design challenge. If you are using the ELRS radio UART direct to the JTN (Jetson Nano) or RPI and from there USB or Can to the VESC, how can we use the Multiplexer for the EMO. That works only for PWM. On the VESC you can configure a pin for a EMO, we would need a I/O from the Rx radio to bring it 1/0 for EMO direct into the VESC vs. using the JTN or RPI. Ideally, I will find a Rx radio with UART and some I/O capability that I can use a channel, lets say Channel 4 as the EMO switch. JackSilb \u2014 Yesterday at 10:21 AM Maybe this is I can pass the UART info along. Use the PWM for the EMO on/off (1/0) at the VESC TCIII \u2014 Yesterday at 10:31 AM The UART data is in crsf format and will need to be decoded using a Python Program: https://pypi.org/project/crsf-parser/ There are multichannel BetaFPV Expresslrs PWM receivers and I have one: https://www.amazon.com/BETAFPV-ExpressLRS-Compatible-Multirotors-Helicopters/dp/B09WHLJ2GN/ref=sr_1_2?crid=17HV1H415IRJR&keywords=BETAFPV+ExpressLRS+Micro+Receiver&qid=1679420003&sprefix=betafpv+expresslrs+micro+receiver%2Caps%2C107&sr=8-2 PyPI crsf-parser A package to parse and create CRSF (Crossfire) frames, developed primarily to interoperate with ExpressLRS Image BETAFPV ExpressLRS Micro Receiver Support 5 CH PWM Outputs Failsafe... Binding Procedure The Micro receiver comes with officially major release V2.0.0 protocol and has not been set for a Binding Phrase. So please make sure the RF TX module works on officially major release V2.0.0 protocol and no Binding Phrase has been set beforehand. Enter binding mode by plugging ... JackSilb \u2014 Yesterday at 10:42 AM I guess we can do this with ELRS UART -> Arduino or Teensy USB HID -> JTN or RPI and Arduino or Teensy I/O to the disable pin of the VESC. As long as the Arduino / Teensy code is solid and we have a watchdog we should be good. Adding the MCU in the mix get us going with lots of future opportunities... I need this for our evGoKart ASAP. I need to get some time to play with me. Lots of work(x2) on on the way Do you want me to share with you our work using the Teensy 4.0 including the PCB? I can send you a PCB too. TCIII \u2014 Yesterday at 10:45 AM I will have more time on Wednesday to test out the Pro Micro setup with the BetaFPV LoRa gamepad and receiver and get back to you with the results. TCIII \u2014 Yesterday at 10:46 AM Address: 11859 SE 91st Circle, Summerfield, FL 34491. JackSilb \u2014 Yesterday at 10:47 AM Can we do the Arduino code to be compatible with the Raspberry Pico? https://www.youtube.com/watch?v=Q97bFwjQ_vQ YouTube Jan Lunge Pi Pico + KMK = the perfect combo for Custom Keyboards Image https://www.youtube.com/watch?v=__QZQEOG6tA YouTube Print 'N Play Use A Raspberry Pi Pico as a HID [Gamepad, Keyboard, Mouse, and Mul... Image Probably a better HW than Arduino Pro Micro TCIII \u2014 Yesterday at 10:56 AM Probably as the Arduino sketch code is C++. JackSilb \u2014 Yesterday at 11:04 AM I purchased few of the Arduinos and Raspberry PI to experiment. Image Charging the controller already. We go from there. JackSilb \u2014 Yesterday at 11:12 AM Yeah, I have a similar from MATEKSYS too. At UCSD, I went away from the PWMs. We go from the JTN to the VESC using USB. The VESC can use the PPM input as output to drive a servo. My current need is for the autonomous evGoKart. Talking you made me find a solution for long distance EMO cheap vs. using the long range telemetry 3DR like only for the EMO. We were using it for the JoStick too with a ROS application on an RPI. Too complex. I will give a try using ELRS UART -> (Arduino Pro Micro or Teensy or Raspberry PICO) USB HID -> USB (Jetson or RPI) and (Arduino Pro Micro or Teensy or Raspberry PICO) I/O to the disable pin of the VESCs. We have one VESC for the throttle and one for the steering. TCIII \u2014 Yesterday at 11:34 AM I have a whole bunch of 3DR Telemetry Transceivers on 915 Mhz. \ud83d\ude42 They were only good for LOS even at 100 mw and highly directional too. \ud83d\ude41 TCIII \u2014 Yesterday at 11:38 AM I bought three of the Hiltego Arduino Pro Micros to work with the BetaFPV Nano receiver and all three programmed just fine with a Sparkfun version of Blinky as a test of their functionality. \ud83d\ude42 I have one Rpi PICO that Ed convinced me to buy as it it very fast compare to the Arduino Minis. REMEMBER that the BetaFPV LiteRadio 3 and the Pro must have the left joystick in the down position when you turn them on or they will buzz and flash the power button RED. As a result of this characteristic, don't energize your car ESC until you have the left joystick at the neutral position (assuming Mode 2) which is basically straight up or your car will go into reverse at full speed.\ud83d\ude32 TCIII \u2014 Today at 6:16 AM Hi Jack. I measured the Nano receiver serial output signal voltage with my digital o'scope this morning and it appears to be 3.43 v which should be safe as an input to either the Nano or the Rpi GPIO bus for direct decoding of the crsf signal. TCIII \u2014 Today at 7:19 AM Hi Jack. I connected the BetaFPV Nano serial output receiver to the Rx input of the Pico Micro running the crsf_decode_hid.ino sketch and viewed the Pico Micro USB HID output in the PC Game Controller Panel \"game controller settings\". I have the RC gamepad frame rate set at the default 150 frames/sec and the joystick display responses are virtually instantaneous and smooth unlike some gamepads. \ud83d\ude42 The \"+\" cursor in the X Axis / Y Axis box is presently controlled by the right joystick horizontal and vertical axes though the \"+\" moves in the opposite direction from the joystick direction of the vertical axis on the gamepad while the left joystick vertical axis (throttle) controls the Z axis red bar and the horizontal axis controls the X axis rotation. The gamepad left push on push off SA switch controls the Y rotation red bar while the left three way switch SB controls the Z rotation red bar. Observations concerning the Joystick axes and switches definitions: The Joystick HID descriptor report, shown below, controls the definition of the joystick axes and switches: Joystick_ Joystick(JOYSTICK_DEFAULT_REPORT_ID,JOYSTICK_TYPE_GAMEPAD, 0, 0, // Button Count, Hat Switch Count true, true, true, // X, Y, Z true, true, true, // Rx, Ry, Rz false, false, // Rudder, Throttle false, false, false); // Accelerator, Brake, Steering Based on a search of the IoT for information concerning the HID descriptor report, the HID descriptor shown above does not match any of the HID descriptor report documentation I could find. I suspect that the HID descriptor report shown above is unique to the Arduino Joystick Library requirements and we need to understand how to modify the HID descriptor report to meet the DC BT gamepad joystick/button requirements. TCIII \u2014 Today at 8:05 AM So we now have a choice on how we connect the LoRa RC gamepad receivers to our SBCs: Connect the Nano receiver UART serial output to a serial Rx pin on the SBC GPIO bus and create a part (https://pypi.org/project/crsf-parser/) that mimics a joystick gamepad or Connect the Nano receiver UART serial output to a Pico Micro running the crsf_decode_hid.ino sketch which makes the crsf output of the Nano receiver look like a HID gamepad (game controller) though it might require some modification of the Joystick HID descriptor report in the sketch to get the RC gamepad to work with DC as a BT gamepad. Comments? PyPI crsf-parser A package to parse and create CRSF (Crossfire) frames, developed primarily to interoperate with ExpressLRS Image TCIII \u2014 Today at 10:23 AM Hi Jack. Do you think that any of your graduate students would be interested in writing a DC part to decode the crsf data stream frames and provide functional gamepad output? \ud83d\ude42 If so, I will be glad to work with them. TCIII \u2014 Today at 11:30 AM I don't think that the Micro HID output looks completely like a PS4/XBox gamepad so the HID joystick descriptor report will probably have to be adjusted to look like a standard gamepad. Since there is no CONTROLLER_TYPE = defined for this HID RC gamepad, Users will have to use the Joystick Wizard to create a custom joystick? One of the issue I see is that LoRa RC gamepads are Mode 2 gamepads where the throttle is on the left joystick and the steering is on the right joystick. \ud83d\ude41 A standard BT gamepad is a Mode 1 gamepad where the throttle is on the right joystick and the steering is on the left joystick. TCIII \u2014 Today at 12:22 PM I suspect that the only way that this LoRa RC gamepad is really going to be really \"user transparent\" is to create a part that decodes and process the crsf serial data stream to create a standard gamepad? TCIII \u2014 Today at 12:42 PM Hi Jack. Here is a tutorial on how to use an RC Radio transmitter as a gamepad controller: https://www.plop.at/en/rc.html It might give us some insight on modifying, if necessary, the crsf_decode_hid.ino sketch to simulate a generic gamepad? I have contacted the author of the crsf_decode_hid.ino sketch about the joystick HID descriptor report that he used and how he created it. TCIII \u2014 Today at 1:39 PM Hi Jack. Attached is a screen shot of what the output of the RC Gamepad/Pro Micro USB HID Game Controller Panel looks like. The \"+\" cursor in the X Axis / Y Axis box is presently controlled by the right joystick horizontal (Steering) and vertical axes though the \"+\" moves in the opposite direction from the joystick direction of the vertical axis on the gamepad while the left joystick vertical axis (Throttle) controls the Z axis red bar and the horizontal axis controls the X axis rotation. The gamepad left push on push off SA switch controls the Y rotation red bar while the left three way switch SB controls the Z rotation red bar. Image","title":"Some Background and References ELRS Radio (Long Range Digital)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#some-background-and-references-pwm-ppm","text":"Hi guys, I have a question about modifying the controller for the donkey car base on this link https://docs.google.com/document/d/1yx1mF0dgEHLx5S3Vqss8YFFkW4thCrl9fpgVliHcuhw/edit we are able to get the signals from controller to the Arduino after modifying the controller, but now we need to know how to connect the Arduino Leonardo to PWM. Ehsan, the Arduino connects to the JTN or RPI. The PWM board does not change until we have the Teensy in the loop. Therefore, the PWM board is the same. It uses I2C to communicate with the JTN or RPI. That is why it is not in the instructions. Basically the Arduino is simulating a JS0 (Joystick) in a Linux machine. It converts PWM signals from the RC controllers as it was a Game/Pad Joystick. The PWM board is still the same, nothing changes. You need to get the latest Donkey, 3.1.1 if I am not mistaken, and change the Joystick Type on myconfig.py along with your calibration values, Webcam, and use channel 1 for steering, channel 2 for Throttle. The only difference on the myconfig.py for using the Arudino (Leonardo) simulating the Joystick is the setting on the Joystick/game controller type. Everything else is as you were using a regular Donkey setup. https://github.com/n6wxd/wireless-rc-adapter/blob/master/wireless-rc-adapter-2.1/src/PinChangeInterrupt/README.md Arduino Uno/Nano/Mini: All pins are usable Arduino Mega: 10, 11, 12, 13, 50, 51, 52, 53, A8 (62), A9 (63), A10 (64), A11 (65), A12 (66), A13 (67), A14 (68), A15 (69) Arduino Leonardo/Micro: 8, 9, 10, 11, 14 (MISO), 15 (SCK), 16 (MOSI) HoodLoader2: All (broken out 1-7) pins are usable Attiny 24/44/84: All pins are usable Attiny 25/45/85: All pins are usable Attiny 13: All pins are usable Attiny 441/841: All pins are usable ATmega644P/ATmega1284P: All pins are usable Arduino Leonardo/Micro allows interrupts on : 8, 9, 10, 11, 14 (MISO), 15 (SCK), 16 (MOSI) So, let\u2019s use 8, 9, 10 https://github.com/n6wxd/wireless-rc-adapter/blob/master/wireless-rc-adapter-2.1/pwm.ino","title":"Some background and References PWM PPM"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo","text":"const uint8_t RC_PINS[6] = {11, 10, 9, 8, PB2, PB1}; https://github.com/n6wxd/wireless-rc-adapter https://github.com/wireless-rc-adapter/wireless-rc-adapter/wiki We just need 3 channels from the RC Rx The Arduino board we got out of Amazon.com (Pro Micro) it is compatible with Arduino Leonardo firmware not Arduino Pro Micro. Also, it does not have Pin 11 available for a connection. We need to change the pins used at the board and Arduino IDE to be 10,9,8 At pwm.ino From","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_1","text":"Ch1 -> Pin 8 Ch2 -> Pin 9 Ch3 -> Pin 10 Also, Ch3 \u201c+\u201d and \u201c-\u201d are used to power the RC Rx Radio Ch3+ (middle pin) -> Pin VCC (5V) Ch3- -> Pin GND (Ground) Note: The power to the Arduino comes from the USB cable that connects to the JTN or RPI. The power to the RC radio comes from the Arduino. https://inventr.io/blogs/arduino/arduino-pro-micro-review-scroller The GPIO for Jetson Nano and RPI are the same. Connect the PWM board using I2C. See instructions for ECE MAE 148. Nothing changes here. The steering servo and the ESC connect to the PWM board, not Arudino.","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#how-to-make-it-work","text":"Install the latest version of the Arduino IDE from here Download the Arduino files from here to a local directory https://github.com/n6wxd/wireless-rc-adapter Plug the Arduino into one of your computer USB port Configure the Arduino GUI to use Arduino Leonardo Configure the USB Port used for the Arduino","title":"How to Make it Work"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#wireless-rc-adapter-21ino","text":"Open wireless-rc-adapter-2.1.ino in the Arduino GUI","title":"wireless-rc-adapter-2.1.ino"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#pwmino","text":"The Arduino IDE should show a tab for pwm.ino Click over the tab pwm.ino Change the pins as below. Our radio has 3 channels, we just use the first 3 pins numbered 10,9,8","title":"pwm.ino"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_2","text":"const uint8_t RC_PINS[6] = {8, 9, 10, PB2, PB1,11}; Save the file","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#ledino","text":"Change back what Steve B. modified for the LEDs From","title":"led.ino"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_3","text":"#define RXLED 13 // RXLED is on pin 17<br> #define TXLED 30 // TXLED is on pin 30<br><br> To","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_4","text":"#define RXLED 17 // RXLED is on pin 17<br> #define TXLED 30 // TXLED is on pin 30<br> Save the file Upload the wireless-rc-adapter-2.1.ino into the Aruduino, the LEDs should blink when uploading the software then blink in different patterns depending on software stage See here https://github.com/wireless-rc-adapter/wireless-rc-adapter/wiki","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#wireless-rc-adapter-21ino_1","text":"First make sure you pair your RC Tx and Rx ... To verify that the software is working you can enable the serial debugging. Edit wireless-rc-adapter-2.1.ino Remove the comments in the front of //#define SERIAL_DEBUG and //#define SERIAL_SPD 115200 // >>> Serial-Debug options for troubleshooting <<< define SERIAL_DEBUG // Enable Serial Debug by uncommenting this line define SERIAL_SPD 115200 // Set debug bitrate between 9600-115200 bps (default: 9600) Save the file Upload / run Then using the terminal from the Arduino IDE you can follow the boot process and calibrations. You can also use that to verify the calibration and values the Arduino is receiving from the RC receiver. After you verify that the 3 channels are working, comment the serial DEBUG lines back, save, upload/run // >>> Serial-Debug options for troubleshooting <<<","title":"wireless-rc-adapter-2.1.ino"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#define-serial_debug-enable-serial-debug-by-uncommenting-this-line","text":"","title":"define SERIAL_DEBUG  // Enable Serial Debug by uncommenting this line"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#define-serial_spd-115200-set-debug-bitrate-between-9600-115200-bps-default-9600","text":"You can test the JS0 operation in a Linux machine, including the Jetson Nano (JTN) and Raspberry PI (RPI) with #Open a terminal #ls /dev/input/ #this command should show a js0 device listed e.g, #ls /dev/input _#by-id by-path event0 event1 js0 mice #Then lets try reading joystick values #sudo apt-get install joystick #sudo jstest /dev/input/js0 If you don\u2019t see the joystick working on the JTN or RPI, then don\u2019t try to use it on Donkey.","title":"define SERIAL_SPD 115200  // Set debug bitrate between 9600-115200 bps (default: 9600)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#calibration","text":"On every startup it tries to load calibration data from the long-term memory and decides if those values are still correct or out of sync. The algorithm triggers a calibration automatically if necessary. Calibration can be triggered manually with powering the adapter while the CAL_CHANNEL is on full duty cycle. In other words the throttle must be up before start and it is automatically starts calibration on boot. When the device is in calibration mode, the leds are flashing steady on the board, and all the channel minimum and maximum values are getting recorded. During this time move all the configured sticks and pots/switches on the transmitter remote to its extents. After there is no more new min's or max's found the algorithm finishing the calibration within CAL_TIMEOUT by checking and saving the values in the long-term memory (eeprom). Leds are flashing twice and turns off, the adapter is now available as joystick on the device it is plugged in.","title":"Calibration"}]}