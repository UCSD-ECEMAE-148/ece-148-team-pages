{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ECE/MAE 148: Introduction to Autonomous Vehicles Documentation is under development.","title":"Welcome to ECE/MAE 148: Introduction to Autonomous Vehicles"},{"location":"#welcome-to-ecemae-148-introduction-to-autonomous-vehicles","text":"Documentation is under development.","title":"Welcome to ECE/MAE 148: Introduction to Autonomous Vehicles"},{"location":"win23team2/","text":"Team 2 ECE/MAE 148 Final Report :wave: The Team: 2 Fast 2 Furious (Left to Right) - Elias Fang (CSE) - Ainesh Arumugam (ECE) - Matthew Merioles (ECE) - Junhao \"Michael\" Chen (MAE) \ud83d\udcdd Project Overview Mario Kart in real-life? That's basically what we did. We designed a boost system similar to those in Mario Kart, where detecting colored pieces of paper on the track can change throttle for a short period of time. Depending on the color of the \"pad\" the car drives over, it will either speed up, slow down, or stop for a few seconds, just like in the game! \ud83c\udfce Our Robot Bird's Eye Front Left Right Back Schematic \ud83c\udf44 Final Project What We Promised Must haves [X] Distinguishing different colors through the camera [X] Adjust the throttle based on the color Nice to haves [X] Have the car detect a flat piece of paper on the track (like a booster pad) [ ] Combine with lane-following algorithm Gantt Chart https://sharing.clickup.com/9010060626/g/h/8cgn7aj-87/769d44f22562beb What We Accomplished Color Detection Used OpenCV for color detection and edge tracing Used color mask algorithm to detect proportion of frame that color takes up Detected multiple colors at the same time Determined HSVs for orange, pink, and blue Demo PyVESC Connection through external webcam Different RPM values are sent through PyVesc to achieve different speed for different colors marked by different states: Blue (Boost) = speed up for 3 sec Pink (Slow)= slow down for 3 sec Orange (Stop) = stop for 3 sec Neutral (Normal) = constant rpm Blue Demo Pink Demo Orange Demo Presentation https://docs.google.com/presentation/d/1oJPRLYIKvHUXEIK9hoYpPFoFAyHuG6sE7ZrU9NQPG8g/edit?usp=sharing Code https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart.py Possible Future Work Change the colored paper into Mario Kart items (mushroom, bananas, etc.) for the car to identify Allow the car to run autonomously on a track and still apply speed changes Race with other teams \ud83d\ude09 \ud83c\udfc1 Autonomous Laps DonkeyCar ROS2 Line Following ROS2 Lanes GNSS Additional Work Done Our team was tasked with implementing a depth feature with our contour function, using depthAI to integrate with the OAK-D Camera! Our updated code is now able to measure how far the contoured object is, measured in cm! [Depth Demo] https://www.youtube.com/watch?v=NYIz7--TpgY [Updated Code] https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart_depth.py Acknowledgements Thanks for Professor Jack Silberman, TA Kishore Nukala, and TA Moises Lopez!","title":"Team 2"},{"location":"win23team2/#team-2-ecemae-148-final-report","text":"","title":"Team 2 ECE/MAE 148 Final Report"},{"location":"win23team2/#wave-the-team-2-fast-2-furious","text":"(Left to Right) - Elias Fang (CSE) - Ainesh Arumugam (ECE) - Matthew Merioles (ECE) - Junhao \"Michael\" Chen (MAE)","title":":wave: The Team: 2 Fast 2 Furious"},{"location":"win23team2/#project-overview","text":"Mario Kart in real-life? That's basically what we did. We designed a boost system similar to those in Mario Kart, where detecting colored pieces of paper on the track can change throttle for a short period of time. Depending on the color of the \"pad\" the car drives over, it will either speed up, slow down, or stop for a few seconds, just like in the game!","title":"\ud83d\udcdd Project Overview"},{"location":"win23team2/#our-robot","text":"","title":"\ud83c\udfce Our Robot"},{"location":"win23team2/#birds-eye","text":"","title":"Bird's Eye"},{"location":"win23team2/#front","text":"","title":"Front"},{"location":"win23team2/#left","text":"","title":"Left"},{"location":"win23team2/#right","text":"","title":"Right"},{"location":"win23team2/#back","text":"","title":"Back"},{"location":"win23team2/#schematic","text":"","title":"Schematic"},{"location":"win23team2/#final-project","text":"","title":"\ud83c\udf44 Final Project"},{"location":"win23team2/#what-we-promised","text":"","title":"What We Promised"},{"location":"win23team2/#must-haves","text":"[X] Distinguishing different colors through the camera [X] Adjust the throttle based on the color","title":"Must haves"},{"location":"win23team2/#nice-to-haves","text":"[X] Have the car detect a flat piece of paper on the track (like a booster pad) [ ] Combine with lane-following algorithm","title":"Nice to haves"},{"location":"win23team2/#gantt-chart","text":"https://sharing.clickup.com/9010060626/g/h/8cgn7aj-87/769d44f22562beb","title":"Gantt Chart"},{"location":"win23team2/#what-we-accomplished","text":"","title":"What We Accomplished"},{"location":"win23team2/#color-detection","text":"Used OpenCV for color detection and edge tracing Used color mask algorithm to detect proportion of frame that color takes up Detected multiple colors at the same time Determined HSVs for orange, pink, and blue Demo","title":"Color Detection"},{"location":"win23team2/#pyvesc","text":"Connection through external webcam Different RPM values are sent through PyVesc to achieve different speed for different colors marked by different states: Blue (Boost) = speed up for 3 sec Pink (Slow)= slow down for 3 sec Orange (Stop) = stop for 3 sec Neutral (Normal) = constant rpm Blue Demo Pink Demo Orange Demo","title":"PyVESC"},{"location":"win23team2/#presentation","text":"https://docs.google.com/presentation/d/1oJPRLYIKvHUXEIK9hoYpPFoFAyHuG6sE7ZrU9NQPG8g/edit?usp=sharing","title":"Presentation"},{"location":"win23team2/#code","text":"https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart.py","title":"Code"},{"location":"win23team2/#possible-future-work","text":"Change the colored paper into Mario Kart items (mushroom, bananas, etc.) for the car to identify Allow the car to run autonomously on a track and still apply speed changes Race with other teams \ud83d\ude09","title":"Possible Future Work"},{"location":"win23team2/#autonomous-laps","text":"DonkeyCar ROS2 Line Following ROS2 Lanes GNSS","title":"\ud83c\udfc1 Autonomous Laps"},{"location":"win23team2/#additional-work-done","text":"Our team was tasked with implementing a depth feature with our contour function, using depthAI to integrate with the OAK-D Camera! Our updated code is now able to measure how far the contoured object is, measured in cm! [Depth Demo] https://www.youtube.com/watch?v=NYIz7--TpgY [Updated Code] https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart_depth.py","title":"Additional Work Done"},{"location":"win23team2/#acknowledgements","text":"Thanks for Professor Jack Silberman, TA Kishore Nukala, and TA Moises Lopez!","title":"Acknowledgements"},{"location":"win23team4/","text":"Team 4 Final Project Report Members: Vasanth Senthil (ECE), Eddy Rodas Lima(MAE), Lingpeng Meng(ECE) Physical Setup Initial Goals Objective Make our RoboCar follow sound using sound localization from multiple microphones. Must Haves Car is able to determine approximate direction of an audio source Car moves in towards audio source once direction of audio is determined Nice to Haves Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume Accurate movement towards source Accomplishments Researched and experimented with different methods of sound localization, such as using time of arrival delay between microphones Used the pyaudio library to collect streams of microphone data from each of the three microphone, and processed this data in order to tell direction real-time based on sound level Used the pyvesc library to create a custom movement file that allows us to easily move the RoboCar based on provided angles and throttle until specified conditions are met The microphone processing file called upon the movement functions after determining current direction. We also spread the left and right microphones to minimize the noise created by the car\u2019s movement, which would influence the accuracy of direction calculations. Demo Videos Static Source Left Static Source Right Moving Source Front Moving Source Back Moving Source Further Away Issues Faced Issue Solution Original idea of using time delay between microphones was not possible due to jetson limitations Decided to use volume of sound signals instead of microphone timing in order to determine direction of audio. Sound from car\u2019s movement, such as servo motors and wheels, were causing our robot to turn randomly. Created wings out of cardboard which lifted and spread out the left/right microphones away from the wheels. Random/sudden loud noises from the environment made the car turn in wrong directions, even when originally was moving correctly towards audio source. Averaged the audio signals of each microphone over the last few ticks using a buffer in order to reduce impact of outlier noises. Required tuning, as a small buffer would not remove outliers as effectively while a large buffer reduced reaction time of robot\u2019s movement when direction of audio actually changed What did not work Original plan was to use the time delay between when microphones head a noise over a sound threshold in order to calculate angle of sound. Problem with that is the Jetson Nano is running Linux, and due to background processes and overhead, it is not precise enough to identify the timing needed for this method to work when the microphones are this close together. This may have been possible with a real-time system such as an Arduino Another method was using real-time localized sound by performing beamforming using microphones in array, but we were not certain if we had enough time to implement and debug this method in time (received the microphones on Saturday) as it was significantly more complex than the other options. Next Steps (If we had more time) Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume, would help filter interference with background noises. Function based on sound difference between microphones to get precise steering angles. Current only steers forward, full left, full right. More accurate movement with our backwards direction Minimize unwanted noise coming from either surroundings or the vehicle.","title":"Team 4"},{"location":"win23team4/#team-4-final-project-report","text":"","title":"Team 4 Final Project Report"},{"location":"win23team4/#members-vasanth-senthil-ece-eddy-rodas-limamae-lingpeng-mengece","text":"","title":"Members: Vasanth Senthil (ECE), Eddy Rodas Lima(MAE), Lingpeng Meng(ECE)"},{"location":"win23team4/#physical-setup","text":"","title":"Physical Setup"},{"location":"win23team4/#initial-goals","text":"","title":"Initial Goals"},{"location":"win23team4/#objective","text":"Make our RoboCar follow sound using sound localization from multiple microphones.","title":"Objective"},{"location":"win23team4/#must-haves","text":"Car is able to determine approximate direction of an audio source Car moves in towards audio source once direction of audio is determined","title":"Must Haves"},{"location":"win23team4/#nice-to-haves","text":"Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume Accurate movement towards source","title":"Nice to Haves"},{"location":"win23team4/#accomplishments","text":"Researched and experimented with different methods of sound localization, such as using time of arrival delay between microphones Used the pyaudio library to collect streams of microphone data from each of the three microphone, and processed this data in order to tell direction real-time based on sound level Used the pyvesc library to create a custom movement file that allows us to easily move the RoboCar based on provided angles and throttle until specified conditions are met The microphone processing file called upon the movement functions after determining current direction. We also spread the left and right microphones to minimize the noise created by the car\u2019s movement, which would influence the accuracy of direction calculations.","title":"Accomplishments"},{"location":"win23team4/#demo-videos","text":"Static Source Left Static Source Right Moving Source Front Moving Source Back Moving Source Further Away","title":"Demo Videos"},{"location":"win23team4/#issues-faced","text":"Issue Solution Original idea of using time delay between microphones was not possible due to jetson limitations Decided to use volume of sound signals instead of microphone timing in order to determine direction of audio. Sound from car\u2019s movement, such as servo motors and wheels, were causing our robot to turn randomly. Created wings out of cardboard which lifted and spread out the left/right microphones away from the wheels. Random/sudden loud noises from the environment made the car turn in wrong directions, even when originally was moving correctly towards audio source. Averaged the audio signals of each microphone over the last few ticks using a buffer in order to reduce impact of outlier noises. Required tuning, as a small buffer would not remove outliers as effectively while a large buffer reduced reaction time of robot\u2019s movement when direction of audio actually changed","title":"Issues Faced"},{"location":"win23team4/#what-did-not-work","text":"Original plan was to use the time delay between when microphones head a noise over a sound threshold in order to calculate angle of sound. Problem with that is the Jetson Nano is running Linux, and due to background processes and overhead, it is not precise enough to identify the timing needed for this method to work when the microphones are this close together. This may have been possible with a real-time system such as an Arduino Another method was using real-time localized sound by performing beamforming using microphones in array, but we were not certain if we had enough time to implement and debug this method in time (received the microphones on Saturday) as it was significantly more complex than the other options.","title":"What did not work"},{"location":"win23team4/#next-steps-if-we-had-more-time","text":"Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume, would help filter interference with background noises. Function based on sound difference between microphones to get precise steering angles. Current only steers forward, full left, full right. More accurate movement with our backwards direction Minimize unwanted noise coming from either surroundings or the vehicle.","title":"Next Steps (If we had more time)"},{"location":"win23team5/","text":"Final Project Repository for Team 5 of the 2023 Winter Class MAE ECE 148 at UCSD Our Final Project has one main objective, which is inspired by a pet dog that plays fetch. Our goal is to design a robot that can identify a green ball, like a tennis ball, locate it, move towards it, pick it up, and return back to its initial location. We achieved this using an OpenCV-based vision system to recognize the ball and Pyvesc to control the car's movements. We also designed a claw mechanism to pick up the ball when it's within range and a servo to move the ball into the claw. In summary, our project demonstrates the capabilities of an autonomous robot that can navigate an environment, recognize objects, and perform tasks like fetching. With further improvements, this type of robot could have many potential applications in various industries.","title":"Team 5"},{"location":"win23team5/#final-project-repository-for-team-5-of-the-2023-winter-class-mae-ece-148-at-ucsd","text":"Our Final Project has one main objective, which is inspired by a pet dog that plays fetch. Our goal is to design a robot that can identify a green ball, like a tennis ball, locate it, move towards it, pick it up, and return back to its initial location. We achieved this using an OpenCV-based vision system to recognize the ball and Pyvesc to control the car's movements. We also designed a claw mechanism to pick up the ball when it's within range and a servo to move the ball into the claw. In summary, our project demonstrates the capabilities of an autonomous robot that can navigate an environment, recognize objects, and perform tasks like fetching. With further improvements, this type of robot could have many potential applications in various industries.","title":"Final Project Repository for Team 5 of the 2023 Winter Class MAE ECE 148 at UCSD"},{"location":"win23team6/","text":"MAE/ECE 148 Winter 2023 at UCSD TEAM 6 Our project uses the OAK-D camera, a roboflow YOLO model, PyVESC module, and an Arduino-powered camera mount to get our car to scan its surroundings until it finds a basketball and drive until it is within about 0.5 m of the ball. Car Assembly Vehicle Body Camera Mount Tech Stack RoboflowOak Module We used roboflow to train a ball detection model and host the model. We, then, made API calls to hosted model to retrieve predictions on frame captures from the OAK-D camera. Once a ball is found, we wrote a script to calculate the angle between the center of the bounding box drawn around the detected ball and the centerline of the camera (which by default is the center of the frame). PyVESC Module We used the pyvesc module to set our servo's angle once a detection has been made. The steering angle is proportional to the calculated angle. Once the steering is set, we increase throttle for about half a second and then stop the motor to make another detection. We then loop over these steps until the ball is within 0.5 m of the frame. Our stopping condition was for the width of the bounding box of the ball to be a certain ratio of the total frame width. The ratio is hardcoded based on fine-tuning to get the car to stop at about 0.5 m. Arduino Board We used an arduino nano to control the camera mount. The camera mount can rotate horizontally (yaw-equivalent) within a range of 180 degrees. And it can move up and down (pitch-equivalent) within a range of 90 degrees. This is used to move the camera around so it can scan the surroundings for a ball (in case the ball is not in frame). Motorized Camera Mount We designed a camera mount that is actuated by 2 servos, one controls the camera's pitch angle and the other controls the camera's yaw angle. The mount elevates the camera 5 inches amove the vehicle's mounting plate. It allows the camera to turn and scan for the target ball. https://user-images.githubusercontent.com/58583277/227646168-1071f237-de95-4f92-ad65-a90ee5fe2b01.mp4 How To Run The Code One can run python run.py from inside the Jetson Nano mounted on their car. This will load the model and also detect the motor and begin the purported task of finding a basketball. If no basketball is found, it will remain stationary. The 148remote/ folder contains a cool arduino program for proof of concept. It moves the camera mount through its full range of motion in a rhythmic fashion. Vehicle In Action Click Here For Video Future Improvements We can use the Jetson to control the servo motors through the Arduino. We can account for the yaw angle of the camera mount and add that to the steering, so that the car can steer toward targets that is not in the field of view of the camera when the camera is pointing straight forward. We can also improve the precision of the ball-recognition model by using more pictures of the ball to train the model. Team 6: Wall-E (used to be Cyclops) Saathvik Dirisala (Data Science) Victor Chen (Computer Engineering) Yang Song (Mechanical Engineering)","title":"Team 6"},{"location":"win23team6/#maeece-148-winter-2023-at-ucsd","text":"","title":"MAE/ECE 148 Winter 2023 at UCSD"},{"location":"win23team6/#team-6","text":"Our project uses the OAK-D camera, a roboflow YOLO model, PyVESC module, and an Arduino-powered camera mount to get our car to scan its surroundings until it finds a basketball and drive until it is within about 0.5 m of the ball.","title":"TEAM 6"},{"location":"win23team6/#car-assembly","text":"","title":"Car Assembly"},{"location":"win23team6/#vehicle-body","text":"","title":"Vehicle Body"},{"location":"win23team6/#camera-mount","text":"","title":"Camera Mount"},{"location":"win23team6/#tech-stack","text":"","title":"Tech Stack"},{"location":"win23team6/#roboflowoak-module","text":"We used roboflow to train a ball detection model and host the model. We, then, made API calls to hosted model to retrieve predictions on frame captures from the OAK-D camera. Once a ball is found, we wrote a script to calculate the angle between the center of the bounding box drawn around the detected ball and the centerline of the camera (which by default is the center of the frame).","title":"RoboflowOak Module"},{"location":"win23team6/#pyvesc-module","text":"We used the pyvesc module to set our servo's angle once a detection has been made. The steering angle is proportional to the calculated angle. Once the steering is set, we increase throttle for about half a second and then stop the motor to make another detection. We then loop over these steps until the ball is within 0.5 m of the frame. Our stopping condition was for the width of the bounding box of the ball to be a certain ratio of the total frame width. The ratio is hardcoded based on fine-tuning to get the car to stop at about 0.5 m.","title":"PyVESC Module"},{"location":"win23team6/#arduino-board","text":"We used an arduino nano to control the camera mount. The camera mount can rotate horizontally (yaw-equivalent) within a range of 180 degrees. And it can move up and down (pitch-equivalent) within a range of 90 degrees. This is used to move the camera around so it can scan the surroundings for a ball (in case the ball is not in frame).","title":"Arduino Board"},{"location":"win23team6/#motorized-camera-mount","text":"We designed a camera mount that is actuated by 2 servos, one controls the camera's pitch angle and the other controls the camera's yaw angle. The mount elevates the camera 5 inches amove the vehicle's mounting plate. It allows the camera to turn and scan for the target ball. https://user-images.githubusercontent.com/58583277/227646168-1071f237-de95-4f92-ad65-a90ee5fe2b01.mp4","title":"Motorized Camera Mount"},{"location":"win23team6/#how-to-run-the-code","text":"One can run python run.py from inside the Jetson Nano mounted on their car. This will load the model and also detect the motor and begin the purported task of finding a basketball. If no basketball is found, it will remain stationary. The 148remote/ folder contains a cool arduino program for proof of concept. It moves the camera mount through its full range of motion in a rhythmic fashion.","title":"How To Run The Code"},{"location":"win23team6/#vehicle-in-action","text":"Click Here For Video","title":"Vehicle In Action"},{"location":"win23team6/#future-improvements","text":"We can use the Jetson to control the servo motors through the Arduino. We can account for the yaw angle of the camera mount and add that to the steering, so that the car can steer toward targets that is not in the field of view of the camera when the camera is pointing straight forward. We can also improve the precision of the ball-recognition model by using more pictures of the ball to train the model.","title":"Future Improvements"},{"location":"win23team6/#team-6-wall-e-used-to-be-cyclops","text":"Saathvik Dirisala (Data Science) Victor Chen (Computer Engineering) Yang Song (Mechanical Engineering)","title":"Team 6: Wall-E (used to be Cyclops)"},{"location":"win23team7/","text":"UCSD ECE/MAE-148 2023 Winter Team 7 Team Members Francisco Downey (BENG), Jonathan Xiong (ECE), Nicholas Preston (MAE), Karthik Srinivasan (MAE) Final Project Overview For our final project, we made our car drive from point A to B, given starting and ending GPS points. Assembled Car Design LIDAR was set in the front of the car. This was an easy decision for the team since we cared for varying obstacles crossing the points that comprise the path from Point A to Point B. Essentially, it was only important to care for obstacles that come into the front of the moving car. GNSS was secure near the center of the car with the antenna placed high and toward the rear. DonkeyCar - 3 Autonomous Laps https://user-images.githubusercontent.com/103704890/227807530-35ed2ea5-2bc0-4b8b-983d-ca6bed659390.mp4 ROS2 Line Following - 3 Autonomous Laps https://user-images.githubusercontent.com/103704890/227801950-dfd60ddd-300e-4af8-9878-c62338184269.mp4 ROS2 Left Lane - 3 Autonomous Laps https://user-images.githubusercontent.com/103704890/227804039-c1aafb7a-8bf6-4f86-89cf-1fa21c7ea5d2.mp4 GPS - 3 Autonomous Laps https://user-images.githubusercontent.com/103704890/227804055-d9c3ccce-ef8d-427d-a6b6-e1db12eba440.mp4 Important to note for the final project, many of the configurations that were found to work in the GPS 3 autonomous laps were used for the final project. The GPS points making up the path were 0.55 meters apart. The nature of the csv points used in the final project matched the one used for this assignment. This allowed the project to move faster as the configurations did work out. See section Algorithmic Design of Directing Car from GPS point A to B. Final Project Plan: Go from point A to B with object detection. Overview Originally, we were going to have LIDAR detect objects in front of the car so the car can see what it needs to go around. However, we did not have enough time to see how to use LIDAR, since there was no assignment with it and not enough time at end of quarter given rain. We, however, got quite experienced with the GPS functionality of the car. Using the GPS modules, we were able to direct the car using a path of car-readble GPS coordinates. Algorithmic Design of Directing Car from GPS point A to B By the end of the quarter, we wrote a program, AtoB.py, which requires two sets of GPS coordinates as inputs. This program generates a .csv file containing a path of GPS coordinates about .55 meters apart from point A to B. These generated points latitude longitude formatted and relative to the base station antenna. The inputted absolute (planet's) GPS coordinates, thus, needed to be translated to these relative coordinates and converted to rectangular from polar positions. It took some testing to increase the precision of the translation as we did not know the exact conversion constants. Demonstration https://user-images.githubusercontent.com/103704890/227807009-ec59e649-c068-4543-9dba-9ddb16ea8ee5.mp4","title":"Team 7"},{"location":"win23team7/#ucsd-ecemae-148-2023-winter-team-7","text":"","title":"UCSD ECE/MAE-148 2023 Winter Team 7"},{"location":"win23team7/#team-members","text":"Francisco Downey (BENG), Jonathan Xiong (ECE), Nicholas Preston (MAE), Karthik Srinivasan (MAE)","title":"Team Members"},{"location":"win23team7/#final-project-overview","text":"For our final project, we made our car drive from point A to B, given starting and ending GPS points.","title":"Final Project Overview"},{"location":"win23team7/#assembled-car-design","text":"LIDAR was set in the front of the car. This was an easy decision for the team since we cared for varying obstacles crossing the points that comprise the path from Point A to Point B. Essentially, it was only important to care for obstacles that come into the front of the moving car. GNSS was secure near the center of the car with the antenna placed high and toward the rear.","title":"Assembled Car Design"},{"location":"win23team7/#donkeycar-3-autonomous-laps","text":"https://user-images.githubusercontent.com/103704890/227807530-35ed2ea5-2bc0-4b8b-983d-ca6bed659390.mp4","title":"DonkeyCar - 3 Autonomous Laps"},{"location":"win23team7/#ros2-line-following-3-autonomous-laps","text":"https://user-images.githubusercontent.com/103704890/227801950-dfd60ddd-300e-4af8-9878-c62338184269.mp4","title":"ROS2 Line Following - 3 Autonomous Laps"},{"location":"win23team7/#ros2-left-lane-3-autonomous-laps","text":"https://user-images.githubusercontent.com/103704890/227804039-c1aafb7a-8bf6-4f86-89cf-1fa21c7ea5d2.mp4","title":"ROS2 Left Lane - 3 Autonomous Laps"},{"location":"win23team7/#gps-3-autonomous-laps","text":"https://user-images.githubusercontent.com/103704890/227804055-d9c3ccce-ef8d-427d-a6b6-e1db12eba440.mp4 Important to note for the final project, many of the configurations that were found to work in the GPS 3 autonomous laps were used for the final project. The GPS points making up the path were 0.55 meters apart. The nature of the csv points used in the final project matched the one used for this assignment. This allowed the project to move faster as the configurations did work out. See section Algorithmic Design of Directing Car from GPS point A to B.","title":"GPS - 3 Autonomous Laps"},{"location":"win23team7/#final-project","text":"Plan: Go from point A to B with object detection. Overview Originally, we were going to have LIDAR detect objects in front of the car so the car can see what it needs to go around. However, we did not have enough time to see how to use LIDAR, since there was no assignment with it and not enough time at end of quarter given rain. We, however, got quite experienced with the GPS functionality of the car. Using the GPS modules, we were able to direct the car using a path of car-readble GPS coordinates. Algorithmic Design of Directing Car from GPS point A to B By the end of the quarter, we wrote a program, AtoB.py, which requires two sets of GPS coordinates as inputs. This program generates a .csv file containing a path of GPS coordinates about .55 meters apart from point A to B. These generated points latitude longitude formatted and relative to the base station antenna. The inputted absolute (planet's) GPS coordinates, thus, needed to be translated to these relative coordinates and converted to rectangular from polar positions. It took some testing to increase the precision of the translation as we did not know the exact conversion constants. Demonstration https://user-images.githubusercontent.com/103704890/227807009-ec59e649-c068-4543-9dba-9ddb16ea8ee5.mp4","title":"Final Project"},{"location":"win23team14/","text":"ECE - MAE 148 Team 14 Winter 2023 Repository Project Goal: We used the OAK-D camera to run object detection (within the camera) and track different traffic details (common signs, speed limits, etc.) Software and Hardware Description: The OAK-D and depthAI are AI-enabled stereo cameras that allow for depth perception and 3D mapping. They are powerful tools for computer vision applications, such as object detection and tracking. The depthAI is a board that enables faster processing of images by offloading the computational workload from the main processor to dedicated hardware. YOLO (You Only Look Once) is a real-time object detection system that is capable of detecting objects in an image or video feed. It is a popular deep learning model that is used in many computer vision applications, including self-driving cars and robotics. PyVesc is a Python library for controlling VESC-based motor controllers. The VESC is an open-source ESC (Electronic Speed Controller) that is widely used in DIY robotics projects. PyVesc allows for easy communication with the VESC and provides an interface for setting motor parameters and reading sensor data. Project Overview: We first used the OAK-D and depthAI to detect stop signs in the robot's field of view. Then, we executed the deep learning model YOLO to process the camera feed and identify the stop sign(text detection can be another method to achieve the same function). Once the stop sign is detected, we implemented PyVesc to send a command to the motor controller to stop the robot and started to set up the OAK-D and depthAI cameras by installing the necessary software libraries. YOLO is capable of detecting multiple objects simultaneously, so we needed to filter out the stop sign from other detected objects. However, we needed a blob converter to take different data types and convert them into a Binary Large Object (BLOB) that could fit in our code. Finally, once the stop sign is detected, we accessed PyVesc to send a command to the motor controller to stop the robot. In summary, the integration of OAK-D, depthAI, YOLO, and PyVesc allows for efficient and accurate stop sign detection and safe stopping of the robot. This implementation can be further customized and optimized for specific robotic platforms and use cases. Final Projet Presentation: https://docs.google.com/presentation/d/1BTMwfktHvDzfzYd6oSnHeaQTEBbtYg8wEMnqoWkamHE/edit?usp=sharing Final Project Video: https://drive.google.com/file/d/1OnO5qWczQbH_aVrgKAtejwLMw6-fFSRW/view?usp=share_link Team Members: Anish Kulkarni, Manuel Abitia, Zizhe Zhang. Special Thanks to: Professor Silberman, Kishore, Moises, and Freddy C. the Robot.","title":"Team 14"}]}