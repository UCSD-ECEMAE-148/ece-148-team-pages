{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ECE/MAE 148: Introduction to Autonomous Vehicles Documentation is under development.","title":"Welcome to ECE/MAE 148: Introduction to Autonomous Vehicles"},{"location":"#welcome-to-ecemae-148-introduction-to-autonomous-vehicles","text":"Documentation is under development.","title":"Welcome to ECE/MAE 148: Introduction to Autonomous Vehicles"},{"location":"win23team4/","text":"Team 4 Final Project Report Members: Vasanth Senthil (ECE), Eddy Rodas Lima(MAE), Lingpeng Meng(ECE) Physical Setup Initial Goals Objective Make our RoboCar follow sound using sound localization from multiple microphones. Must Haves Car is able to determine approximate direction of an audio source Car moves in towards audio source once direction of audio is determined Nice to Haves Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume Accurate movement towards source Accomplishments Researched and experimented with different methods of sound localization, such as using time of arrival delay between microphones Used the pyaudio library to collect streams of microphone data from each of the three microphone, and processed this data in order to tell direction real-time based on sound level Used the pyvesc library to create a custom movement file that allows us to easily move the RoboCar based on provided angles and throttle until specified conditions are met The microphone processing file called upon the movement functions after determining current direction. We also spread the left and right microphones to minimize the noise created by the car\u2019s movement, which would influence the accuracy of direction calculations. Demo Videos Static Source Left Static Source Right Moving Source Front Moving Source Back Moving Source Further Away Issues Faced Issue Solution Original idea of using time delay between microphones was not possible due to jetson limitations Decided to use volume of sound signals instead of microphone timing in order to determine direction of audio. Sound from car\u2019s movement, such as servo motors and wheels, were causing our robot to turn randomly. Created wings out of cardboard which lifted and spread out the left/right microphones away from the wheels. Random/sudden loud noises from the environment made the car turn in wrong directions, even when originally was moving correctly towards audio source. Averaged the audio signals of each microphone over the last few ticks using a buffer in order to reduce impact of outlier noises. Required tuning, as a small buffer would not remove outliers as effectively while a large buffer reduced reaction time of robot\u2019s movement when direction of audio actually changed What did not work Original plan was to use the time delay between when microphones head a noise over a sound threshold in order to calculate angle of sound. Problem with that is the Jetson Nano is running Linux, and due to background processes and overhead, it is not precise enough to identify the timing needed for this method to work when the microphones are this close together. This may have been possible with a real-time system such as an Arduino Another method was using real-time localized sound by performing beamforming using microphones in array, but we were not certain if we had enough time to implement and debug this method in time (received the microphones on Saturday) as it was significantly more complex than the other options. Next Steps (If we had more time) Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume, would help filter interference with background noises. Function based on sound difference between microphones to get precise steering angles. Current only steers forward, full left, full right. More accurate movement with our backwards direction Minimize unwanted noise coming from either surroundings or the vehicle.","title":"Team 4"},{"location":"win23team4/#team-4-final-project-report","text":"","title":"Team 4 Final Project Report"},{"location":"win23team4/#members-vasanth-senthil-ece-eddy-rodas-limamae-lingpeng-mengece","text":"","title":"Members: Vasanth Senthil (ECE), Eddy Rodas Lima(MAE), Lingpeng Meng(ECE)"},{"location":"win23team4/#physical-setup","text":"","title":"Physical Setup"},{"location":"win23team4/#initial-goals","text":"","title":"Initial Goals"},{"location":"win23team4/#objective","text":"Make our RoboCar follow sound using sound localization from multiple microphones.","title":"Objective"},{"location":"win23team4/#must-haves","text":"Car is able to determine approximate direction of an audio source Car moves in towards audio source once direction of audio is determined","title":"Must Haves"},{"location":"win23team4/#nice-to-haves","text":"Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume Accurate movement towards source","title":"Nice to Haves"},{"location":"win23team4/#accomplishments","text":"Researched and experimented with different methods of sound localization, such as using time of arrival delay between microphones Used the pyaudio library to collect streams of microphone data from each of the three microphone, and processed this data in order to tell direction real-time based on sound level Used the pyvesc library to create a custom movement file that allows us to easily move the RoboCar based on provided angles and throttle until specified conditions are met The microphone processing file called upon the movement functions after determining current direction. We also spread the left and right microphones to minimize the noise created by the car\u2019s movement, which would influence the accuracy of direction calculations.","title":"Accomplishments"},{"location":"win23team4/#demo-videos","text":"Static Source Left Static Source Right Moving Source Front Moving Source Back Moving Source Further Away","title":"Demo Videos"},{"location":"win23team4/#issues-faced","text":"Issue Solution Original idea of using time delay between microphones was not possible due to jetson limitations Decided to use volume of sound signals instead of microphone timing in order to determine direction of audio. Sound from car\u2019s movement, such as servo motors and wheels, were causing our robot to turn randomly. Created wings out of cardboard which lifted and spread out the left/right microphones away from the wheels. Random/sudden loud noises from the environment made the car turn in wrong directions, even when originally was moving correctly towards audio source. Averaged the audio signals of each microphone over the last few ticks using a buffer in order to reduce impact of outlier noises. Required tuning, as a small buffer would not remove outliers as effectively while a large buffer reduced reaction time of robot\u2019s movement when direction of audio actually changed","title":"Issues Faced"},{"location":"win23team4/#what-did-not-work","text":"Original plan was to use the time delay between when microphones head a noise over a sound threshold in order to calculate angle of sound. Problem with that is the Jetson Nano is running Linux, and due to background processes and overhead, it is not precise enough to identify the timing needed for this method to work when the microphones are this close together. This may have been possible with a real-time system such as an Arduino Another method was using real-time localized sound by performing beamforming using microphones in array, but we were not certain if we had enough time to implement and debug this method in time (received the microphones on Saturday) as it was significantly more complex than the other options.","title":"What did not work"},{"location":"win23team4/#next-steps-if-we-had-more-time","text":"Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume, would help filter interference with background noises. Function based on sound difference between microphones to get precise steering angles. Current only steers forward, full left, full right. More accurate movement with our backwards direction Minimize unwanted noise coming from either surroundings or the vehicle.","title":"Next Steps (If we had more time)"},{"location":"win23team5/","text":"Final Project Repository for Team 5 of the 2023 Winter Class MAE ECE 148 at UCSD Our Final Project has one main objective, which is inspired by a pet dog that plays fetch. Our goal is to design a robot that can identify a green ball, like a tennis ball, locate it, move towards it, pick it up, and return back to its initial location. We achieved this using an OpenCV-based vision system to recognize the ball and Pyvesc to control the car's movements. We also designed a claw mechanism to pick up the ball when it's within range and a servo to move the ball into the claw. In summary, our project demonstrates the capabilities of an autonomous robot that can navigate an environment, recognize objects, and perform tasks like fetching. With further improvements, this type of robot could have many potential applications in various industries.","title":"Team 5"},{"location":"win23team5/#final-project-repository-for-team-5-of-the-2023-winter-class-mae-ece-148-at-ucsd","text":"Our Final Project has one main objective, which is inspired by a pet dog that plays fetch. Our goal is to design a robot that can identify a green ball, like a tennis ball, locate it, move towards it, pick it up, and return back to its initial location. We achieved this using an OpenCV-based vision system to recognize the ball and Pyvesc to control the car's movements. We also designed a claw mechanism to pick up the ball when it's within range and a servo to move the ball into the claw. In summary, our project demonstrates the capabilities of an autonomous robot that can navigate an environment, recognize objects, and perform tasks like fetching. With further improvements, this type of robot could have many potential applications in various industries.","title":"Final Project Repository for Team 5 of the 2023 Winter Class MAE ECE 148 at UCSD"},{"location":"win23team7/","text":"UCSD ECE/MAE-148 2023 Winter Team 7 Team Members Francisco Downey (BENG), Jonathan Xiong (ECE), Nicholas Preston (MAE), Karthik Srinivasan (MAE) Final Project Overview For our final project, we made our car drive from point A to B, given starting and ending GPS points. Assembled Car Design LIDAR was set in the front of the car. This was an easy decision for the team since we cared for varying obstacles crossing the points that comprise the path from Point A to Point B. Essentially, it was only important to care for obstacles that come into the front of the moving car. GNSS was secure near the center of the car with the antenna placed high and toward the rear. DonkeyCar - 3 Autonomous Laps https://user-images.githubusercontent.com/103704890/227807530-35ed2ea5-2bc0-4b8b-983d-ca6bed659390.mp4 ROS2 Line Following - 3 Autonomous Laps https://user-images.githubusercontent.com/103704890/227801950-dfd60ddd-300e-4af8-9878-c62338184269.mp4 ROS2 Left Lane - 3 Autonomous Laps https://user-images.githubusercontent.com/103704890/227804039-c1aafb7a-8bf6-4f86-89cf-1fa21c7ea5d2.mp4 GPS - 3 Autonomous Laps https://user-images.githubusercontent.com/103704890/227804055-d9c3ccce-ef8d-427d-a6b6-e1db12eba440.mp4 Important to note for the final project, many of the configurations that were found to work in the GPS 3 autonomous laps were used for the final project. The GPS points making up the path were 0.55 meters apart. The nature of the csv points used in the final project matched the one used for this assignment. This allowed the project to move faster as the configurations did work out. See section Algorithmic Design of Directing Car from GPS point A to B. Final Project Plan: Go from point A to B with object detection. Overview Originally, we were going to have LIDAR detect objects in front of the car so the car can see what it needs to go around. However, we did not have enough time to see how to use LIDAR, since there was no assignment with it and not enough time at end of quarter given rain. We, however, got quite experienced with the GPS functionality of the car. Using the GPS modules, we were able to direct the car using a path of car-readble GPS coordinates. Algorithmic Design of Directing Car from GPS point A to B By the end of the quarter, we wrote a program, AtoB.py, which requires two sets of GPS coordinates as inputs. This program generates a .csv file containing a path of GPS coordinates about .55 meters apart from point A to B. These generated points latitude longitude formatted and relative to the base station antenna. The inputted absolute (planet's) GPS coordinates, thus, needed to be translated to these relative coordinates and converted to rectangular from polar positions. It took some testing to increase the precision of the translation as we did not know the exact conversion constants. Demonstration https://user-images.githubusercontent.com/103704890/227807009-ec59e649-c068-4543-9dba-9ddb16ea8ee5.mp4","title":"Team 7"},{"location":"win23team7/#ucsd-ecemae-148-2023-winter-team-7","text":"","title":"UCSD ECE/MAE-148 2023 Winter Team 7"},{"location":"win23team7/#team-members","text":"Francisco Downey (BENG), Jonathan Xiong (ECE), Nicholas Preston (MAE), Karthik Srinivasan (MAE)","title":"Team Members"},{"location":"win23team7/#final-project-overview","text":"For our final project, we made our car drive from point A to B, given starting and ending GPS points.","title":"Final Project Overview"},{"location":"win23team7/#assembled-car-design","text":"LIDAR was set in the front of the car. This was an easy decision for the team since we cared for varying obstacles crossing the points that comprise the path from Point A to Point B. Essentially, it was only important to care for obstacles that come into the front of the moving car. GNSS was secure near the center of the car with the antenna placed high and toward the rear.","title":"Assembled Car Design"},{"location":"win23team7/#donkeycar-3-autonomous-laps","text":"https://user-images.githubusercontent.com/103704890/227807530-35ed2ea5-2bc0-4b8b-983d-ca6bed659390.mp4","title":"DonkeyCar - 3 Autonomous Laps"},{"location":"win23team7/#ros2-line-following-3-autonomous-laps","text":"https://user-images.githubusercontent.com/103704890/227801950-dfd60ddd-300e-4af8-9878-c62338184269.mp4","title":"ROS2 Line Following - 3 Autonomous Laps"},{"location":"win23team7/#ros2-left-lane-3-autonomous-laps","text":"https://user-images.githubusercontent.com/103704890/227804039-c1aafb7a-8bf6-4f86-89cf-1fa21c7ea5d2.mp4","title":"ROS2 Left Lane - 3 Autonomous Laps"},{"location":"win23team7/#gps-3-autonomous-laps","text":"https://user-images.githubusercontent.com/103704890/227804055-d9c3ccce-ef8d-427d-a6b6-e1db12eba440.mp4 Important to note for the final project, many of the configurations that were found to work in the GPS 3 autonomous laps were used for the final project. The GPS points making up the path were 0.55 meters apart. The nature of the csv points used in the final project matched the one used for this assignment. This allowed the project to move faster as the configurations did work out. See section Algorithmic Design of Directing Car from GPS point A to B.","title":"GPS - 3 Autonomous Laps"},{"location":"win23team7/#final-project","text":"Plan: Go from point A to B with object detection. Overview Originally, we were going to have LIDAR detect objects in front of the car so the car can see what it needs to go around. However, we did not have enough time to see how to use LIDAR, since there was no assignment with it and not enough time at end of quarter given rain. We, however, got quite experienced with the GPS functionality of the car. Using the GPS modules, we were able to direct the car using a path of car-readble GPS coordinates. Algorithmic Design of Directing Car from GPS point A to B By the end of the quarter, we wrote a program, AtoB.py, which requires two sets of GPS coordinates as inputs. This program generates a .csv file containing a path of GPS coordinates about .55 meters apart from point A to B. These generated points latitude longitude formatted and relative to the base station antenna. The inputted absolute (planet's) GPS coordinates, thus, needed to be translated to these relative coordinates and converted to rectangular from polar positions. It took some testing to increase the precision of the translation as we did not know the exact conversion constants. Demonstration https://user-images.githubusercontent.com/103704890/227807009-ec59e649-c068-4543-9dba-9ddb16ea8ee5.mp4","title":"Final Project"},{"location":"win23team8/","text":"Team 8 Final Project Proposal Team Members - Youssef Georgy | Electrical & Computer Engineering - Rizzi Galibut | Mechanical & Aerospace Engineering - Shuhang Xu | Computer Science - Kavin Raj | Cognitive Science w/ Emphasis in Machine Learning Such a lovely team!! Project Overall A waiter-bot that takes visual input from the camera, navigates autonomously to different specified locations (i.e., tables) and then back to the starting point. Use image-detection via camera to give robocar a location or GPS coordinates to navigate to, then use GPS data to plot path there and avoid any obstacles in the way. The robocar will be able to take any location given (provided it\u2019s in range of the network connection) and determine how to get there Physical Setup Gantt Chart Demonstration Using DepthAI for text recognition Accomplished: - Used DepthAI to enable the camera to detect numbers which are associated with different tables (e.g., 001, 002, etc.) and different CSV files - Originally started by looking at OpenCV and Tesseract for OCR - These are not SpatialAI platforms so accomplishing what we were trying to do would be much harder - Default code provided in DepthAI library launched windows that displayed video stream and words detected, which worked when directly connected to camera through host computer but running code through Jetson would require a container to launch these windows - Needed to find a way to disable them and have camera run in the background Default code also rewrote the variable that contained the decoded text each time it detected something - Had to rewrite and remove sections of the code to stop detection once certain prompts are given - Light conditions really mattered when showing prompt to camera - Worked better during the day and when shown prompt on a backlit-screen (with white background) What did't work as expected: - Camera could read \u201c001\u201d but not \u201c1\u201d, etc., so we decided to use a set of numbers instead - Number recognition is much more accurate than word recognition so we decided to use numbers as the prompt Using GPS to record different paths GPS navigation code is based on the USCD donkeycar GPS library We are trying to make it stay in auto-pilot mode by default and reset the origin in the beginning Depend on what text is detected with DepthAI, make it move following the pre-recorded path When it moves back to the origin, stop and start detecting next text Desired but not accomplished Include the ability to restart the process once the waiter-bot returned to the starting point without having to manually run the script again Have waiter-bot stay at the table for a certain amount of time before returning to the starting point Incorporate the LiDAR so the waiter-bot can avoid obstacles (e.g., students walking past) while traveling to tables Could also be used to detect when the waiter-bot reaches the table and ensure it doesn\u2019t crash into it Have controller inputs programmed in so the waiter-bot can be truly autonomous. Currently still requires human input","title":"Team 8"},{"location":"win23team8/#team-8-final-project-proposal","text":"","title":"Team 8 Final Project Proposal"},{"location":"win23team8/#team-members","text":"- Youssef Georgy | Electrical & Computer Engineering - Rizzi Galibut | Mechanical & Aerospace Engineering - Shuhang Xu | Computer Science - Kavin Raj | Cognitive Science w/ Emphasis in Machine Learning Such a lovely team!!","title":"Team Members"},{"location":"win23team8/#project-overall","text":"A waiter-bot that takes visual input from the camera, navigates autonomously to different specified locations (i.e., tables) and then back to the starting point. Use image-detection via camera to give robocar a location or GPS coordinates to navigate to, then use GPS data to plot path there and avoid any obstacles in the way. The robocar will be able to take any location given (provided it\u2019s in range of the network connection) and determine how to get there","title":"Project Overall"},{"location":"win23team8/#physical-setup","text":"","title":"Physical Setup"},{"location":"win23team8/#gantt-chart","text":"","title":"Gantt Chart"},{"location":"win23team8/#demonstration","text":"","title":"Demonstration"},{"location":"win23team8/#using-depthai-for-text-recognition","text":"Accomplished: - Used DepthAI to enable the camera to detect numbers which are associated with different tables (e.g., 001, 002, etc.) and different CSV files - Originally started by looking at OpenCV and Tesseract for OCR - These are not SpatialAI platforms so accomplishing what we were trying to do would be much harder - Default code provided in DepthAI library launched windows that displayed video stream and words detected, which worked when directly connected to camera through host computer but running code through Jetson would require a container to launch these windows - Needed to find a way to disable them and have camera run in the background Default code also rewrote the variable that contained the decoded text each time it detected something - Had to rewrite and remove sections of the code to stop detection once certain prompts are given - Light conditions really mattered when showing prompt to camera - Worked better during the day and when shown prompt on a backlit-screen (with white background) What did't work as expected: - Camera could read \u201c001\u201d but not \u201c1\u201d, etc., so we decided to use a set of numbers instead - Number recognition is much more accurate than word recognition so we decided to use numbers as the prompt","title":"Using DepthAI for text recognition"},{"location":"win23team8/#using-gps-to-record-different-paths","text":"GPS navigation code is based on the USCD donkeycar GPS library We are trying to make it stay in auto-pilot mode by default and reset the origin in the beginning Depend on what text is detected with DepthAI, make it move following the pre-recorded path When it moves back to the origin, stop and start detecting next text","title":"Using GPS to record different paths"},{"location":"win23team8/#desired-but-not-accomplished","text":"Include the ability to restart the process once the waiter-bot returned to the starting point without having to manually run the script again Have waiter-bot stay at the table for a certain amount of time before returning to the starting point Incorporate the LiDAR so the waiter-bot can avoid obstacles (e.g., students walking past) while traveling to tables Could also be used to detect when the waiter-bot reaches the table and ensure it doesn\u2019t crash into it Have controller inputs programmed in so the waiter-bot can be truly autonomous. Currently still requires human input","title":"Desired but not accomplished"},{"location":"win23team9/","text":"ECE/MAE 148 Winter 2023 Team 9 JetBuddy Indoor Delivery Bot based on DepthAI, OpenCV and LiDAR Table of Contents Team Members Hardware and Schematics Parts Schematics Final Project Abstract Part 1: Human Detection and Following with Depthai and PyVesc Part 2: Stopping Mechanism with Lidar Part 3: Facial Recognition Part 4: Spatial Detection with DepthAI Reflection Challenges Potential Improvements Presentation Files Reference Team members Ben Zhang (ECE) Joseph Katona (ECE) Yichen Yang (ECE) Zijian Wang (MAE) Hardware Parts Full Assembly Mounting Plate Jetson Case Camera LiDAR Mount Final Project Delivery Box Schematics Wire Diagram Final Project Abstract This project aims to develop a delivery system for our robocar that can detect and follow humans while also incorporating a stopping mechanism to prevent collisions. Additionally, the robot will utilize facial recognition to identify individuals and personalize interactions. Part 1: Human Detection and Following with Depthai and PyVesc The OAKD camera will be used to detect and track humans in the robot's vicinity. The PyVesc motor controllers will then be used to move the robot in the direction of the detected human. Required Components Tiny-Yolov3 model integrated in DepthAi for object detection PyVesc Python package for robocar control Algorithm Workflow Use Tiny-Yolov3 to detect the bounding box of the person in the OAKD camera's field of view. Determine the position of the person by finding the central line of the bounding box, and denote the x-axis value as x0. Calculate the error between the central line of the frame (416x416 pixels), e = x - x0. Calculate the steering value using the formula: v = (Kp * e) / 416 + 0.5, where Kp = 1. Use PyVesc to control steering by calling vesc.set_servo(v). Additional Settings Use vesc.set_rpm() to run the car once it detects people. The steering value is sampled at a rate of 5Hz to prevent frequent drifting. Part 2: Stopping Mechanism with Lidar The Lidar sensor will be used to detect obstacles in the robot's path. If an obstacle is detected, the robot will stop moving and wait for the obstacle to clear before continuing on its path. The LiDAR on this robot aim to Detect anything that is in a close range If the position is too clase, the robot will stop to avoid collision The robot will back up after it stop for a while and still detect obstacle is close Transform raw binary data from LiDAR to numerical data through BinASCII library How to read LiDAR? Each measurement data point of LiDAR is consists of a distance value of 2 bytes and a confidence of 1 byte We transform this data through chopping it to bytes and translate it. We get the angle by getting the start angle and end angle. Putting all the distance into a list and it will stop the car if there\u2019s an object within certain distance that LiDAR detected. Part 3: Facial Recognition The robot will be equipped with a facial recognition system, using a webcam, that will allow it to identify individuals and personalize interactions. Once it recognizes the right person, the delivery box will open. The facial recognition software uses a simple python import of facial_recognition. In the facial_recognition library all we do is use openCV to capture images for the frames and use facial_recognitions \"matching\" function to to add a box around the persons face. In our case when this value is detected over an interval then a true value is then sent to the box to open. Part 4: Spatial Detection with DepthAI Utilizing Depthai's pipeline system we take their spatial location pipeline to simply calculate the distance of individual from the camera. The Object detection pipeline detects a person and creates a bounded box, then with the x and y coordinates from the bounded box we can pinpoint where we want the camera to point. After these coordinates are gathered the z location is stored in a circular list. This is because the bounded box and tracker of object distance aren't always in sync so some erroneous values are given. Once we have around 50 samples then we take the average to get a good idea of what the distance of the person from the car is. Finally we utilize pyvescs set_rpm() features to give out a more smooth acceleration system. So, basically if you're far away the robot will speed up and slow down as it moves closer to you. Get more info on Spatial Depth here Gantt Chart Demonstrations The Video might not show up, please go to img folder for full demo. Your browser does not support HTML video. Reflection Challenges Getting everything to work together Different libraries working together and all send signals to PyVESC Everything worked fine on a local machine but when running on the Jetson, crashes would occur Scope of the original idea Mapping the path for future references using SLAM Depth ai pipeline caused crashes X-Link Problem(Serial bus issues) Translate raw LiDAR output to data we need Making the car look smooth Better algorithm to adjust speed(rpm) Potential Improvements Implement all the features together flawlessly Currently cannot run together good due to delay from different components Get the locking mechanism working Locking mechanism to make sure the right receiver get the package LiDar also scans the path for future path planning Trying to find a person if it cannot detect anything Maybe try different frameworks since we can use different libraries without limitation in ROS or donkeycar Presentations Project Proposal & Progress Report Final Presntation Reference We would like to give special thanks to: Professor Jack Silberman TA Moises Lopez TA Kishore Nukala All The teams that helped us on the way","title":"Team 9"},{"location":"win23team9/#team-members","text":"Ben Zhang (ECE) Joseph Katona (ECE) Yichen Yang (ECE) Zijian Wang (MAE)","title":"Team members"},{"location":"win23team9/#hardware","text":"","title":"Hardware"},{"location":"win23team9/#parts","text":"","title":"Parts"},{"location":"win23team9/#full-assembly","text":"","title":"Full Assembly"},{"location":"win23team9/#mounting-plate","text":"","title":"Mounting Plate"},{"location":"win23team9/#jetson-case","text":"","title":"Jetson Case"},{"location":"win23team9/#camera-lidar-mount","text":"","title":"Camera LiDAR Mount"},{"location":"win23team9/#final-project-delivery-box","text":"","title":"Final Project Delivery Box"},{"location":"win23team9/#schematics","text":"","title":"Schematics"},{"location":"win23team9/#wire-diagram","text":"","title":"Wire Diagram"},{"location":"win23team9/#final-project","text":"","title":"Final Project"},{"location":"win23team9/#abstract","text":"This project aims to develop a delivery system for our robocar that can detect and follow humans while also incorporating a stopping mechanism to prevent collisions. Additionally, the robot will utilize facial recognition to identify individuals and personalize interactions.","title":"Abstract"},{"location":"win23team9/#part-1-human-detection-and-following-with-depthai-and-pyvesc","text":"The OAKD camera will be used to detect and track humans in the robot's vicinity. The PyVesc motor controllers will then be used to move the robot in the direction of the detected human.","title":"Part 1: Human Detection and Following with Depthai and PyVesc"},{"location":"win23team9/#required-components","text":"Tiny-Yolov3 model integrated in DepthAi for object detection PyVesc Python package for robocar control","title":"Required Components"},{"location":"win23team9/#algorithm-workflow","text":"Use Tiny-Yolov3 to detect the bounding box of the person in the OAKD camera's field of view. Determine the position of the person by finding the central line of the bounding box, and denote the x-axis value as x0. Calculate the error between the central line of the frame (416x416 pixels), e = x - x0. Calculate the steering value using the formula: v = (Kp * e) / 416 + 0.5, where Kp = 1. Use PyVesc to control steering by calling vesc.set_servo(v).","title":"Algorithm Workflow"},{"location":"win23team9/#additional-settings","text":"Use vesc.set_rpm() to run the car once it detects people. The steering value is sampled at a rate of 5Hz to prevent frequent drifting.","title":"Additional Settings"},{"location":"win23team9/#part-2-stopping-mechanism-with-lidar","text":"The Lidar sensor will be used to detect obstacles in the robot's path. If an obstacle is detected, the robot will stop moving and wait for the obstacle to clear before continuing on its path.","title":"Part 2: Stopping Mechanism with Lidar"},{"location":"win23team9/#the-lidar-on-this-robot-aim-to","text":"Detect anything that is in a close range If the position is too clase, the robot will stop to avoid collision The robot will back up after it stop for a while and still detect obstacle is close Transform raw binary data from LiDAR to numerical data through BinASCII library","title":"The LiDAR on this robot aim to"},{"location":"win23team9/#how-to-read-lidar","text":"Each measurement data point of LiDAR is consists of a distance value of 2 bytes and a confidence of 1 byte We transform this data through chopping it to bytes and translate it. We get the angle by getting the start angle and end angle. Putting all the distance into a list and it will stop the car if there\u2019s an object within certain distance that LiDAR detected.","title":"How to read LiDAR?"},{"location":"win23team9/#part-3-facial-recognition","text":"The robot will be equipped with a facial recognition system, using a webcam, that will allow it to identify individuals and personalize interactions. Once it recognizes the right person, the delivery box will open. The facial recognition software uses a simple python import of facial_recognition. In the facial_recognition library all we do is use openCV to capture images for the frames and use facial_recognitions \"matching\" function to to add a box around the persons face. In our case when this value is detected over an interval then a true value is then sent to the box to open.","title":"Part 3: Facial Recognition"},{"location":"win23team9/#part-4-spatial-detection-with-depthai","text":"Utilizing Depthai's pipeline system we take their spatial location pipeline to simply calculate the distance of individual from the camera. The Object detection pipeline detects a person and creates a bounded box, then with the x and y coordinates from the bounded box we can pinpoint where we want the camera to point. After these coordinates are gathered the z location is stored in a circular list. This is because the bounded box and tracker of object distance aren't always in sync so some erroneous values are given. Once we have around 50 samples then we take the average to get a good idea of what the distance of the person from the car is. Finally we utilize pyvescs set_rpm() features to give out a more smooth acceleration system. So, basically if you're far away the robot will speed up and slow down as it moves closer to you. Get more info on Spatial Depth here","title":"Part 4: Spatial Detection with DepthAI"},{"location":"win23team9/#gantt-chart","text":"","title":"Gantt Chart"},{"location":"win23team9/#demonstrations","text":"The Video might not show up, please go to img folder for full demo. Your browser does not support HTML video.","title":"Demonstrations"},{"location":"win23team9/#reflection","text":"","title":"Reflection"},{"location":"win23team9/#challenges","text":"Getting everything to work together Different libraries working together and all send signals to PyVESC Everything worked fine on a local machine but when running on the Jetson, crashes would occur Scope of the original idea Mapping the path for future references using SLAM Depth ai pipeline caused crashes X-Link Problem(Serial bus issues) Translate raw LiDAR output to data we need Making the car look smooth Better algorithm to adjust speed(rpm)","title":"Challenges"},{"location":"win23team9/#potential-improvements","text":"Implement all the features together flawlessly Currently cannot run together good due to delay from different components Get the locking mechanism working Locking mechanism to make sure the right receiver get the package LiDar also scans the path for future path planning Trying to find a person if it cannot detect anything Maybe try different frameworks since we can use different libraries without limitation in ROS or donkeycar","title":"Potential Improvements"},{"location":"win23team9/#presentations","text":"Project Proposal & Progress Report Final Presntation","title":"Presentations"},{"location":"win23team9/#reference","text":"We would like to give special thanks to: Professor Jack Silberman TA Moises Lopez TA Kishore Nukala All The teams that helped us on the way","title":"Reference"},{"location":"win23team14/","text":"ECE - MAE 148 Team 14 Winter 2023 Repository Project Goal: We used the OAK-D camera to run object detection (within the camera) and track different traffic details (common signs, speed limits, etc.) Software and Hardware Description: The OAK-D and depthAI are AI-enabled stereo cameras that allow for depth perception and 3D mapping. They are powerful tools for computer vision applications, such as object detection and tracking. The depthAI is a board that enables faster processing of images by offloading the computational workload from the main processor to dedicated hardware. YOLO (You Only Look Once) is a real-time object detection system that is capable of detecting objects in an image or video feed. It is a popular deep learning model that is used in many computer vision applications, including self-driving cars and robotics. PyVesc is a Python library for controlling VESC-based motor controllers. The VESC is an open-source ESC (Electronic Speed Controller) that is widely used in DIY robotics projects. PyVesc allows for easy communication with the VESC and provides an interface for setting motor parameters and reading sensor data. Project Overview: We first used the OAK-D and depthAI to detect stop signs in the robot's field of view. Then, we executed the deep learning model YOLO to process the camera feed and identify the stop sign(text detection can be another method to achieve the same function). Once the stop sign is detected, we implemented PyVesc to send a command to the motor controller to stop the robot and started to set up the OAK-D and depthAI cameras by installing the necessary software libraries. YOLO is capable of detecting multiple objects simultaneously, so we needed to filter out the stop sign from other detected objects. However, we needed a blob converter to take different data types and convert them into a Binary Large Object (BLOB) that could fit in our code. Finally, once the stop sign is detected, we accessed PyVesc to send a command to the motor controller to stop the robot. In summary, the integration of OAK-D, depthAI, YOLO, and PyVesc allows for efficient and accurate stop sign detection and safe stopping of the robot. This implementation can be further customized and optimized for specific robotic platforms and use cases. Final Projet Presentation: https://docs.google.com/presentation/d/1BTMwfktHvDzfzYd6oSnHeaQTEBbtYg8wEMnqoWkamHE/edit?usp=sharing Final Project Video: https://drive.google.com/file/d/1OnO5qWczQbH_aVrgKAtejwLMw6-fFSRW/view?usp=share_link Team Members: Anish Kulkarni, Manuel Abitia, Zizhe Zhang. Special Thanks to: Professor Silberman, Kishore, Moises, and Freddy C. the Robot.","title":"Team 14"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/","text":"UCSD Robocar Using RC Controller Arduino Micro Pro - Leonard Compatible 22Mar23 - V2.0 ExpressLRS 2.4GHz (ELRS) Radio Long Range Digital #Here are the radios we have for UCSD evGKart #There radios use the open source ELRS #There are several advantages on ELRS: Long Range Low Latency, several suppliers, software upgradable #There are few options for radios. We got the radios below because they were compacts, seemed rugged, and received good reviews #These radios will allow us to keep it all digital without the need to use PWM/PPM. ex: Radio UART - > Single Board Computer (SBC). We will use a microcontroller (MCU) to translate near real-time the protocol used on the radios into serial communication with a SBC using USB and also use the MCU for the emergency stop (off) [EMO] directly from the radio command vs. using separate radio for the EMO. Since the radio for PPM/PWM was affordable we have one too in case we want to directly control RC Cars and or use a multiplexer as part of our EMO. This would require two PWM like cables for each radio channel used. BetaFPV LiteRadio 3 Pro Radio Transmitter- ELRS (Supports External Nano TX Module) Links Image https://betafpv.com/collections/tx/products/literadio-3-pro-radio-transmitter https://support.betafpv.com/hc/en-us/articles/5987468200601-Manual-for-Lite-Radio3-Pro https://www.getfpv.com/betafpv-literadio-3-pro-radio-transmitter-elrs-supports-external-nano-tx-module.html MATEKSYS ExpressLRS 2.4GHz Receiver - ELRS R24 D Links Image https://www.getfpv.com/mateksys-expresslrs-2-4ghz-receiver-elrs-r24-d.html MATEKSYS ExpressLRS 2.4GHz Receiver - PWM ELRS-R24-P6 Links Image https://www.getfpv.com/mateksys-expresslrs-2-4ghz-receiver-pwm-elrs-r24-p6.html Upgrading the ELRS firmware 24Mar23 #Get back to revise these to show version 3.2.0 again. I had to use 3.2.0 because it is the version that ELRS configurator has for the receiver. It seems I need to keep the ERLSv2.luna vs3 for the radio to read the luna script. I hope they will fix it in the future. _#There is a good software that helps on on the upgrade https://www.expresslrs.org/quick-start/installing-configurator/ #They support Windows, Mac, and Linux #Ideally you upgrade the radios Tx and Rx on the same session to make sure they have the same firmware version #Also if you have multiple of the same Tx and Rx doing them consecutively can save you time and help reduce the risk of having different firmware versions. #For the BETA FPV 3 PRO https://www.expresslrs.org/quick-start/transmitters/betfpvlr3pro/ #Turn on the TX radio #Plum the USB C cable connected to your computer #The radio will ask what connection to use #Select USB Serial (Debug) #You will need to look for the Radio in the connections options #Version 2.5.2 #Build & Flash #The first time it will take a while to download, install tools and build the firmware #Download and save on your computer the the LUA Script file #Unplug the USB cable, turn off the radio, turn on again #To upload the LUA script to the radio, unplug the USB cable if connected, connect it again #Select USB Storage (SD) #Then you can use your computer to upload the LUA script you saved earlier https://www.expresslrs.org/quick-start/transmitters/lua-howto/ Download the ELRSv3 Lua Script (you can simply right-click, save-as) into your radio's SD Card under the Scripts/Tools #elrsV2.lua #Also, let's delete the older .lua script from the root of the DISK_IMG of the radio #Remove the USB cable from the radio #Hold the right top joystick to the left for 1~2 seconds #Tools #EspressLRS #See if the configuration loads, here is where I have problems with V3.x for some problem. V2.5.2 works just fine #The BetaFPV LiteRadio 3 Pro Radio Transmitter uses the two smaller joystick to navigate the configuration menu Moving the right side change the GUI on the display Moving the right side to the left and holding it for few seconds get you into the settings Receiver MATEKYS R24-D 2.4Ghz https://www.expresslrs.org/quick-start/receivers/matek2400/ If this is the first time you're flashing/updating your receiver or you're updating it from a previous 2.x firmware via WiFi, first ensure that it has version 2.5.2. Once it has the 2.5.2 flashed, update to 3.x. #Connect to the Wifi Network the receiver has created. It should be named something like ExpressLRS RX with the same expresslrs password as the TX Module Hotspot. #After the Rx radio boots, wait to see the LED flashing quick. That is an indicadion that its Access Point and web server is running. #Using a web browser http://10.0.0.1/ #Connect to the website of the device and upload new firmware Then if you connect the receiver to your local WiFi you can get to it by usings its IP address or name. You can scan the network and look for a device called elrs_rx ex: http://elrs_rx.local #Because we will be on the field and not necessarily close to WiFi, let\u2019s leave the AP of the radio on so we can configure it. Not very safe since someone can connect to it while the AP is on. We will check how to protect it with a password on it a bit later. Moreover, I did not see how to name the radios, it would be hard to know what Rx radios is what in the network. #Let's get the Rx to be on 115200 baud rate #Just connect to the webinterface of the Rx and set the baudrate #We need to use a USB to TTL cable - example from Amazon #Or you can use the embedded WiFi, how cool is that? Lets try the UART first Red wire of the USB to TTL = + Black wire of the USB to TTL = - Green = White = #Press and hold the boot button while connecting the USB to TTL device to your computer http://www.mateksys.com/?portfolio=elrs-r24#tab-id-3 For ELRS-R24-D, if update from 2.x to 3.x, Pls select target MATEK 2400 RX R24D and click on Force Flash https://github.com/kkbin505/Simple_RX Readme.md CRSF decode library for arduino atmega32u4/328p. Based on arduino SBUS decode, modified to decode crsf protocol from elrs receiver #Let's keep it simple first just by using Arduinos, then we will try a Raspberry PI Pico, and then UCSD DRTC if we want to make everything CAN https://www.amazon.com/gp/product/B09C5H78BP/ref=ppx_yo_dt_b_asin_title_o01_s02?ie=UTF8&psc=1 #I got this coming too so we use a better USB connector, USB C vs. uUSB. #I just did not see there was a version with USB C earlier. https://www.amazon.com/gp/product/B0BCW67NJP/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1 https://github.com/kkbin505/Simple_RX Some Background and References ELRS Radio (Long Range Digital) TCIII \u2014 03/17/2023 1:11 PM Hi Jack. I spent half of today learning about how to update (flash) the BETAFPV gamepad/plug-in transmitter/receiver software. Both the gamepad or the plug-in transmitter software version has to be the same as the receiver and vice versa. The gamepad configuration and update flashing can be done with the BETAFPV Configurator while even BETAFPV uses the ELRS Configurator to update the plug-in transmitter and receiver. Nothing like consistency. \ud83d\ude04 The BETAFPV LiteRadio 3 gamepad RC transmitter can output 25, 50, or 100 mw while the plug-in RC transmitter can output 100, 250, or 500 mw and comes with ELRS version 2.01. The Matek ELRS six channel PWM receiver comes with ELRS version 3.0 while the BETAFPV Nano serial output receiver comes with ELRS version 1.0.0-RC5 so I am going to have to reprogram both receivers with ELRS version 2.01. These LoRa gamepads and receivers are definitely not for beginners. \ud83d\ude32 TCIII \u2014 Yesterday at 7:06 AM Hi Jack. I think that it would be best to converse about the Expresslrs LoRa gamepad project on DM until we get the circuitry and software validated, otherwise we might wind up with DC Users trying to implement a LoRa gamepad system that has not been full vetted for functionality and have substantial issues. TCIII \u2014 Yesterday at 7:25 AM Hi Jack. I bought a couple of Arduino Pro Micros so that I could use the crsf_decode_hid.ino sketch from https://github.com/kkbin505/Simple_RX The sketch compiled just fine after I installed the additional required libraries and I will connect the serial output of the BetaFPV serial output Nano receiver to the Pro Micro and see what kind of output I get using the serial monitor. The Arduino Joystick Library (https://github.com/MHeironimus/ArduinoJoystic+kLibrary) is used by the crsf_decode_hid.ino sketch, but the included sketches in the library are designed to take individual joystick and button inputs connected to the Pro Micro and and produce a HID joystick that can be used by PCs for games. Could you please take a look at the crsf_decode_hid.ino sketch as I understand the debug portion, but I don't see where he is outputting the decoded crsf gamepad joystick/button HID values on the USB port. If the crsf_decode_hid.ino sketch is really making the Nano Receiver crsf output look like a HID gamepad then we have it made, though it does require the Pro Micro to interface the Receiver to the SBC as an HID device. \ud83d\ude42 I am using a BetaFPV LiteRadio 3 RC gamepad to do the testing compared to your BetFPV LiteRadio 3 Pro. All of the BetaFPV products are still using Expresslrs version 2.x when Expresslrs version 3.x is now available. The BetaFPV LiteRadio 3 can only be programmed with the BetaFPV Configurator so that gamepad is limited to Expresslrs version 2.x where as your BetFPV LiteRadio 3 Pro can be programmed with the Expresslrs Configurator and can be programmed with Expresslrs 3.x. Remember, both the RC gamepad and the receiver must have the same software version of Expresslrs and pass phrase to bind correctly. GitHub GitHub - kkbin505/Simple_RX: crsf protocol decode for avr crsf protocol decode for avr. Contribute to kkbin505/Simple_RX development by creating an account on GitHub. GitHub - kkbin505/Simple_RX: crsf protocol decode for avr GitHub GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... An Arduino library that adds one or more joysticks to the list of HID devices an Arduino Leonardo or Arduino Micro can support. - GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... This afternoon I will try to determine the voltage of the BetaFPV Nano serial channel receiver output. It will either be 5 vdc or 3.3 vdc. The Nano receiver is powered by 5 vdc, but there is a 3.3 vdc on the Nano receiver pwb because the receiver/processor and WIFI ICs work on 3.3 vdc. So it would seem to me that the serial channel is a 3.3 vdc signal output which will be compatible with either the Rpi or Nano GPIO bus for onboard crsf decoding. Though I like the fact that the Pro Micro, when programmed with the crsf_decode_hid.ino sketch looks just like a HID compliant game controller which makes it portable between RC cars. Additionally the Joystick Descriptor in the crsf_decode_hid.ino sketch allows the customizing of the crsf data stream as to the type of LoRa game controller in use. TCIII \u2014 Yesterday at 9:53 AM Observations concerning BetaFPV Expresslrs products: 1. Only the BetaFPV LiteRadio 3 Pro can be programmed with the Expresslrs Configurator, unlike the LiteRadio 3 and the LiteRadio 2 SE which require the BetaFPV Configurator, which allows access to the latest Expresslrs version 3.x. 2. The LiteRadio 3 and the LiteRadio 2 SE must programmed with the BetaFPV Configurator because they use a custom version of Expresslrs which is stuck at version 2.x.\ud83d\ude41 3. All of the BetaFPV receivers can be programmed with the Expresslrs Configurator, but keep in mind that both the BetaFPV RC gamepads and receivers must have the same version of the software and pass phrase to bind. 4. Therefore if you are using a LiteRadio 3 or a LiteRadio 2 SE, you cannot upgrade the software of any of the BetaFPV receivers to Expresslrs 3.x or you will lose binding capability. 5. The default baud rate of the BetaFPV Nano serial channel receiver UART is 420000 baud so I had to use the Expresslrs Configurator to change the Nano receiver's UART baud rate to 115200 baud to work with the Pro Micro. 6. Unfortunately the Expresslrs Configurator will only show Expresslrs version 2.5 and up for flashing and the Nano receiver for my purposes needed software version 2.x to update the UART baud rate. 7. Fortunately the Expresslrs Configurator allows access to the Expresslrs software version archives so I could use Expresslrs version 2.0 to update the UART baud rate. \ud83d\ude42 Since I bought the BetaFPV LiteRadio 3, which has an external transmitter port, I also purchased the BetaFPV Nano RF TX Module that can be programmed with the Expresslrs Configurator fortunately. The stock BetaFPV LiteRadio 3 has an output of 100 mw while the BetaFPV Nano RF TX Module can be programmed for outputs of 100, 250, and 500 mw. The higher transmitter output power will obviously result in shortened battery run times and are probably not necessary as 100 mw can be good for over 10 km LOS. \ud83d\ude04 JackSilb \u2014 Yesterday at 10:02 AM You need a USB to TTL to connect it to a Jetson Nano or RPI USB no? Unless you want go direct into the I/Os. TCIII \u2014 Yesterday at 10:06 AM Using the Pro Micro programmed with the crsf_decode_hid.ino sketch allows the BetaFPV Nano serial channel receiver appear to be a HID compliant game controller, like a BT gamepad, to either the Rpi or the Nano without any additional hardware. JackSilb \u2014 Yesterday at 10:06 AM I have the Lite Radio 3 Pro with the cute small display. That will be useful for displaying some info. JackSilb \u2014 Yesterday at 10:07 AM We had something similar for the Teensy too. Stopped using it once we got the VESCs. TCIII \u2014 Yesterday at 10:07 AM That is excellent as you can program any BetaFPV receiver with the latest Expresslrs software. JackSilb \u2014 Yesterday at 10:08 AM Here is the design challenge. If you are using the ELRS radio UART direct to the JTN (Jetson Nano) or RPI and from there USB or Can to the VESC, how can we use the Multiplexer for the EMO. That works only for PWM. On the VESC you can configure a pin for a EMO, we would need a I/O from the Rx radio to bring it 1/0 for EMO direct into the VESC vs. using the JTN or RPI. Ideally, I will find a Rx radio with UART and some I/O capability that I can use a channel, lets say Channel 4 as the EMO switch. JackSilb \u2014 Yesterday at 10:21 AM Maybe this is I can pass the UART info along. Use the PWM for the EMO on/off (1/0) at the VESC TCIII \u2014 Yesterday at 10:31 AM The UART data is in crsf format and will need to be decoded using a Python Program: https://pypi.org/project/crsf-parser/ There are multichannel BetaFPV Expresslrs PWM receivers and I have one: https://www.amazon.com/BETAFPV-ExpressLRS-Compatible-Multirotors-Helicopters/dp/B09WHLJ2GN/ref=sr_1_2?crid=17HV1H415IRJR&keywords=BETAFPV+ExpressLRS+Micro+Receiver&qid=1679420003&sprefix=betafpv+expresslrs+micro+receiver%2Caps%2C107&sr=8-2 PyPI crsf-parser A package to parse and create CRSF (Crossfire) frames, developed primarily to interoperate with ExpressLRS Image BETAFPV ExpressLRS Micro Receiver Support 5 CH PWM Outputs Failsafe... Binding Procedure The Micro receiver comes with officially major release V2.0.0 protocol and has not been set for a Binding Phrase. So please make sure the RF TX module works on officially major release V2.0.0 protocol and no Binding Phrase has been set beforehand. Enter binding mode by plugging ... JackSilb \u2014 Yesterday at 10:42 AM I guess we can do this with ELRS UART -> Arduino or Teensy USB HID -> JTN or RPI and Arduino or Teensy I/O to the disable pin of the VESC. As long as the Arduino / Teensy code is solid and we have a watchdog we should be good. Adding the MCU in the mix get us going with lots of future opportunities... I need this for our evGoKart ASAP. I need to get some time to play with me. Lots of work(x2) on on the way Do you want me to share with you our work using the Teensy 4.0 including the PCB? I can send you a PCB too. TCIII \u2014 Yesterday at 10:45 AM I will have more time on Wednesday to test out the Pro Micro setup with the BetaFPV LoRa gamepad and receiver and get back to you with the results. TCIII \u2014 Yesterday at 10:46 AM Address: 11859 SE 91st Circle, Summerfield, FL 34491. JackSilb \u2014 Yesterday at 10:47 AM Can we do the Arduino code to be compatible with the Raspberry Pico? https://www.youtube.com/watch?v=Q97bFwjQ_vQ YouTube Jan Lunge Pi Pico + KMK = the perfect combo for Custom Keyboards Image https://www.youtube.com/watch?v=__QZQEOG6tA YouTube Print 'N Play Use A Raspberry Pi Pico as a HID [Gamepad, Keyboard, Mouse, and Mul... Image Probably a better HW than Arduino Pro Micro TCIII \u2014 Yesterday at 10:56 AM Probably as the Arduino sketch code is C++. JackSilb \u2014 Yesterday at 11:04 AM I purchased few of the Arduinos and Raspberry PI to experiment. Image Charging the controller already. We go from there. JackSilb \u2014 Yesterday at 11:12 AM Yeah, I have a similar from MATEKSYS too. At UCSD, I went away from the PWMs. We go from the JTN to the VESC using USB. The VESC can use the PPM input as output to drive a servo. My current need is for the autonomous evGoKart. Talking you made me find a solution for long distance EMO cheap vs. using the long range telemetry 3DR like only for the EMO. We were using it for the JoStick too with a ROS application on an RPI. Too complex. I will give a try using ELRS UART -> (Arduino Pro Micro or Teensy or Raspberry PICO) USB HID -> USB (Jetson or RPI) and (Arduino Pro Micro or Teensy or Raspberry PICO) I/O to the disable pin of the VESCs. We have one VESC for the throttle and one for the steering. TCIII \u2014 Yesterday at 11:34 AM I have a whole bunch of 3DR Telemetry Transceivers on 915 Mhz. \ud83d\ude42 They were only good for LOS even at 100 mw and highly directional too. \ud83d\ude41 TCIII \u2014 Yesterday at 11:38 AM I bought three of the Hiltego Arduino Pro Micros to work with the BetaFPV Nano receiver and all three programmed just fine with a Sparkfun version of Blinky as a test of their functionality. \ud83d\ude42 I have one Rpi PICO that Ed convinced me to buy as it it very fast compare to the Arduino Minis. REMEMBER that the BetaFPV LiteRadio 3 and the Pro must have the left joystick in the down position when you turn them on or they will buzz and flash the power button RED. As a result of this characteristic, don't energize your car ESC until you have the left joystick at the neutral position (assuming Mode 2) which is basically straight up or your car will go into reverse at full speed.\ud83d\ude32 TCIII \u2014 Today at 6:16 AM Hi Jack. I measured the Nano receiver serial output signal voltage with my digital o'scope this morning and it appears to be 3.43 v which should be safe as an input to either the Nano or the Rpi GPIO bus for direct decoding of the crsf signal. TCIII \u2014 Today at 7:19 AM Hi Jack. I connected the BetaFPV Nano serial output receiver to the Rx input of the Pico Micro running the crsf_decode_hid.ino sketch and viewed the Pico Micro USB HID output in the PC Game Controller Panel \"game controller settings\". I have the RC gamepad frame rate set at the default 150 frames/sec and the joystick display responses are virtually instantaneous and smooth unlike some gamepads. \ud83d\ude42 The \"+\" cursor in the X Axis / Y Axis box is presently controlled by the right joystick horizontal and vertical axes though the \"+\" moves in the opposite direction from the joystick direction of the vertical axis on the gamepad while the left joystick vertical axis (throttle) controls the Z axis red bar and the horizontal axis controls the X axis rotation. The gamepad left push on push off SA switch controls the Y rotation red bar while the left three way switch SB controls the Z rotation red bar. Observations concerning the Joystick axes and switches definitions: The Joystick HID descriptor report, shown below, controls the definition of the joystick axes and switches: Joystick_ Joystick(JOYSTICK_DEFAULT_REPORT_ID,JOYSTICK_TYPE_GAMEPAD, 0, 0, // Button Count, Hat Switch Count true, true, true, // X, Y, Z true, true, true, // Rx, Ry, Rz false, false, // Rudder, Throttle false, false, false); // Accelerator, Brake, Steering Based on a search of the IoT for information concerning the HID descriptor report, the HID descriptor shown above does not match any of the HID descriptor report documentation I could find. I suspect that the HID descriptor report shown above is unique to the Arduino Joystick Library requirements and we need to understand how to modify the HID descriptor report to meet the DC BT gamepad joystick/button requirements. TCIII \u2014 Today at 8:05 AM So we now have a choice on how we connect the LoRa RC gamepad receivers to our SBCs: Connect the Nano receiver UART serial output to a serial Rx pin on the SBC GPIO bus and create a part (https://pypi.org/project/crsf-parser/) that mimics a joystick gamepad or Connect the Nano receiver UART serial output to a Pico Micro running the crsf_decode_hid.ino sketch which makes the crsf output of the Nano receiver look like a HID gamepad (game controller) though it might require some modification of the Joystick HID descriptor report in the sketch to get the RC gamepad to work with DC as a BT gamepad. Comments? PyPI crsf-parser A package to parse and create CRSF (Crossfire) frames, developed primarily to interoperate with ExpressLRS Image TCIII \u2014 Today at 10:23 AM Hi Jack. Do you think that any of your graduate students would be interested in writing a DC part to decode the crsf data stream frames and provide functional gamepad output? \ud83d\ude42 If so, I will be glad to work with them. TCIII \u2014 Today at 11:30 AM I don't think that the Micro HID output looks completely like a PS4/XBox gamepad so the HID joystick descriptor report will probably have to be adjusted to look like a standard gamepad. Since there is no CONTROLLER_TYPE = defined for this HID RC gamepad, Users will have to use the Joystick Wizard to create a custom joystick? One of the issue I see is that LoRa RC gamepads are Mode 2 gamepads where the throttle is on the left joystick and the steering is on the right joystick. \ud83d\ude41 A standard BT gamepad is a Mode 1 gamepad where the throttle is on the right joystick and the steering is on the left joystick. TCIII \u2014 Today at 12:22 PM I suspect that the only way that this LoRa RC gamepad is really going to be really \"user transparent\" is to create a part that decodes and process the crsf serial data stream to create a standard gamepad? TCIII \u2014 Today at 12:42 PM Hi Jack. Here is a tutorial on how to use an RC Radio transmitter as a gamepad controller: https://www.plop.at/en/rc.html It might give us some insight on modifying, if necessary, the crsf_decode_hid.ino sketch to simulate a generic gamepad? I have contacted the author of the crsf_decode_hid.ino sketch about the joystick HID descriptor report that he used and how he created it. TCIII \u2014 Today at 1:39 PM Hi Jack. Attached is a screen shot of what the output of the RC Gamepad/Pro Micro USB HID Game Controller Panel looks like. The \"+\" cursor in the X Axis / Y Axis box is presently controlled by the right joystick horizontal (Steering) and vertical axes though the \"+\" moves in the opposite direction from the joystick direction of the vertical axis on the gamepad while the left joystick vertical axis (Throttle) controls the Z axis red bar and the horizontal axis controls the X axis rotation. The gamepad left push on push off SA switch controls the Y rotation red bar while the left three way switch SB controls the Z rotation red bar. Image Some background and References PWM PPM Hi guys, I have a question about modifying the controller for the donkey car base on this link https://docs.google.com/document/d/1yx1mF0dgEHLx5S3Vqss8YFFkW4thCrl9fpgVliHcuhw/edit we are able to get the signals from controller to the Arduino after modifying the controller, but now we need to know how to connect the Arduino Leonardo to PWM. Ehsan, the Arduino connects to the JTN or RPI. The PWM board does not change until we have the Teensy in the loop. Therefore, the PWM board is the same. It uses I2C to communicate with the JTN or RPI. That is why it is not in the instructions. Basically the Arduino is simulating a JS0 (Joystick) in a Linux machine. It converts PWM signals from the RC controllers as it was a Game/Pad Joystick. The PWM board is still the same, nothing changes. You need to get the latest Donkey, 3.1.1 if I am not mistaken, and change the Joystick Type on myconfig.py along with your calibration values, Webcam, and use channel 1 for steering, channel 2 for Throttle. The only difference on the myconfig.py for using the Arudino (Leonardo) simulating the Joystick is the setting on the Joystick/game controller type. Everything else is as you were using a regular Donkey setup. https://github.com/n6wxd/wireless-rc-adapter/blob/master/wireless-rc-adapter-2.1/src/PinChangeInterrupt/README.md Arduino Uno/Nano/Mini: All pins are usable Arduino Mega: 10, 11, 12, 13, 50, 51, 52, 53, A8 (62), A9 (63), A10 (64), A11 (65), A12 (66), A13 (67), A14 (68), A15 (69) Arduino Leonardo/Micro: 8, 9, 10, 11, 14 (MISO), 15 (SCK), 16 (MOSI) HoodLoader2: All (broken out 1-7) pins are usable Attiny 24/44/84: All pins are usable Attiny 25/45/85: All pins are usable Attiny 13: All pins are usable Attiny 441/841: All pins are usable ATmega644P/ATmega1284P: All pins are usable Arduino Leonardo/Micro allows interrupts on : 8, 9, 10, 11, 14 (MISO), 15 (SCK), 16 (MOSI) So, let\u2019s use 8, 9, 10 https://github.com/n6wxd/wireless-rc-adapter/blob/master/wireless-rc-adapter-2.1/pwm.ino elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO) const uint8_t RC_PINS[6] = {11, 10, 9, 8, PB2, PB1}; https://github.com/n6wxd/wireless-rc-adapter https://github.com/wireless-rc-adapter/wireless-rc-adapter/wiki We just need 3 channels from the RC Rx The Arduino board we got out of Amazon.com (Pro Micro) it is compatible with Arduino Leonardo firmware not Arduino Pro Micro. Also, it does not have Pin 11 available for a connection. We need to change the pins used at the board and Arduino IDE to be 10,9,8 At pwm.ino From elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO) Ch1 -> Pin 8 Ch2 -> Pin 9 Ch3 -> Pin 10 Also, Ch3 \u201c+\u201d and \u201c-\u201d are used to power the RC Rx Radio Ch3+ (middle pin) -> Pin VCC (5V) Ch3- -> Pin GND (Ground) Note: The power to the Arduino comes from the USB cable that connects to the JTN or RPI. The power to the RC radio comes from the Arduino. https://inventr.io/blogs/arduino/arduino-pro-micro-review-scroller The GPIO for Jetson Nano and RPI are the same. Connect the PWM board using I2C. See instructions for ECE MAE 148. Nothing changes here. The steering servo and the ESC connect to the PWM board, not Arudino. How to Make it Work Install the latest version of the Arduino IDE from here Download the Arduino files from here to a local directory https://github.com/n6wxd/wireless-rc-adapter Plug the Arduino into one of your computer USB port Configure the Arduino GUI to use Arduino Leonardo Configure the USB Port used for the Arduino wireless-rc-adapter-2.1.ino Open wireless-rc-adapter-2.1.ino in the Arduino GUI pwm.ino The Arduino IDE should show a tab for pwm.ino Click over the tab pwm.ino Change the pins as below. Our radio has 3 channels, we just use the first 3 pins numbered 10,9,8 elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO) const uint8_t RC_PINS[6] = {8, 9, 10, PB2, PB1,11}; Save the file led.ino Change back what Steve B. modified for the LEDs From elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO) #define RXLED 13 // RXLED is on pin 17<br> #define TXLED 30 // TXLED is on pin 30<br><br> To elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO) #define RXLED 17 // RXLED is on pin 17<br> #define TXLED 30 // TXLED is on pin 30<br> Save the file Upload the wireless-rc-adapter-2.1.ino into the Aruduino, the LEDs should blink when uploading the software then blink in different patterns depending on software stage See here https://github.com/wireless-rc-adapter/wireless-rc-adapter/wiki wireless-rc-adapter-2.1.ino First make sure you pair your RC Tx and Rx ... To verify that the software is working you can enable the serial debugging. Edit wireless-rc-adapter-2.1.ino Remove the comments in the front of //#define SERIAL_DEBUG and //#define SERIAL_SPD 115200 // >>> Serial-Debug options for troubleshooting <<< define SERIAL_DEBUG // Enable Serial Debug by uncommenting this line define SERIAL_SPD 115200 // Set debug bitrate between 9600-115200 bps (default: 9600) Save the file Upload / run Then using the terminal from the Arduino IDE you can follow the boot process and calibrations. You can also use that to verify the calibration and values the Arduino is receiving from the RC receiver. After you verify that the 3 channels are working, comment the serial DEBUG lines back, save, upload/run // >>> Serial-Debug options for troubleshooting <<< define SERIAL_DEBUG // Enable Serial Debug by uncommenting this line define SERIAL_SPD 115200 // Set debug bitrate between 9600-115200 bps (default: 9600) You can test the JS0 operation in a Linux machine, including the Jetson Nano (JTN) and Raspberry PI (RPI) with #Open a terminal #ls /dev/input/ #this command should show a js0 device listed e.g, #ls /dev/input _#by-id by-path event0 event1 js0 mice #Then lets try reading joystick values #sudo apt-get install joystick #sudo jstest /dev/input/js0 If you don\u2019t see the joystick working on the JTN or RPI, then don\u2019t try to use it on Donkey. Calibration On every startup it tries to load calibration data from the long-term memory and decides if those values are still correct or out of sync. The algorithm triggers a calibration automatically if necessary. Calibration can be triggered manually with powering the adapter while the CAL_CHANNEL is on full duty cycle. In other words the throttle must be up before start and it is automatically starts calibration on boot. When the device is in calibration mode, the leds are flashing steady on the board, and all the channel minimum and maximum values are getting recorded. During this time move all the configured sticks and pots/switches on the transmitter remote to its extents. After there is no more new min's or max's found the algorithm finishing the calibration within CAL_TIMEOUT by checking and saving the values in the long-term memory (eeprom). Leds are flashing twice and turns off, the adapter is now available as joystick on the device it is plugged in.","title":"Controller"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#ucsd-robocar-using-rc-controller","text":"","title":"UCSD Robocar Using RC Controller"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#arduino-micro-pro-leonard-compatible-22mar23-v20","text":"","title":"Arduino Micro Pro - Leonard Compatible 22Mar23 - V2.0 "},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#expresslrs-24ghz-elrs-radio-long-range-digital","text":"#Here are the radios we have for UCSD evGKart #There radios use the open source ELRS #There are several advantages on ELRS: Long Range Low Latency, several suppliers, software upgradable #There are few options for radios. We got the radios below because they were compacts, seemed rugged, and received good reviews #These radios will allow us to keep it all digital without the need to use PWM/PPM. ex: Radio UART - > Single Board Computer (SBC). We will use a microcontroller (MCU) to translate near real-time the protocol used on the radios into serial communication with a SBC using USB and also use the MCU for the emergency stop (off) [EMO] directly from the radio command vs. using separate radio for the EMO. Since the radio for PPM/PWM was affordable we have one too in case we want to directly control RC Cars and or use a multiplexer as part of our EMO. This would require two PWM like cables for each radio channel used.","title":"ExpressLRS 2.4GHz (ELRS) Radio Long Range Digital"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#betafpv-literadio-3-pro-radio-transmitter-elrs-supports-external-nano-tx-module","text":"Links Image https://betafpv.com/collections/tx/products/literadio-3-pro-radio-transmitter https://support.betafpv.com/hc/en-us/articles/5987468200601-Manual-for-Lite-Radio3-Pro https://www.getfpv.com/betafpv-literadio-3-pro-radio-transmitter-elrs-supports-external-nano-tx-module.html","title":"BetaFPV LiteRadio 3 Pro Radio Transmitter- ELRS (Supports External Nano TX Module)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#mateksys-expresslrs-24ghz-receiver-elrs-r24-d","text":"Links Image https://www.getfpv.com/mateksys-expresslrs-2-4ghz-receiver-elrs-r24-d.html","title":"MATEKSYS ExpressLRS 2.4GHz Receiver - ELRS R24 D"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#mateksys-expresslrs-24ghz-receiver-pwm-elrs-r24-p6","text":"Links Image https://www.getfpv.com/mateksys-expresslrs-2-4ghz-receiver-pwm-elrs-r24-p6.html","title":"MATEKSYS ExpressLRS 2.4GHz Receiver - PWM ELRS-R24-P6"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#upgrading-the-elrs-firmware","text":"24Mar23 #Get back to revise these to show version 3.2.0 again. I had to use 3.2.0 because it is the version that ELRS configurator has for the receiver. It seems I need to keep the ERLSv2.luna vs3 for the radio to read the luna script. I hope they will fix it in the future. _#There is a good software that helps on on the upgrade https://www.expresslrs.org/quick-start/installing-configurator/ #They support Windows, Mac, and Linux #Ideally you upgrade the radios Tx and Rx on the same session to make sure they have the same firmware version #Also if you have multiple of the same Tx and Rx doing them consecutively can save you time and help reduce the risk of having different firmware versions. #For the BETA FPV 3 PRO https://www.expresslrs.org/quick-start/transmitters/betfpvlr3pro/ #Turn on the TX radio #Plum the USB C cable connected to your computer #The radio will ask what connection to use #Select USB Serial (Debug) #You will need to look for the Radio in the connections options #Version 2.5.2 #Build & Flash #The first time it will take a while to download, install tools and build the firmware #Download and save on your computer the the LUA Script file #Unplug the USB cable, turn off the radio, turn on again #To upload the LUA script to the radio, unplug the USB cable if connected, connect it again #Select USB Storage (SD) #Then you can use your computer to upload the LUA script you saved earlier https://www.expresslrs.org/quick-start/transmitters/lua-howto/ Download the ELRSv3 Lua Script (you can simply right-click, save-as) into your radio's SD Card under the Scripts/Tools #elrsV2.lua #Also, let's delete the older .lua script from the root of the DISK_IMG of the radio #Remove the USB cable from the radio #Hold the right top joystick to the left for 1~2 seconds #Tools #EspressLRS #See if the configuration loads, here is where I have problems with V3.x for some problem. V2.5.2 works just fine #The BetaFPV LiteRadio 3 Pro Radio Transmitter uses the two smaller joystick to navigate the configuration menu Moving the right side change the GUI on the display Moving the right side to the left and holding it for few seconds get you into the settings","title":"Upgrading the ELRS firmware"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#receiver-matekys-r24-d-24ghz","text":"https://www.expresslrs.org/quick-start/receivers/matek2400/ If this is the first time you're flashing/updating your receiver or you're updating it from a previous 2.x firmware via WiFi, first ensure that it has version 2.5.2. Once it has the 2.5.2 flashed, update to 3.x. #Connect to the Wifi Network the receiver has created. It should be named something like ExpressLRS RX with the same expresslrs password as the TX Module Hotspot. #After the Rx radio boots, wait to see the LED flashing quick. That is an indicadion that its Access Point and web server is running. #Using a web browser http://10.0.0.1/ #Connect to the website of the device and upload new firmware Then if you connect the receiver to your local WiFi you can get to it by usings its IP address or name. You can scan the network and look for a device called elrs_rx ex: http://elrs_rx.local #Because we will be on the field and not necessarily close to WiFi, let\u2019s leave the AP of the radio on so we can configure it. Not very safe since someone can connect to it while the AP is on. We will check how to protect it with a password on it a bit later. Moreover, I did not see how to name the radios, it would be hard to know what Rx radios is what in the network. #Let's get the Rx to be on 115200 baud rate #Just connect to the webinterface of the Rx and set the baudrate #We need to use a USB to TTL cable - example from Amazon #Or you can use the embedded WiFi, how cool is that? Lets try the UART first Red wire of the USB to TTL = + Black wire of the USB to TTL = - Green = White = #Press and hold the boot button while connecting the USB to TTL device to your computer http://www.mateksys.com/?portfolio=elrs-r24#tab-id-3 For ELRS-R24-D, if update from 2.x to 3.x, Pls select target MATEK 2400 RX R24D and click on Force Flash https://github.com/kkbin505/Simple_RX Readme.md CRSF decode library for arduino atmega32u4/328p. Based on arduino SBUS decode, modified to decode crsf protocol from elrs receiver #Let's keep it simple first just by using Arduinos, then we will try a Raspberry PI Pico, and then UCSD DRTC if we want to make everything CAN https://www.amazon.com/gp/product/B09C5H78BP/ref=ppx_yo_dt_b_asin_title_o01_s02?ie=UTF8&psc=1 #I got this coming too so we use a better USB connector, USB C vs. uUSB. #I just did not see there was a version with USB C earlier. https://www.amazon.com/gp/product/B0BCW67NJP/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1 https://github.com/kkbin505/Simple_RX","title":"Receiver MATEKYS R24-D 2.4Ghz"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#some-background-and-references-elrs-radio-long-range-digital","text":"TCIII \u2014 03/17/2023 1:11 PM Hi Jack. I spent half of today learning about how to update (flash) the BETAFPV gamepad/plug-in transmitter/receiver software. Both the gamepad or the plug-in transmitter software version has to be the same as the receiver and vice versa. The gamepad configuration and update flashing can be done with the BETAFPV Configurator while even BETAFPV uses the ELRS Configurator to update the plug-in transmitter and receiver. Nothing like consistency. \ud83d\ude04 The BETAFPV LiteRadio 3 gamepad RC transmitter can output 25, 50, or 100 mw while the plug-in RC transmitter can output 100, 250, or 500 mw and comes with ELRS version 2.01. The Matek ELRS six channel PWM receiver comes with ELRS version 3.0 while the BETAFPV Nano serial output receiver comes with ELRS version 1.0.0-RC5 so I am going to have to reprogram both receivers with ELRS version 2.01. These LoRa gamepads and receivers are definitely not for beginners. \ud83d\ude32 TCIII \u2014 Yesterday at 7:06 AM Hi Jack. I think that it would be best to converse about the Expresslrs LoRa gamepad project on DM until we get the circuitry and software validated, otherwise we might wind up with DC Users trying to implement a LoRa gamepad system that has not been full vetted for functionality and have substantial issues. TCIII \u2014 Yesterday at 7:25 AM Hi Jack. I bought a couple of Arduino Pro Micros so that I could use the crsf_decode_hid.ino sketch from https://github.com/kkbin505/Simple_RX The sketch compiled just fine after I installed the additional required libraries and I will connect the serial output of the BetaFPV serial output Nano receiver to the Pro Micro and see what kind of output I get using the serial monitor. The Arduino Joystick Library (https://github.com/MHeironimus/ArduinoJoystic+kLibrary) is used by the crsf_decode_hid.ino sketch, but the included sketches in the library are designed to take individual joystick and button inputs connected to the Pro Micro and and produce a HID joystick that can be used by PCs for games. Could you please take a look at the crsf_decode_hid.ino sketch as I understand the debug portion, but I don't see where he is outputting the decoded crsf gamepad joystick/button HID values on the USB port. If the crsf_decode_hid.ino sketch is really making the Nano Receiver crsf output look like a HID gamepad then we have it made, though it does require the Pro Micro to interface the Receiver to the SBC as an HID device. \ud83d\ude42 I am using a BetaFPV LiteRadio 3 RC gamepad to do the testing compared to your BetFPV LiteRadio 3 Pro. All of the BetaFPV products are still using Expresslrs version 2.x when Expresslrs version 3.x is now available. The BetaFPV LiteRadio 3 can only be programmed with the BetaFPV Configurator so that gamepad is limited to Expresslrs version 2.x where as your BetFPV LiteRadio 3 Pro can be programmed with the Expresslrs Configurator and can be programmed with Expresslrs 3.x. Remember, both the RC gamepad and the receiver must have the same software version of Expresslrs and pass phrase to bind correctly. GitHub GitHub - kkbin505/Simple_RX: crsf protocol decode for avr crsf protocol decode for avr. Contribute to kkbin505/Simple_RX development by creating an account on GitHub. GitHub - kkbin505/Simple_RX: crsf protocol decode for avr GitHub GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... An Arduino library that adds one or more joysticks to the list of HID devices an Arduino Leonardo or Arduino Micro can support. - GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... This afternoon I will try to determine the voltage of the BetaFPV Nano serial channel receiver output. It will either be 5 vdc or 3.3 vdc. The Nano receiver is powered by 5 vdc, but there is a 3.3 vdc on the Nano receiver pwb because the receiver/processor and WIFI ICs work on 3.3 vdc. So it would seem to me that the serial channel is a 3.3 vdc signal output which will be compatible with either the Rpi or Nano GPIO bus for onboard crsf decoding. Though I like the fact that the Pro Micro, when programmed with the crsf_decode_hid.ino sketch looks just like a HID compliant game controller which makes it portable between RC cars. Additionally the Joystick Descriptor in the crsf_decode_hid.ino sketch allows the customizing of the crsf data stream as to the type of LoRa game controller in use. TCIII \u2014 Yesterday at 9:53 AM Observations concerning BetaFPV Expresslrs products: 1. Only the BetaFPV LiteRadio 3 Pro can be programmed with the Expresslrs Configurator, unlike the LiteRadio 3 and the LiteRadio 2 SE which require the BetaFPV Configurator, which allows access to the latest Expresslrs version 3.x. 2. The LiteRadio 3 and the LiteRadio 2 SE must programmed with the BetaFPV Configurator because they use a custom version of Expresslrs which is stuck at version 2.x.\ud83d\ude41 3. All of the BetaFPV receivers can be programmed with the Expresslrs Configurator, but keep in mind that both the BetaFPV RC gamepads and receivers must have the same version of the software and pass phrase to bind. 4. Therefore if you are using a LiteRadio 3 or a LiteRadio 2 SE, you cannot upgrade the software of any of the BetaFPV receivers to Expresslrs 3.x or you will lose binding capability. 5. The default baud rate of the BetaFPV Nano serial channel receiver UART is 420000 baud so I had to use the Expresslrs Configurator to change the Nano receiver's UART baud rate to 115200 baud to work with the Pro Micro. 6. Unfortunately the Expresslrs Configurator will only show Expresslrs version 2.5 and up for flashing and the Nano receiver for my purposes needed software version 2.x to update the UART baud rate. 7. Fortunately the Expresslrs Configurator allows access to the Expresslrs software version archives so I could use Expresslrs version 2.0 to update the UART baud rate. \ud83d\ude42 Since I bought the BetaFPV LiteRadio 3, which has an external transmitter port, I also purchased the BetaFPV Nano RF TX Module that can be programmed with the Expresslrs Configurator fortunately. The stock BetaFPV LiteRadio 3 has an output of 100 mw while the BetaFPV Nano RF TX Module can be programmed for outputs of 100, 250, and 500 mw. The higher transmitter output power will obviously result in shortened battery run times and are probably not necessary as 100 mw can be good for over 10 km LOS. \ud83d\ude04 JackSilb \u2014 Yesterday at 10:02 AM You need a USB to TTL to connect it to a Jetson Nano or RPI USB no? Unless you want go direct into the I/Os. TCIII \u2014 Yesterday at 10:06 AM Using the Pro Micro programmed with the crsf_decode_hid.ino sketch allows the BetaFPV Nano serial channel receiver appear to be a HID compliant game controller, like a BT gamepad, to either the Rpi or the Nano without any additional hardware. JackSilb \u2014 Yesterday at 10:06 AM I have the Lite Radio 3 Pro with the cute small display. That will be useful for displaying some info. JackSilb \u2014 Yesterday at 10:07 AM We had something similar for the Teensy too. Stopped using it once we got the VESCs. TCIII \u2014 Yesterday at 10:07 AM That is excellent as you can program any BetaFPV receiver with the latest Expresslrs software. JackSilb \u2014 Yesterday at 10:08 AM Here is the design challenge. If you are using the ELRS radio UART direct to the JTN (Jetson Nano) or RPI and from there USB or Can to the VESC, how can we use the Multiplexer for the EMO. That works only for PWM. On the VESC you can configure a pin for a EMO, we would need a I/O from the Rx radio to bring it 1/0 for EMO direct into the VESC vs. using the JTN or RPI. Ideally, I will find a Rx radio with UART and some I/O capability that I can use a channel, lets say Channel 4 as the EMO switch. JackSilb \u2014 Yesterday at 10:21 AM Maybe this is I can pass the UART info along. Use the PWM for the EMO on/off (1/0) at the VESC TCIII \u2014 Yesterday at 10:31 AM The UART data is in crsf format and will need to be decoded using a Python Program: https://pypi.org/project/crsf-parser/ There are multichannel BetaFPV Expresslrs PWM receivers and I have one: https://www.amazon.com/BETAFPV-ExpressLRS-Compatible-Multirotors-Helicopters/dp/B09WHLJ2GN/ref=sr_1_2?crid=17HV1H415IRJR&keywords=BETAFPV+ExpressLRS+Micro+Receiver&qid=1679420003&sprefix=betafpv+expresslrs+micro+receiver%2Caps%2C107&sr=8-2 PyPI crsf-parser A package to parse and create CRSF (Crossfire) frames, developed primarily to interoperate with ExpressLRS Image BETAFPV ExpressLRS Micro Receiver Support 5 CH PWM Outputs Failsafe... Binding Procedure The Micro receiver comes with officially major release V2.0.0 protocol and has not been set for a Binding Phrase. So please make sure the RF TX module works on officially major release V2.0.0 protocol and no Binding Phrase has been set beforehand. Enter binding mode by plugging ... JackSilb \u2014 Yesterday at 10:42 AM I guess we can do this with ELRS UART -> Arduino or Teensy USB HID -> JTN or RPI and Arduino or Teensy I/O to the disable pin of the VESC. As long as the Arduino / Teensy code is solid and we have a watchdog we should be good. Adding the MCU in the mix get us going with lots of future opportunities... I need this for our evGoKart ASAP. I need to get some time to play with me. Lots of work(x2) on on the way Do you want me to share with you our work using the Teensy 4.0 including the PCB? I can send you a PCB too. TCIII \u2014 Yesterday at 10:45 AM I will have more time on Wednesday to test out the Pro Micro setup with the BetaFPV LoRa gamepad and receiver and get back to you with the results. TCIII \u2014 Yesterday at 10:46 AM Address: 11859 SE 91st Circle, Summerfield, FL 34491. JackSilb \u2014 Yesterday at 10:47 AM Can we do the Arduino code to be compatible with the Raspberry Pico? https://www.youtube.com/watch?v=Q97bFwjQ_vQ YouTube Jan Lunge Pi Pico + KMK = the perfect combo for Custom Keyboards Image https://www.youtube.com/watch?v=__QZQEOG6tA YouTube Print 'N Play Use A Raspberry Pi Pico as a HID [Gamepad, Keyboard, Mouse, and Mul... Image Probably a better HW than Arduino Pro Micro TCIII \u2014 Yesterday at 10:56 AM Probably as the Arduino sketch code is C++. JackSilb \u2014 Yesterday at 11:04 AM I purchased few of the Arduinos and Raspberry PI to experiment. Image Charging the controller already. We go from there. JackSilb \u2014 Yesterday at 11:12 AM Yeah, I have a similar from MATEKSYS too. At UCSD, I went away from the PWMs. We go from the JTN to the VESC using USB. The VESC can use the PPM input as output to drive a servo. My current need is for the autonomous evGoKart. Talking you made me find a solution for long distance EMO cheap vs. using the long range telemetry 3DR like only for the EMO. We were using it for the JoStick too with a ROS application on an RPI. Too complex. I will give a try using ELRS UART -> (Arduino Pro Micro or Teensy or Raspberry PICO) USB HID -> USB (Jetson or RPI) and (Arduino Pro Micro or Teensy or Raspberry PICO) I/O to the disable pin of the VESCs. We have one VESC for the throttle and one for the steering. TCIII \u2014 Yesterday at 11:34 AM I have a whole bunch of 3DR Telemetry Transceivers on 915 Mhz. \ud83d\ude42 They were only good for LOS even at 100 mw and highly directional too. \ud83d\ude41 TCIII \u2014 Yesterday at 11:38 AM I bought three of the Hiltego Arduino Pro Micros to work with the BetaFPV Nano receiver and all three programmed just fine with a Sparkfun version of Blinky as a test of their functionality. \ud83d\ude42 I have one Rpi PICO that Ed convinced me to buy as it it very fast compare to the Arduino Minis. REMEMBER that the BetaFPV LiteRadio 3 and the Pro must have the left joystick in the down position when you turn them on or they will buzz and flash the power button RED. As a result of this characteristic, don't energize your car ESC until you have the left joystick at the neutral position (assuming Mode 2) which is basically straight up or your car will go into reverse at full speed.\ud83d\ude32 TCIII \u2014 Today at 6:16 AM Hi Jack. I measured the Nano receiver serial output signal voltage with my digital o'scope this morning and it appears to be 3.43 v which should be safe as an input to either the Nano or the Rpi GPIO bus for direct decoding of the crsf signal. TCIII \u2014 Today at 7:19 AM Hi Jack. I connected the BetaFPV Nano serial output receiver to the Rx input of the Pico Micro running the crsf_decode_hid.ino sketch and viewed the Pico Micro USB HID output in the PC Game Controller Panel \"game controller settings\". I have the RC gamepad frame rate set at the default 150 frames/sec and the joystick display responses are virtually instantaneous and smooth unlike some gamepads. \ud83d\ude42 The \"+\" cursor in the X Axis / Y Axis box is presently controlled by the right joystick horizontal and vertical axes though the \"+\" moves in the opposite direction from the joystick direction of the vertical axis on the gamepad while the left joystick vertical axis (throttle) controls the Z axis red bar and the horizontal axis controls the X axis rotation. The gamepad left push on push off SA switch controls the Y rotation red bar while the left three way switch SB controls the Z rotation red bar. Observations concerning the Joystick axes and switches definitions: The Joystick HID descriptor report, shown below, controls the definition of the joystick axes and switches: Joystick_ Joystick(JOYSTICK_DEFAULT_REPORT_ID,JOYSTICK_TYPE_GAMEPAD, 0, 0, // Button Count, Hat Switch Count true, true, true, // X, Y, Z true, true, true, // Rx, Ry, Rz false, false, // Rudder, Throttle false, false, false); // Accelerator, Brake, Steering Based on a search of the IoT for information concerning the HID descriptor report, the HID descriptor shown above does not match any of the HID descriptor report documentation I could find. I suspect that the HID descriptor report shown above is unique to the Arduino Joystick Library requirements and we need to understand how to modify the HID descriptor report to meet the DC BT gamepad joystick/button requirements. TCIII \u2014 Today at 8:05 AM So we now have a choice on how we connect the LoRa RC gamepad receivers to our SBCs: Connect the Nano receiver UART serial output to a serial Rx pin on the SBC GPIO bus and create a part (https://pypi.org/project/crsf-parser/) that mimics a joystick gamepad or Connect the Nano receiver UART serial output to a Pico Micro running the crsf_decode_hid.ino sketch which makes the crsf output of the Nano receiver look like a HID gamepad (game controller) though it might require some modification of the Joystick HID descriptor report in the sketch to get the RC gamepad to work with DC as a BT gamepad. Comments? PyPI crsf-parser A package to parse and create CRSF (Crossfire) frames, developed primarily to interoperate with ExpressLRS Image TCIII \u2014 Today at 10:23 AM Hi Jack. Do you think that any of your graduate students would be interested in writing a DC part to decode the crsf data stream frames and provide functional gamepad output? \ud83d\ude42 If so, I will be glad to work with them. TCIII \u2014 Today at 11:30 AM I don't think that the Micro HID output looks completely like a PS4/XBox gamepad so the HID joystick descriptor report will probably have to be adjusted to look like a standard gamepad. Since there is no CONTROLLER_TYPE = defined for this HID RC gamepad, Users will have to use the Joystick Wizard to create a custom joystick? One of the issue I see is that LoRa RC gamepads are Mode 2 gamepads where the throttle is on the left joystick and the steering is on the right joystick. \ud83d\ude41 A standard BT gamepad is a Mode 1 gamepad where the throttle is on the right joystick and the steering is on the left joystick. TCIII \u2014 Today at 12:22 PM I suspect that the only way that this LoRa RC gamepad is really going to be really \"user transparent\" is to create a part that decodes and process the crsf serial data stream to create a standard gamepad? TCIII \u2014 Today at 12:42 PM Hi Jack. Here is a tutorial on how to use an RC Radio transmitter as a gamepad controller: https://www.plop.at/en/rc.html It might give us some insight on modifying, if necessary, the crsf_decode_hid.ino sketch to simulate a generic gamepad? I have contacted the author of the crsf_decode_hid.ino sketch about the joystick HID descriptor report that he used and how he created it. TCIII \u2014 Today at 1:39 PM Hi Jack. Attached is a screen shot of what the output of the RC Gamepad/Pro Micro USB HID Game Controller Panel looks like. The \"+\" cursor in the X Axis / Y Axis box is presently controlled by the right joystick horizontal (Steering) and vertical axes though the \"+\" moves in the opposite direction from the joystick direction of the vertical axis on the gamepad while the left joystick vertical axis (Throttle) controls the Z axis red bar and the horizontal axis controls the X axis rotation. The gamepad left push on push off SA switch controls the Y rotation red bar while the left three way switch SB controls the Z rotation red bar. Image","title":"Some Background and References ELRS Radio (Long Range Digital)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#some-background-and-references-pwm-ppm","text":"Hi guys, I have a question about modifying the controller for the donkey car base on this link https://docs.google.com/document/d/1yx1mF0dgEHLx5S3Vqss8YFFkW4thCrl9fpgVliHcuhw/edit we are able to get the signals from controller to the Arduino after modifying the controller, but now we need to know how to connect the Arduino Leonardo to PWM. Ehsan, the Arduino connects to the JTN or RPI. The PWM board does not change until we have the Teensy in the loop. Therefore, the PWM board is the same. It uses I2C to communicate with the JTN or RPI. That is why it is not in the instructions. Basically the Arduino is simulating a JS0 (Joystick) in a Linux machine. It converts PWM signals from the RC controllers as it was a Game/Pad Joystick. The PWM board is still the same, nothing changes. You need to get the latest Donkey, 3.1.1 if I am not mistaken, and change the Joystick Type on myconfig.py along with your calibration values, Webcam, and use channel 1 for steering, channel 2 for Throttle. The only difference on the myconfig.py for using the Arudino (Leonardo) simulating the Joystick is the setting on the Joystick/game controller type. Everything else is as you were using a regular Donkey setup. https://github.com/n6wxd/wireless-rc-adapter/blob/master/wireless-rc-adapter-2.1/src/PinChangeInterrupt/README.md Arduino Uno/Nano/Mini: All pins are usable Arduino Mega: 10, 11, 12, 13, 50, 51, 52, 53, A8 (62), A9 (63), A10 (64), A11 (65), A12 (66), A13 (67), A14 (68), A15 (69) Arduino Leonardo/Micro: 8, 9, 10, 11, 14 (MISO), 15 (SCK), 16 (MOSI) HoodLoader2: All (broken out 1-7) pins are usable Attiny 24/44/84: All pins are usable Attiny 25/45/85: All pins are usable Attiny 13: All pins are usable Attiny 441/841: All pins are usable ATmega644P/ATmega1284P: All pins are usable Arduino Leonardo/Micro allows interrupts on : 8, 9, 10, 11, 14 (MISO), 15 (SCK), 16 (MOSI) So, let\u2019s use 8, 9, 10 https://github.com/n6wxd/wireless-rc-adapter/blob/master/wireless-rc-adapter-2.1/pwm.ino","title":"Some background and References PWM PPM"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo","text":"const uint8_t RC_PINS[6] = {11, 10, 9, 8, PB2, PB1}; https://github.com/n6wxd/wireless-rc-adapter https://github.com/wireless-rc-adapter/wireless-rc-adapter/wiki We just need 3 channels from the RC Rx The Arduino board we got out of Amazon.com (Pro Micro) it is compatible with Arduino Leonardo firmware not Arduino Pro Micro. Also, it does not have Pin 11 available for a connection. We need to change the pins used at the board and Arduino IDE to be 10,9,8 At pwm.ino From","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_1","text":"Ch1 -> Pin 8 Ch2 -> Pin 9 Ch3 -> Pin 10 Also, Ch3 \u201c+\u201d and \u201c-\u201d are used to power the RC Rx Radio Ch3+ (middle pin) -> Pin VCC (5V) Ch3- -> Pin GND (Ground) Note: The power to the Arduino comes from the USB cable that connects to the JTN or RPI. The power to the RC radio comes from the Arduino. https://inventr.io/blogs/arduino/arduino-pro-micro-review-scroller The GPIO for Jetson Nano and RPI are the same. Connect the PWM board using I2C. See instructions for ECE MAE 148. Nothing changes here. The steering servo and the ESC connect to the PWM board, not Arudino.","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#how-to-make-it-work","text":"Install the latest version of the Arduino IDE from here Download the Arduino files from here to a local directory https://github.com/n6wxd/wireless-rc-adapter Plug the Arduino into one of your computer USB port Configure the Arduino GUI to use Arduino Leonardo Configure the USB Port used for the Arduino","title":"How to Make it Work"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#wireless-rc-adapter-21ino","text":"Open wireless-rc-adapter-2.1.ino in the Arduino GUI","title":"wireless-rc-adapter-2.1.ino"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#pwmino","text":"The Arduino IDE should show a tab for pwm.ino Click over the tab pwm.ino Change the pins as below. Our radio has 3 channels, we just use the first 3 pins numbered 10,9,8","title":"pwm.ino"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_2","text":"const uint8_t RC_PINS[6] = {8, 9, 10, PB2, PB1,11}; Save the file","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#ledino","text":"Change back what Steve B. modified for the LEDs From","title":"led.ino"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_3","text":"#define RXLED 13 // RXLED is on pin 17<br> #define TXLED 30 // TXLED is on pin 30<br><br> To","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_4","text":"#define RXLED 17 // RXLED is on pin 17<br> #define TXLED 30 // TXLED is on pin 30<br> Save the file Upload the wireless-rc-adapter-2.1.ino into the Aruduino, the LEDs should blink when uploading the software then blink in different patterns depending on software stage See here https://github.com/wireless-rc-adapter/wireless-rc-adapter/wiki","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#wireless-rc-adapter-21ino_1","text":"First make sure you pair your RC Tx and Rx ... To verify that the software is working you can enable the serial debugging. Edit wireless-rc-adapter-2.1.ino Remove the comments in the front of //#define SERIAL_DEBUG and //#define SERIAL_SPD 115200 // >>> Serial-Debug options for troubleshooting <<< define SERIAL_DEBUG // Enable Serial Debug by uncommenting this line define SERIAL_SPD 115200 // Set debug bitrate between 9600-115200 bps (default: 9600) Save the file Upload / run Then using the terminal from the Arduino IDE you can follow the boot process and calibrations. You can also use that to verify the calibration and values the Arduino is receiving from the RC receiver. After you verify that the 3 channels are working, comment the serial DEBUG lines back, save, upload/run // >>> Serial-Debug options for troubleshooting <<<","title":"wireless-rc-adapter-2.1.ino"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#define-serial_debug-enable-serial-debug-by-uncommenting-this-line","text":"","title":"define SERIAL_DEBUG  // Enable Serial Debug by uncommenting this line"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#define-serial_spd-115200-set-debug-bitrate-between-9600-115200-bps-default-9600","text":"You can test the JS0 operation in a Linux machine, including the Jetson Nano (JTN) and Raspberry PI (RPI) with #Open a terminal #ls /dev/input/ #this command should show a js0 device listed e.g, #ls /dev/input _#by-id by-path event0 event1 js0 mice #Then lets try reading joystick values #sudo apt-get install joystick #sudo jstest /dev/input/js0 If you don\u2019t see the joystick working on the JTN or RPI, then don\u2019t try to use it on Donkey.","title":"define SERIAL_SPD 115200  // Set debug bitrate between 9600-115200 bps (default: 9600)"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#calibration","text":"On every startup it tries to load calibration data from the long-term memory and decides if those values are still correct or out of sync. The algorithm triggers a calibration automatically if necessary. Calibration can be triggered manually with powering the adapter while the CAL_CHANNEL is on full duty cycle. In other words the throttle must be up before start and it is automatically starts calibration on boot. When the device is in calibration mode, the leds are flashing steady on the board, and all the channel minimum and maximum values are getting recorded. During this time move all the configured sticks and pots/switches on the transmitter remote to its extents. After there is no more new min's or max's found the algorithm finishing the calibration within CAL_TIMEOUT by checking and saving the values in the long-term memory (eeprom). Leds are flashing twice and turns off, the adapter is now available as joystick on the device it is plugged in.","title":"Calibration"}]}