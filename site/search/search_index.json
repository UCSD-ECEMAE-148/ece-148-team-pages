{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"win23team3/","text":"Final Report for Team 3 (ECE/MAE 148) Team Members Nathaniel Barnaby - ECE Yang-Jie Qin - ECE Cheuk Hin Bryan Cheng - MAE Patrick Nguyen - MAE Project Overview Our initial final project was a combination of the usage of an IMU and GNSS to implement position tracking. An IMU is an inertial measurement unit, composing of gyros, accelerometers, and more sensors to track the movement of something. With these sensors, the car's location can be estimated off of a general initial GPS location with the addition to its movement measured by its speed, acceleration, turning, etc. This ended up being too complex for our team which resulted in little progress. We were then assigned new mini tasks which consists of using 2 of the sensors provided in our kits. The assignment was to use the OAK-D camera and the lidar separately to measure depth of both a small (0.15m) object and a larger (0.5m) object at different distances. We ended up comparing results of both objects at distances of 0.5, 1, 2, 3, and 4 meters. We would then compare the outputed values from the sensors to what the actual correspond measurment. A comparison between the accuracy of depth finding between the Oak-D camera and lidar would also be necesasry. A second task was assigned which was to output a face recognition system out of the OAK-D camera. Results For the distance measurement assignment, both the camera and lidar were able to successfully measure distance for the small the large object at the different ranges. For the camera, it was accurate at determining smaller distances, but at larger distances (3+ meters) error seemed to begin growing exponentionally. The difference between small and large objects was negligible as long as the area in which the distances were averaged fit within the object, which at further distances can start causing fluctuations with smaller objects. For the lidar, it was accurate at determining all distances with a linear or almost static amount of error. At smaller distances this was larger than the error of the camera, but at larger distances it vastly outperformed the camera due to the nature of the camera's exponential error. Additionally, the lidar had to be hand-calibrated, so with more time the error could have been lowered due to this effect. Also, since we recorded distance measurements within a range of 0.4 degrees, the measurement would be inaccurate with a smaller object at longer distances. This could be overcame by decreasing the range and waiting longer. For most scenarios the lidar seems to be the winning choice for distance measurement. While at lower distances the camera seemed to outperform the lidar, the lidar seems to be more consistent with its measurements than the camera. Additionally, the lidar offers 360 degrees of distance measurements while the camera only works in one direction. DepthAI Distance Measurement with ~0.5 m^2 Object Video Link: https://youtube.com/shorts/I1zaVxE-bjM?feature=share DepthAI Distance Measurement with 0.15 m^2 Box Video Link: https://youtube.com/shorts/GPADi4bS-fw?feature=share LiDAR Distance Measurement with ~0.5 m^2 Object Video Link: https://youtu.be/Ee81_VBHxKI LiDAR Distance Measurement with 0.15 m^2 Box Video Link: https://youtu.be/ARffTMx1PJ0 As for the camera face recognition, we were succesfully able to output video display which recognizes faces. This is a relatively fast responding system. It outputs the number of faces it recognizes which we tested from 0-3 faces real time. There is some error within the system as it can be innacurate thinking other shiny objects and or parts of a face are another face. It is also not limited to stationary faces as it recognizes people moving too. Face Recognition Video Link: https://youtu.be/nUz8OR_zHPA Gantt Chart Hardware: Mechanical Design \\ Camera/flashlight Mount \\ Electronics Tray \\ Front/rear Electronics Plate Offset \\ GPS Mount \\ IMU Mount \\ Jetson Case Key Mount \\ Jetson Nano Main Case \\ Lidar Tower \\ Servo Voltage Converter \\ Vesc Power Distributor Previous Designs Electronic Components \\ Jetson Nano \\ OAK-D Camera \\ Lidar LD06 Electronic Wiring Schematic Final Set Up Bird's Eye View Left View Right View Packages and Drivers cv2 (OpenCV) depthai (DepthAI) numpy math binascii Milestones Face Recognition using DepthAI - Detects faces through a webcam and displays a count in the terminal Distance Measurement using DepthAI - Using the disparity between the left and right cameras of the OAKD, distance can be calculated. This was averaged over an area to give an estimated distance of an object. Distance Measurement using LiDAR - Using a LiDAR is is relatively simple to detect distances in a 360 degree range. By averaging distances over a very small range (0.4 degrees) we determined the distance of an object. Potential Future Work/Unaccomplished Goals Recognizing and labeling specific faces Running code off of the OAK-D Camera instead of needing an external computer to run the code. Presentations Final Project Proposal: https://docs.google.com/presentation/d/1vLvXRnHzHm6p_IpEQy8KJgz--vOFd6M9xa7Q-qHD2Ls/edit?usp=sharing Final Presentation: https://docs.google.com/presentation/d/17J6LZ2QZ177BDr7g3x7ZcxHlyUQ-m4LtXGy9BRernRI/edit?usp=sharing Acknowledgments Professor Jack Silberman, TA Kishore Nukala, Moises Lopez-Mendoza, Design and Innovation Building, all of our wonderful classmates","title":"Team 3"},{"location":"win23team3/#final-report-for-team-3-ecemae-148","text":"","title":"Final Report for Team 3 (ECE/MAE 148)"},{"location":"win23team3/#team-members","text":"Nathaniel Barnaby - ECE Yang-Jie Qin - ECE Cheuk Hin Bryan Cheng - MAE Patrick Nguyen - MAE","title":"Team Members"},{"location":"win23team3/#project-overview","text":"Our initial final project was a combination of the usage of an IMU and GNSS to implement position tracking. An IMU is an inertial measurement unit, composing of gyros, accelerometers, and more sensors to track the movement of something. With these sensors, the car's location can be estimated off of a general initial GPS location with the addition to its movement measured by its speed, acceleration, turning, etc. This ended up being too complex for our team which resulted in little progress. We were then assigned new mini tasks which consists of using 2 of the sensors provided in our kits. The assignment was to use the OAK-D camera and the lidar separately to measure depth of both a small (0.15m) object and a larger (0.5m) object at different distances. We ended up comparing results of both objects at distances of 0.5, 1, 2, 3, and 4 meters. We would then compare the outputed values from the sensors to what the actual correspond measurment. A comparison between the accuracy of depth finding between the Oak-D camera and lidar would also be necesasry. A second task was assigned which was to output a face recognition system out of the OAK-D camera.","title":"Project Overview"},{"location":"win23team3/#results","text":"For the distance measurement assignment, both the camera and lidar were able to successfully measure distance for the small the large object at the different ranges. For the camera, it was accurate at determining smaller distances, but at larger distances (3+ meters) error seemed to begin growing exponentionally. The difference between small and large objects was negligible as long as the area in which the distances were averaged fit within the object, which at further distances can start causing fluctuations with smaller objects. For the lidar, it was accurate at determining all distances with a linear or almost static amount of error. At smaller distances this was larger than the error of the camera, but at larger distances it vastly outperformed the camera due to the nature of the camera's exponential error. Additionally, the lidar had to be hand-calibrated, so with more time the error could have been lowered due to this effect. Also, since we recorded distance measurements within a range of 0.4 degrees, the measurement would be inaccurate with a smaller object at longer distances. This could be overcame by decreasing the range and waiting longer. For most scenarios the lidar seems to be the winning choice for distance measurement. While at lower distances the camera seemed to outperform the lidar, the lidar seems to be more consistent with its measurements than the camera. Additionally, the lidar offers 360 degrees of distance measurements while the camera only works in one direction. DepthAI Distance Measurement with ~0.5 m^2 Object Video Link: https://youtube.com/shorts/I1zaVxE-bjM?feature=share DepthAI Distance Measurement with 0.15 m^2 Box Video Link: https://youtube.com/shorts/GPADi4bS-fw?feature=share LiDAR Distance Measurement with ~0.5 m^2 Object Video Link: https://youtu.be/Ee81_VBHxKI LiDAR Distance Measurement with 0.15 m^2 Box Video Link: https://youtu.be/ARffTMx1PJ0 As for the camera face recognition, we were succesfully able to output video display which recognizes faces. This is a relatively fast responding system. It outputs the number of faces it recognizes which we tested from 0-3 faces real time. There is some error within the system as it can be innacurate thinking other shiny objects and or parts of a face are another face. It is also not limited to stationary faces as it recognizes people moving too. Face Recognition Video Link: https://youtu.be/nUz8OR_zHPA","title":"Results"},{"location":"win23team3/#gantt-chart","text":"","title":"Gantt Chart"},{"location":"win23team3/#hardware-mechanical-design","text":"\\ Camera/flashlight Mount \\ Electronics Tray \\ Front/rear Electronics Plate Offset \\ GPS Mount \\ IMU Mount \\ Jetson Case Key Mount \\ Jetson Nano Main Case \\ Lidar Tower \\ Servo Voltage Converter \\ Vesc Power Distributor","title":"Hardware: Mechanical Design"},{"location":"win23team3/#previous-designs","text":"","title":"Previous Designs"},{"location":"win23team3/#electronic-components","text":"\\ Jetson Nano \\ OAK-D Camera \\ Lidar LD06","title":"Electronic Components"},{"location":"win23team3/#electronic-wiring-schematic","text":"","title":"Electronic Wiring Schematic"},{"location":"win23team3/#final-set-up","text":"Bird's Eye View Left View Right View","title":"Final Set Up"},{"location":"win23team3/#packages-and-drivers","text":"cv2 (OpenCV) depthai (DepthAI) numpy math binascii","title":"Packages and Drivers"},{"location":"win23team3/#milestones","text":"Face Recognition using DepthAI - Detects faces through a webcam and displays a count in the terminal Distance Measurement using DepthAI - Using the disparity between the left and right cameras of the OAKD, distance can be calculated. This was averaged over an area to give an estimated distance of an object. Distance Measurement using LiDAR - Using a LiDAR is is relatively simple to detect distances in a 360 degree range. By averaging distances over a very small range (0.4 degrees) we determined the distance of an object.","title":"Milestones"},{"location":"win23team3/#potential-future-workunaccomplished-goals","text":"Recognizing and labeling specific faces Running code off of the OAK-D Camera instead of needing an external computer to run the code.","title":"Potential Future Work/Unaccomplished Goals"},{"location":"win23team3/#presentations","text":"Final Project Proposal: https://docs.google.com/presentation/d/1vLvXRnHzHm6p_IpEQy8KJgz--vOFd6M9xa7Q-qHD2Ls/edit?usp=sharing Final Presentation: https://docs.google.com/presentation/d/17J6LZ2QZ177BDr7g3x7ZcxHlyUQ-m4LtXGy9BRernRI/edit?usp=sharing","title":"Presentations"},{"location":"win23team3/#acknowledgments","text":"Professor Jack Silberman, TA Kishore Nukala, Moises Lopez-Mendoza, Design and Innovation Building, all of our wonderful classmates","title":"Acknowledgments"},{"location":"win23team4/","text":"Team 4 Final Project Report Members: Vasanth Senthil (ECE), Eddy Rodas Lima(MAE), Lingpeng Meng(ECE) Physical Setup Initial Goals Objective Make our RoboCar follow sound using sound localization from multiple microphones. Must Haves Car is able to determine approximate direction of an audio source Car moves in towards audio source once direction of audio is determined Nice to Haves Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume Accurate movement towards source Accomplishments Researched and experimented with different methods of sound localization, such as using time of arrival delay between microphones Used the pyaudio library to collect streams of microphone data from each of the three microphone, and processed this data in order to tell direction real-time based on sound level Used the pyvesc library to create a custom movement file that allows us to easily move the RoboCar based on provided angles and throttle until specified conditions are met The microphone processing file called upon the movement functions after determining current direction. We also spread the left and right microphones to minimize the noise created by the car\u2019s movement, which would influence the accuracy of direction calculations. Demo Videos Static Source Left Static Source Right Moving Source Front Moving Source Back Moving Source Further Away Issues Faced Issue Solution Original idea of using time delay between microphones was not possible due to jetson limitations Decided to use volume of sound signals instead of microphone timing in order to determine direction of audio. Sound from car\u2019s movement, such as servo motors and wheels, were causing our robot to turn randomly. Created wings out of cardboard which lifted and spread out the left/right microphones away from the wheels. Random/sudden loud noises from the environment made the car turn in wrong directions, even when originally was moving correctly towards audio source. Averaged the audio signals of each microphone over the last few ticks using a buffer in order to reduce impact of outlier noises. Required tuning, as a small buffer would not remove outliers as effectively while a large buffer reduced reaction time of robot\u2019s movement when direction of audio actually changed What did not work Original plan was to use the time delay between when microphones head a noise over a sound threshold in order to calculate angle of sound. Problem with that is the Jetson Nano is running Linux, and due to background processes and overhead, it is not precise enough to identify the timing needed for this method to work when the microphones are this close together. This may have been possible with a real-time system such as an Arduino Another method was using real-time localized sound by performing beamforming using microphones in array, but we were not certain if we had enough time to implement and debug this method in time (received the microphones on Saturday) as it was significantly more complex than the other options. Next Steps (If we had more time) Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume, would help filter interference with background noises. Function based on sound difference between microphones to get precise steering angles. Current only steers forward, full left, full right. More accurate movement with our backwards direction Minimize unwanted noise coming from either surroundings or the vehicle.","title":"Team 4"},{"location":"win23team4/#team-4-final-project-report","text":"","title":"Team 4 Final Project Report"},{"location":"win23team4/#members-vasanth-senthil-ece-eddy-rodas-limamae-lingpeng-mengece","text":"","title":"Members: Vasanth Senthil (ECE), Eddy Rodas Lima(MAE), Lingpeng Meng(ECE)"},{"location":"win23team4/#physical-setup","text":"","title":"Physical Setup"},{"location":"win23team4/#initial-goals","text":"","title":"Initial Goals"},{"location":"win23team4/#objective","text":"Make our RoboCar follow sound using sound localization from multiple microphones.","title":"Objective"},{"location":"win23team4/#must-haves","text":"Car is able to determine approximate direction of an audio source Car moves in towards audio source once direction of audio is determined","title":"Must Haves"},{"location":"win23team4/#nice-to-haves","text":"Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume Accurate movement towards source","title":"Nice to Haves"},{"location":"win23team4/#accomplishments","text":"Researched and experimented with different methods of sound localization, such as using time of arrival delay between microphones Used the pyaudio library to collect streams of microphone data from each of the three microphone, and processed this data in order to tell direction real-time based on sound level Used the pyvesc library to create a custom movement file that allows us to easily move the RoboCar based on provided angles and throttle until specified conditions are met The microphone processing file called upon the movement functions after determining current direction. We also spread the left and right microphones to minimize the noise created by the car\u2019s movement, which would influence the accuracy of direction calculations.","title":"Accomplishments"},{"location":"win23team4/#demo-videos","text":"Static Source Left Static Source Right Moving Source Front Moving Source Back Moving Source Further Away","title":"Demo Videos"},{"location":"win23team4/#issues-faced","text":"Issue Solution Original idea of using time delay between microphones was not possible due to jetson limitations Decided to use volume of sound signals instead of microphone timing in order to determine direction of audio. Sound from car\u2019s movement, such as servo motors and wheels, were causing our robot to turn randomly. Created wings out of cardboard which lifted and spread out the left/right microphones away from the wheels. Random/sudden loud noises from the environment made the car turn in wrong directions, even when originally was moving correctly towards audio source. Averaged the audio signals of each microphone over the last few ticks using a buffer in order to reduce impact of outlier noises. Required tuning, as a small buffer would not remove outliers as effectively while a large buffer reduced reaction time of robot\u2019s movement when direction of audio actually changed","title":"Issues Faced"},{"location":"win23team4/#what-did-not-work","text":"Original plan was to use the time delay between when microphones head a noise over a sound threshold in order to calculate angle of sound. Problem with that is the Jetson Nano is running Linux, and due to background processes and overhead, it is not precise enough to identify the timing needed for this method to work when the microphones are this close together. This may have been possible with a real-time system such as an Arduino Another method was using real-time localized sound by performing beamforming using microphones in array, but we were not certain if we had enough time to implement and debug this method in time (received the microphones on Saturday) as it was significantly more complex than the other options.","title":"What did not work"},{"location":"win23team4/#next-steps-if-we-had-more-time","text":"Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume, would help filter interference with background noises. Function based on sound difference between microphones to get precise steering angles. Current only steers forward, full left, full right. More accurate movement with our backwards direction Minimize unwanted noise coming from either surroundings or the vehicle.","title":"Next Steps (If we had more time)"}]}